# =============================================================================
#         Kaggle æŒä¹…åŒ–åª’ä½“å¤„ç†æœåŠ¡ - ä¸»åº”ç”¨ v2.0 (æ–¹æ¡ˆ2)
# =============================================================================
#
# åŠŸèƒ½:
#   - å¯åŠ¨ MixFileCLI ä½œä¸ºæ–‡ä»¶å­˜å‚¨åç«¯ã€‚
#   - æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ Flask APIï¼Œç”¨äº:
#     1. ä» URL ä¸‹è½½æ–‡ä»¶å¹¶ä¸Šä¼ åˆ° MixFileã€‚
#     2. ä»è§†é¢‘ URL ä¸­æå–é«˜è´¨é‡å­—å¹• (å¯é€‰)ã€‚
#     3. å°†åŸå§‹è§†é¢‘å’Œç”Ÿæˆçš„å­—å¹•çµæ´»åœ°ä¸Šä¼ åˆ° MixFileã€‚
#   - é€šè¿‡ FRP å°†å†…éƒ¨æœåŠ¡å®‰å…¨åœ°æš´éœ²åˆ°å…¬ç½‘ã€‚
#   - æ‰€æœ‰æ•æ„Ÿé…ç½® (FRP, APIå¯†é’¥) å‡é€šè¿‡åŠ å¯†æ–¹å¼ç®¡ç†ã€‚
#   - å®ç°é«˜å®¹é”™çš„ä»»åŠ¡å¤„ç†é€»è¾‘ï¼Œå­ä»»åŠ¡å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹ã€‚
#
# =============================================================================

# --- æ ¸å¿ƒ Python åº“ ---
import os
import subprocess
import threading
import time
import uuid
import socket
import json
import base64
import hashlib
import signal
import sys
import shutil
import mimetypes
from pathlib import Path
from datetime import timedelta
from urllib.parse import urljoin, quote, unquote
from concurrent.futures import ThreadPoolExecutor

# --- Web æ¡†æ¶ä¸ HTTP å®¢æˆ·ç«¯ ---
from flask import Flask, request, jsonify, Response
import requests

# --- åŠ å¯†ä¸å¯†é’¥ç®¡ç† ---
# å°è¯•å¯¼å…¥å…³é”®åº“ï¼Œå¦‚æœå¤±è´¥åˆ™åœ¨åç»­æ£€æŸ¥ä¸­å¤„ç†
try:
    from kaggle_secrets import UserSecretsClient
    from cryptography.fernet import Fernet
except ImportError:
    UserSecretsClient = None
    Fernet = None

# --- å­—å¹•æå–ç›¸å…³åº“ (å°†åœ¨ check_environment ä¸­éªŒè¯) ---
try:
    import torch
    import torchaudio
    from pydub import AudioSegment
    import pydantic
except ImportError as e:
    # å…è®¸å¯åŠ¨æ—¶å¯¼å…¥å¤±è´¥ï¼Œåç»­ç¯å¢ƒæ£€æŸ¥ä¼šæ•è·
    print(f"[è­¦å‘Š] é¢„åŠ è½½éƒ¨åˆ†åº“å¤±è´¥: {e}ã€‚å°†åœ¨ç¯å¢ƒæ£€æŸ¥ä¸­ç¡®è®¤ã€‚")
    torch = None
    torchaudio = None
    AudioSegment = None
    pydantic = None

# =============================================================================
# --- ç¬¬ 1 æ­¥: å…¨å±€é…ç½® ---
# =============================================================================

# -- A. MixFileCLI é…ç½® --
mixfile_config_yaml = """
uploader: "çº¿è·¯A2"
upload_task: 10
upload_retry: 10
download_task: 5
chunk_size: 1024
port: 4719
host: "0.0.0.0"
password: ""
webdav_path: "data.mix_dav"
history: "history.mix_list"
"""

# -- B. åŠ å¯†çš„æœåŠ¡é…ç½® --
# !!! é‡è¦ !!!
# åœ¨è¿™é‡Œç²˜è´´ä½ ä» encrypt_util.py å·¥å…·ä¸­è·å¾—çš„åŠ å¯†é…ç½®å­—ç¬¦ä¸²
# ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²ç”¨äº FRP (åå‘ä»£ç†)
ENCRYPTED_FRP_CONFIG = "gAAAAABouqe8kDRGZxX7I43TbmNqmy2TZNRJK0f5GqrXwKwW0VR3TtfyfFmDvHCtPbfMzcBkT1qgfMPzhPWDuHRmwibWSmVOgi4nZe_J1TTmLAWMSzQOieGMWE2LXRIIKbf-dLMCnjPDKRkLmZBuWtreN5QgZC41Px3hRU7pxqw51edAkOmvZiE7nf9G0AQ0IvCAR9WHMDg6"

# ç¬¬äºŒä¸ªå­—ç¬¦ä¸²ç”¨äºå­—å¹•æœåŠ¡ (Gemini API å¯†é’¥ç­‰)
ENCRYPTED_SUBTITLE_CONFIG = "gAAAAABouqgGsJh7-osSg3oc2OhtcGotCfGufGZLgeV_4hF6mr5qrhh2wKCjalhFRuPdeUPMNHWqz5CRju55o4CPcr_Deprf-hktkuQ9sDhtRI0mWMkqqwlqPO28AClTO-q0bKXZ74BEPcTCXPVeHZaG_gTolEMblHXXuNK7Mm3VlJV7S5PE1WuvjG4uhne7UtbzYWx5TBE46df09vAxEXb7Tyo05B2jSjkm8NVM3JRmOfSWFpvv9r-J7n3ZCyPCHDj6hc0-6IBkqnX4il41zdgovtxXVugNeploPQylxPI240L1nm6zRKfSlplkNhZ3k1reH3LjDyY1zfp5aEhNKqQXBUrQY_4c--Km2LbQml9BbjRhOnJmUF2uo7Aa7c8mZUM4-jWNDU8KD0dCJFrWkeUQoW7bPlicrw==" # ç¤ºä¾‹ï¼Œè¯·æ›¿æ¢

# -- C. æœ¬åœ°æœåŠ¡ä¸ç«¯å£é…ç½® --
MIXFILE_LOCAL_PORT = 4719
FLASK_API_LOCAL_PORT = 5000
MIXFILE_REMOTE_PORT = 20000  # æ˜ å°„åˆ°å…¬ç½‘çš„ MixFile ç«¯å£
FLASK_API_REMOTE_PORT = 20001  # æ˜ å°„åˆ°å…¬ç½‘çš„ Flask API ç«¯å£

# -- D. Killer API & è¿›ç¨‹ç®¡ç†é…ç½® --
KILLER_API_SHUTDOWN_TOKEN = "change-this-to-a-secure-random-string" # !!! å¼ºçƒˆå»ºè®®ä¿®æ”¹ !!!
PROCESS_KEYWORDS_TO_KILL = ["java", "frpc"]
EXCLUDE_KEYWORDS_FROM_KILL = ["jupyter", "kernel", "ipykernel", "conda", "grep"]

# -- E. å­—å¹•æå–æµç¨‹é…ç½® --
# è¿™äº›å€¼æœªæ¥ä¹Ÿå¯ä»¥åŠ å…¥åˆ°åŠ å¯†é…ç½®ä¸­
SUBTITLE_CHUNK_DURATION_MS = 10 * 60 * 1000  # 10åˆ†é’Ÿ
SUBTITLE_BATCH_SIZE = 40
SUBTITLE_CONCURRENT_REQUESTS = 8
SUBTITLE_REQUESTS_PER_MINUTE = 8

# =============================================================================
# --- ç¬¬ 2 æ­¥: è§£å¯†ä¸é…ç½®åŠ è½½æ¨¡å— ---
# =============================================================================

class DecryptionError(Exception):
    # è‡ªå®šä¹‰å¼‚å¸¸ï¼Œç”¨äºè¡¨ç¤ºè§£å¯†è¿‡ç¨‹ä¸­çš„ç‰¹å®šå¤±è´¥ã€‚
    pass

def _get_decryption_cipher():
    #
    # è¾…åŠ©å‡½æ•°ï¼šä» Kaggle Secrets è·å–å¯†ç å¹¶ç”Ÿæˆ Fernet è§£å¯†å™¨ã€‚
    # è¿™æ˜¯ä¸€ä¸ªå…±äº«é€»è¾‘ï¼Œè¢«å…¶ä»–è§£å¯†å‡½æ•°è°ƒç”¨ã€‚
    #
    if not UserSecretsClient or not Fernet:
        raise DecryptionError("å…³é”®åº“ (kaggle_secrets, cryptography) æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥ã€‚")
    
    print("ğŸ” æ­£åœ¨ä» Kaggle Secrets è·å–è§£å¯†å¯†é’¥ 'FRP_DECRYPTION_KEY'...")
    try:
        secrets = UserSecretsClient()
        decryption_key_password = secrets.get_secret("FRP_DECRYPTION_KEY")
        if not decryption_key_password:
             raise ValueError("Kaggle Secret 'FRP_DECRYPTION_KEY' çš„å€¼ä¸ºç©ºã€‚")
    except Exception as e:
        print(f"âŒ æ— æ³•ä» Kaggle Secrets è·å–å¯†é’¥ï¼è¯·ç¡®ä¿ä½ å·²æ­£ç¡®è®¾ç½®äº†åä¸º 'FRP_DECRYPTION_KEY' çš„ Secretã€‚")
        raise DecryptionError(f"è·å– Kaggle Secret å¤±è´¥: {e}") from e

    # ä½¿ç”¨ SHA256 ä»ç”¨æˆ·å¯†ç æ´¾ç”Ÿä¸€ä¸ªç¡®å®šæ€§çš„ã€å®‰å…¨çš„32å­—èŠ‚å¯†é’¥
    key = base64.urlsafe_b64encode(hashlib.sha256(decryption_key_password.encode()).digest())
    return Fernet(key)

def get_decrypted_config(encrypted_string: str, config_name: str) -> dict:
    #
    # é€šç”¨çš„è§£å¯†å‡½æ•°ï¼Œç”¨äºè§£å¯†ä»»ä½•ç»™å®šçš„åŠ å¯†å­—ç¬¦ä¸²ã€‚
    #
    # Args:
    #     encrypted_string (str): ä» encrypt_util.py è·å–çš„ Base64 ç¼–ç çš„åŠ å¯†å­—ç¬¦ä¸²ã€‚
    #     config_name (str): é…ç½®çš„åç§°ï¼Œç”¨äºæ—¥å¿—è¾“å‡º (ä¾‹å¦‚ "FRP", "Subtitle")ã€‚
    #
    # Returns:
    #     dict: è§£å¯†å¹¶è§£æåçš„é…ç½®å­—å…¸ã€‚
    # 
    # Raises:
    #     DecryptionError: å¦‚æœè§£å¯†è¿‡ç¨‹çš„ä»»ä½•æ­¥éª¤å¤±è´¥ã€‚
    #
    print(f"ğŸ”‘ æ­£åœ¨å‡†å¤‡è§£å¯† {config_name} é…ç½®...")
    try:
        if "PASTE_YOUR_ENCRYPTED" in encrypted_string:
             raise ValueError(f"æ£€æµ‹åˆ°å ä½ç¬¦åŠ å¯†å­—ç¬¦ä¸²ï¼Œè¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®é…ç½®ã€‚")
        
        cipher = _get_decryption_cipher()
        decrypted_bytes = cipher.decrypt(encrypted_string.encode('utf-8'))
        config = json.loads(decrypted_bytes.decode('utf-8'))
        print(f"âœ… {config_name} é…ç½®è§£å¯†æˆåŠŸï¼")
        return config
    except ValueError as e:
        print(f"âŒ è§£å¯† {config_name} é…ç½®å¤±è´¥: {e}")
        raise DecryptionError(f"é…ç½®å­—ç¬¦ä¸²æ— æ•ˆ: {e}")
    except Exception as e:
        print(f"âŒ è§£å¯† {config_name} é…ç½®å¤±è´¥ï¼")
        print("   è¿™é€šå¸¸æ„å‘³ç€ä½ çš„ Kaggle Secret (å¯†ç ) ä¸åŠ å¯†æ—¶ä½¿ç”¨çš„å¯†ç ä¸åŒ¹é…ï¼Œ")
        print("   æˆ–è€…åŠ å¯†å­—ç¬¦ä¸²å·²æŸåã€‚")
        raise DecryptionError(f"è§£å¯†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}") from e
        
# =============================================================================
# --- ç¬¬ 3 æ­¥: ç¯å¢ƒæ£€æŸ¥ã€è¾…åŠ©å‡½æ•°ä¸ Killer API ---
# =============================================================================

def check_environment():
    #
    # æ£€æŸ¥è¿è¡Œæ‰€éœ€çš„æ‰€æœ‰å…³é”®ä¾èµ–å’Œåº“æ˜¯å¦éƒ½å·²æ­£ç¡®å®‰è£…ã€‚
    # å¦‚æœç¼ºå°‘å…³é”®ç»„ä»¶ï¼Œåˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œä½¿ç¨‹åºæå‰å¤±è´¥ã€‚
    #
    print("\n" + "="*60)
    print("ğŸ”¬ æ­£åœ¨æ‰§è¡Œç¯å¢ƒä¾èµ–æ£€æŸ¥...")
    print("="*60)
    
    # æ£€æŸ¥åŸºç¡€åº“
    if not UserSecretsClient or not Fernet:
        raise RuntimeError("å…³é”®å®‰å…¨åº“ 'kaggle_secrets' æˆ– 'cryptography' æœªæ‰¾åˆ°ã€‚æ— æ³•ç»§ç»­ã€‚")
    print("âœ… å®‰å…¨åº“ (kaggle_secrets, cryptography) - æ­£å¸¸")
    
    # æ£€æŸ¥å­—å¹•æå–æ ¸å¿ƒåº“
    critical_subtitle_libs = {
        "torch": torch,
        "torchaudio": torchaudio,
        "pydub": AudioSegment,
        "pydantic": pydantic
    }
    for name, lib in critical_subtitle_libs.items():
        if not lib:
            raise RuntimeError(f"å­—å¹•æå–æ ¸å¿ƒåº“ '{name}' æœªæ‰¾åˆ°æˆ–å¯¼å…¥å¤±è´¥ã€‚è¯·æ£€æŸ¥ Kaggle ç¯å¢ƒã€‚")
        print(f"âœ… å­—å¹•åº“ ({name}) - æ­£å¸¸")

    # æ£€æŸ¥å‘½ä»¤è¡Œå·¥å…·
    if not shutil.which("ffmpeg"):
        raise RuntimeError("'ffmpeg' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¿™æ˜¯æå–éŸ³é¢‘æ‰€å¿…éœ€çš„ã€‚")
    print("âœ… å‘½ä»¤è¡Œå·¥å…· (ffmpeg) - æ­£å¸¸")
    
    if not shutil.which("java"):
        raise RuntimeError("'java' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¿™æ˜¯è¿è¡Œ MixFileCLI æ‰€å¿…éœ€çš„ã€‚")
    print("âœ… å‘½ä»¤è¡Œå·¥å…· (java) - æ­£å¸¸")

    print("\nâœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼æ‰€æœ‰å…³é”®ä¾èµ–å‡å·²å°±ç»ªã€‚\n")


def log_system_event(level: str, message: str):
    # ä¸€ä¸ªç®€å•çš„å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—è®°å½•å™¨ã€‚
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
    prefix = f"[{timestamp} SYSTEM {level.upper()}]"
    print(f"{prefix} {message}", flush=True)


def ms_to_srt_time(ms: int) -> str:
    # å°†æ¯«ç§’è½¬æ¢ä¸º SRT å­—å¹•æ–‡ä»¶çš„æ—¶é—´æ ¼å¼ (HH:MM:SS,ms)ã€‚
    try:
        ms = int(ms)
    except (ValueError, TypeError):
        ms = 0
    td = timedelta(milliseconds=ms)
    hours, remainder = divmod(td.seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    milliseconds = td.microseconds // 1000
    return f"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}"


def run_command(command: str, log_file: str = None) -> subprocess.Popen:
    #
    # åœ¨åå°ä»¥éé˜»å¡æ–¹å¼æ‰§è¡Œä¸€ä¸ª shell å‘½ä»¤ã€‚
    #
    # Args:
    #     command (str): è¦æ‰§è¡Œçš„å‘½ä»¤ã€‚
    #     log_file (str, optional): å°† stdout å’Œ stderr é‡å®šå‘åˆ°çš„æ—¥å¿—æ–‡ä»¶åã€‚
    #
    # Returns:
    #     subprocess.Popen: æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹å¯¹è±¡ã€‚
    #
    log_system_event("info", f"æ‰§è¡Œå‘½ä»¤: {command}")
    stdout_dest = None
    stderr_dest = None
    if log_file:
        log_handle = open(log_file, 'w', encoding='utf-8')
        stdout_dest = log_handle
        stderr_dest = log_handle
    
    # ä½¿ç”¨ Popen ä»¥éé˜»å¡æ–¹å¼å¯åŠ¨è¿›ç¨‹
    process = subprocess.Popen(
        command,
        shell=True,
        stdout=stdout_dest,
        stderr=stderr_dest,
        encoding='utf-8',
        errors='ignore'
    )
    return process


def wait_for_port(port: int, host: str = '127.0.0.1', timeout: float = 60.0) -> bool:
    #
    # ç­‰å¾…æŒ‡å®šçš„æœ¬åœ°ç«¯å£å˜ä¸ºå¯ç”¨çŠ¶æ€ã€‚
    #
    # Args:
    #     port (int): è¦æ£€æŸ¥çš„ç«¯å£å·ã€‚
    #     host (str, optional): ç›‘å¬çš„ä¸»æœºåœ°å€ã€‚é»˜è®¤ä¸º '127.0.0.1'ã€‚
    #     timeout (float, optional): æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ã€‚é»˜è®¤ä¸º 60.0ã€‚
    #
    # Returns:
    #     bool: å¦‚æœç«¯å£åœ¨è¶…æ—¶å‰å˜ä¸ºå¯ç”¨ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    #
    log_system_event("info", f"æ­£åœ¨ç­‰å¾…ç«¯å£ {host}:{port} å¯åŠ¨...")
    start_time = time.perf_counter()
    while True:
        try:
            with socket.create_connection((host, port), timeout=1):
                log_system_event("info", f"âœ… ç«¯å£ {port} å·²æˆåŠŸå¯åŠ¨ï¼")
                return True
        except (socket.timeout, ConnectionRefusedError):
            time.sleep(1)
            if time.perf_counter() - start_time >= timeout:
                log_system_event("error", f"âŒ ç­‰å¾…ç«¯å£ {port} è¶…æ—¶ ({timeout}ç§’)ã€‚")
                return False

# --- Killer API é€»è¾‘ (ç”¨äºè¿œç¨‹å…³é—­) ---

def _find_and_kill_targeted_processes(signal_to_send=signal.SIGTERM):
    # æŸ¥æ‰¾å¹¶ç»ˆæ­¢æ­¤è„šæœ¬å¯åŠ¨çš„æ‰€æœ‰å…³é”®å­è¿›ç¨‹ (java, frpc)ã€‚
    killed_pids_info = []
    log_system_event("info", f"æŸ¥æ‰¾å¹¶å°è¯•ç»ˆæ­¢ä¸ '{PROCESS_KEYWORDS_TO_KILL}' ç›¸å…³çš„è¿›ç¨‹...")
    
    current_kernel_pid = os.getpid()
    pids_to_target = set()

    try:
        # ä½¿ç”¨ ps å‘½ä»¤è·å–æ‰€æœ‰è¿›ç¨‹ä¿¡æ¯
        ps_output = subprocess.check_output(['ps', '-eo', 'pid,ppid,args'], text=True)
        
        for line in ps_output.splitlines()[1:]:
            parts = line.strip().split(None, 2)
            if len(parts) < 3: continue
            
            try:
                pid = int(parts[0])
                command_line = parts[2]
                command_lower = command_line.lower()
            except (ValueError, IndexError):
                continue
            
            # æ’é™¤å½“å‰å†…æ ¸è¿›ç¨‹å’Œç³»ç»Ÿå…³é”®è¿›ç¨‹
            if pid == current_kernel_pid: continue
            is_excluded = any(ex_kw.lower() in command_lower for ex_kw in EXCLUDE_KEYWORDS_FROM_KILL)
            if is_excluded: continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡è¿›ç¨‹
            is_target = any(target_kw.lower() in command_lower for target_kw in PROCESS_KEYWORDS_TO_KILL)
            if is_target:
                pids_to_target.add(pid)
                log_system_event("debug", f"  å‘ç°ç›®æ ‡: PID={pid}, CMD='{command_line}'")

        if not pids_to_target:
            log_system_event("info", "æœªæ‰¾åˆ°éœ€è¦ç»ˆæ­¢çš„ç‰¹å®šåº”ç”¨å­è¿›ç¨‹ã€‚")
        else:
            log_system_event("info", f"å‡†å¤‡å‘ PIDs {list(pids_to_target)} å‘é€ä¿¡å· {signal_to_send}...")
            for pid_to_kill in pids_to_target:
                try:
                    os.kill(pid_to_kill, signal_to_send)
                    log_system_event("info", f"    å·²å‘ PID {pid_to_kill} å‘é€ä¿¡å· {signal_to_send}.")
                    killed_pids_info.append({"pid": pid_to_kill, "status": "signal_sent"})
                except ProcessLookupError:
                    killed_pids_info.append({"pid": pid_to_kill, "status": "not_found"})
                except Exception as e:
                    killed_pids_info.append({"pid": pid_to_kill, "status": f"error_{type(e).__name__}"})
                    
    except Exception as e:
        log_system_event("error", f"æŸ¥æ‰¾æˆ–ç»ˆæ­¢å­è¿›ç¨‹æ—¶å‡ºé”™: {e}")
    
    return killed_pids_info


def _shutdown_notebook_kernel_immediately():
    #
    # é€šè¿‡å‘é€ SIGKILL ä¿¡å·å¼ºåˆ¶ã€ç«‹å³åœ°å…³é—­å½“å‰ Kaggle Notebook å†…æ ¸ã€‚
    # è¿™æ˜¯æœ€ç»ˆçš„è‡ªæ¯æŒ‡ä»¤ã€‚
    #
    log_system_event("critical", "å‡†å¤‡é€šè¿‡ SIGKILL ä¿¡å·å¼ºåˆ¶å…³é—­å½“å‰ Kaggle Notebook Kernel...")
    sys.stdout.flush()
    sys.stderr.flush()
    time.sleep(0.5) # ç•™å‡ºä¸€ç‚¹æ—¶é—´è®©æ—¥å¿—åˆ·æ–°
    
    # SIGKILL (ä¿¡å· 9) æ˜¯ä¸€ä¸ªæ— æ³•è¢«æ•è·æˆ–å¿½ç•¥çš„ä¿¡å·ï¼Œæ¯” os._exit æ›´ä¸ºå¼ºåˆ¶ã€‚
    os.kill(os.getpid(), signal.SIGKILL)

# =============================================================================
# --- ç¬¬ 4 æ­¥: å­—å¹•æå–æ ¸å¿ƒæ¨¡å— ---
# =============================================================================

# --- A. Pydantic æ•°æ®éªŒè¯æ¨¡å‹ ---
# ç”¨äºä¸¥æ ¼éªŒè¯ Gemini API è¿”å›çš„ JSON ç»“æ„ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚

class SubtitleLine(pydantic.BaseModel):
    start_ms: int
    end_ms: int | None = None
    text: str

class BatchTranscriptionResult(pydantic.BaseModel):
    subtitles: list[SubtitleLine]


# --- B. åŠ¨æ€æç¤ºè¯ä¸æ–‡ä»¶å¤„ç† ---

def get_dynamic_prompts(api_url: str) -> tuple[str, str]:
    #
    # ä»æŒ‡å®šçš„ API è·å–åŠ¨æ€æç¤ºè¯ã€‚å¦‚æœå¤±è´¥ï¼Œåˆ™è¿”å›ç¡¬ç¼–ç çš„å¤‡ç”¨æç¤ºè¯ã€‚
    #
    log_system_event("info", "æ­£åœ¨å°è¯•ä» API è·å–åŠ¨æ€æç¤ºè¯...")
    try:
        response = requests.get(api_url, timeout=10)
        response.raise_for_status()
        prompts = response.json()
        if "system_instruction" in prompts and "prompt_for_task" in prompts:
            log_system_event("info", "âœ… æˆåŠŸä» API è·å–åŠ¨æ€æç¤ºè¯ã€‚")
            return prompts['system_instruction'], prompts['prompt_for_task']
        else:
            raise ValueError("API å“åº”ä¸­ç¼ºå°‘å¿…è¦çš„é”®ã€‚")
    except Exception as e:
        log_system_event("warning", f"è·å–åŠ¨æ€æç¤ºè¯å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨å¤‡ç”¨æç¤ºè¯ã€‚")
        # --- å¤‡ç”¨æç¤ºè¯ (Fallback Prompts) ---
        fallback_system = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­—å¹•ç¿»è¯‘æ¨¡å‹ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æä¾›çš„ä»»ä½•è¯­è¨€çš„éŸ³é¢‘å†…å®¹ç¿»è¯‘æˆ**ç®€ä½“ä¸­æ–‡**ã€‚"
            "åœ¨ä½ çš„æ‰€æœ‰è¾“å‡ºä¸­ï¼Œ`text` å­—æ®µçš„å€¼**å¿…é¡»æ˜¯ç®€ä½“ä¸­æ–‡**ã€‚"
            "**ç»å¯¹ç¦æ­¢**åœ¨ `text` å­—æ®µä¸­è¿”å›ä»»ä½•åŸå§‹è¯­è¨€ï¼ˆå¦‚æ—¥è¯­ï¼‰çš„æ–‡æœ¬ã€‚"
        )
        fallback_task = (
            "æˆ‘å°†ä¸ºä½ æä¾›ä¸€ç³»åˆ—éŸ³é¢‘ç‰‡æ®µå’Œå®ƒä»¬åœ¨è§†é¢‘ä¸­çš„ç»å¯¹æ—¶é—´ `[AUDIO_INFO] start_ms --> end_ms`ã€‚\n"
            "è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n"
            "1.  **å¬å–éŸ³é¢‘**å¹¶ç†è§£å…¶å†…å®¹ã€‚\n"
            "2.  **åˆ›å»ºå†…éƒ¨æ—¶é—´è½´**: å°†**ç¿»è¯‘æˆä¸­æ–‡å**çš„æ–‡æœ¬ï¼Œæ ¹æ®è¯­éŸ³åœé¡¿åˆ†å‰²æˆæ›´çŸ­çš„å­—å¹•è¡Œï¼Œå¹¶ä¸ºæ¯ä¸€è¡Œä¼°ç®—ä¸€ä¸ªç²¾ç¡®çš„ `start_ms` å’Œ `end_ms`ã€‚\n"
            "3.  **æ ¼å¼åŒ–è¾“å‡º**: ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªæ ¼å¼å®Œå…¨æ­£ç¡®çš„ JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½• markdown æ ‡è®°ã€‚ç»“æ„å¦‚ä¸‹ï¼Œå…¶ä¸­ `text` å­—æ®µçš„å€¼å¿…é¡»æ˜¯ä½ ç¿»è¯‘åçš„**ç®€ä½“ä¸­æ–‡**ï¼š\n"
            '{\n'
            '  "subtitles": [\n'
            '    { "start_ms": 12345, "end_ms": 13456, "text": "è¿™æ˜¯ç¿»è¯‘åçš„ç¬¬ä¸€å¥å­—å¹•" },\n'
            '    { "start_ms": 13600, "text": "è¿™æ˜¯ç¬¬äºŒå¥" }\n'
            '  ]\n'
            '}\n'
        )
        return fallback_system, fallback_task


def read_and_encode_file_base64(filepath: str) -> str | None:
    # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è¿”å› Base64 ç¼–ç çš„å­—ç¬¦ä¸²ã€‚
    try:
        with open(filepath, "rb") as f:
            binary_data = f.read()
        return base64.b64encode(binary_data).decode('utf-8')
    except Exception as e:
        log_system_event("error", f"æ— æ³•è¯»å–æˆ–ç¼–ç æ–‡ä»¶: {filepath}. é”™è¯¯: {e}")
        return None

# --- C. éŸ³é¢‘é¢„å¤„ç†ç®¡é“ ---

def preprocess_audio_for_subtitles(
    video_path: Path,
    temp_dir: Path,
    update_status_callback: callable
) -> list[dict]:
    #
    # å®Œæ•´çš„éŸ³é¢‘é¢„å¤„ç†æµç¨‹ï¼šæå– -> é™å™ª -> VAD åˆ‡åˆ†ã€‚
    #
    # Args:
    #     video_path (Path): è¾“å…¥çš„è§†é¢‘æ–‡ä»¶è·¯å¾„ã€‚
    #     temp_dir (Path): ç”¨äºå­˜æ”¾æ‰€æœ‰ä¸­é—´æ–‡ä»¶çš„ä¸´æ—¶ç›®å½•ã€‚
    #     update_status_callback (callable): ç”¨äºæ›´æ–°ä»»åŠ¡çŠ¶æ€çš„å›è°ƒå‡½æ•°ã€‚
    #
    # Returns:
    #     list[dict]: ä¸€ä¸ªåŒ…å«æ‰€æœ‰æœ‰æ•ˆè¯­éŸ³ç‰‡æ®µä¿¡æ¯çš„åˆ—è¡¨ï¼Œ
    #                 æ¯ä¸ªå…ƒç´ æ˜¯ {"path": str, "start_ms": int, "end_ms": int}ã€‚
    # 
    # Raises:
    #     Exception: å¦‚æœåœ¨ä»»ä½•å…³é”®æ­¥éª¤ï¼ˆå¦‚ ffmpegï¼‰ä¸­å‘ç”Ÿå¤±è´¥ã€‚
    #
    
    # 1. ä½¿ç”¨ ffmpeg æå–åŸå§‹éŸ³é¢‘
    update_status_callback(stage="subtitle_extract_audio", details="æ­£åœ¨ä»è§†é¢‘ä¸­æå–éŸ³é¢‘...")
    raw_audio_path = temp_dir / "raw_audio.wav"
    try:
        command = [
            "ffmpeg", "-i", str(video_path),
            "-ac", "1", "-ar", "16000", # å•å£°é“, 16kHz é‡‡æ ·ç‡ (è¯­éŸ³è¯†åˆ«æ ‡å‡†)
            "-vn", "-y", "-loglevel", "error", str(raw_audio_path)
        ]
        # ä½¿ç”¨ subprocess.run ç­‰å¾…å‘½ä»¤å®Œæˆ
        process = subprocess.run(command, check=True, capture_output=True, text=True)
    except subprocess.CalledProcessError as e:
        log_system_event("error", f"FFmpeg æå–éŸ³é¢‘å¤±è´¥ã€‚Stderr: {e.stderr}")
        raise RuntimeError(f"FFmpeg æå–éŸ³é¢‘å¤±è´¥: {e.stderr}")

    # 2. å°è¯•åŠ è½½ AI é™å™ªæ¨¡å‹
    denoiser_model = None
    try:
        from denoiser import pretrained
        update_status_callback(stage="subtitle_denoise", details="æ­£åœ¨åŠ è½½ AI é™å™ªæ¨¡å‹...")
        denoiser_model = pretrained.dns64().cuda()
        log_system_event("info", "AI é™å™ªæ¨¡å‹åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        log_system_event("warning", f"åŠ è½½ AI é™å™ªæ¨¡å‹å¤±è´¥ï¼Œå°†è·³è¿‡é™å™ªæ­¥éª¤ã€‚é”™è¯¯: {e}")

    # 3. åˆ†å—å¤„ç†éŸ³é¢‘ï¼šé™å™ª -> VAD
    update_status_callback(stage="subtitle_vad", details="æ­£åœ¨è¿›è¡ŒéŸ³é¢‘åˆ†å—ä¸è¯­éŸ³æ£€æµ‹...")
    original_audio = AudioSegment.from_wav(raw_audio_path)
    total_duration_ms = len(original_audio)
    chunk_files = []
    chunks_dir = temp_dir / "audio_chunks"
    chunks_dir.mkdir(exist_ok=True)
    num_chunks = -(-total_duration_ms // SUBTITLE_CHUNK_DURATION_MS)

    for i in range(num_chunks):
        start_time_ms = i * SUBTITLE_CHUNK_DURATION_MS
        end_time_ms = min((i + 1) * SUBTITLE_CHUNK_DURATION_MS, total_duration_ms)
        
        log_system_event("info", f"æ­£åœ¨å¤„ç†éŸ³é¢‘æ€»å— {i+1}/{num_chunks}...")
        
        audio_chunk = original_audio[start_time_ms:end_time_ms]
        temp_chunk_path = temp_dir / f"temp_chunk_{i}.wav"
        audio_chunk.export(temp_chunk_path, format="wav")
        
        processing_path = temp_chunk_path
        
        # 3.1 AI é™å™ª (å¦‚æœæ¨¡å‹åŠ è½½æˆåŠŸ)
        if denoiser_model:
            try:
                wav, sr = torchaudio.load(temp_chunk_path)
                wav = wav.cuda()
                with torch.no_grad():
                    denoised_wav = denoiser_model(wav[None])[0]
                denoised_chunk_path = temp_dir / f"denoised_chunk_{i}.wav"
                torchaudio.save(denoised_chunk_path, denoised_wav.cpu(), 16000)
                processing_path = denoised_chunk_path
            except Exception as e:
                log_system_event("warning", f"å½“å‰å—é™å™ªå¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹éŸ³é¢‘ã€‚é”™è¯¯: {e}")
        
        # 3.2 VAD è¯­éŸ³æ£€æµ‹
        try:
            from faster_whisper.audio import decode_audio
            from faster_whisper.vad import VadOptions, get_speech_timestamps
            
            vad_parameters = {
                "threshold": 0.4, 
                "min_speech_duration_ms": 250, 
                "max_speech_duration_s": 15.0, # ç¨å¾®æ”¾å®½ä»¥å®¹çº³é•¿å¥
                "min_silence_duration_ms": 1000, 
                "speech_pad_ms": 150
            }
            sampling_rate = 16000
            audio_data = decode_audio(str(processing_path), sampling_rate=sampling_rate)
            speech_timestamps = get_speech_timestamps(audio_data, vad_options=VadOptions(**vad_parameters))
            
            # 3.3 æ ¹æ® VAD ç»“æœä»åŸå§‹éŸ³é¢‘ä¸­ç²¾ç¡®åˆ‡ç‰‡
            for speech in speech_timestamps:
                relative_start_ms = int(speech["start"] / sampling_rate * 1000)
                relative_end_ms = int(speech["end"] / sampling_rate * 1000)
                absolute_start_ms = start_time_ms + relative_start_ms
                absolute_end_ms = start_time_ms + relative_end_ms
                
                final_chunk = original_audio[absolute_start_ms:absolute_end_ms]
                final_chunk_path = chunks_dir / f"chunk_{absolute_start_ms}.wav"
                final_chunk.export(str(final_chunk_path), format="wav")
                chunk_files.append({
                    "path": str(final_chunk_path),
                    "start_ms": absolute_start_ms,
                    "end_ms": absolute_end_ms
                })
        except Exception as e:
            log_system_event("error", f"å½“å‰å— VAD å¤„ç†å¤±è´¥: {e}")
    
    log_system_event("info", f"éŸ³é¢‘åˆ†å—å¤„ç†å®Œæˆï¼Œæ€»å…±åˆ‡åˆ†ä¸º {len(chunk_files)} ä¸ªæœ‰æ•ˆè¯­éŸ³ç‰‡æ®µã€‚")
    return chunk_files

# --- D. AI äº¤äº’ä¸å¹¶å‘è°ƒåº¦ ---

def _process_subtitle_batch_with_ai(
    chunk_group: list[dict],
    group_index: int,
    api_key: str,
    gemini_endpoint_prefix: str,  # <<< æ–°å¢å‚æ•°
    system_instruction: str,
    prompt_for_task: str
) -> list[dict]:
    #
    # å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„éŸ³é¢‘å—ï¼Œè°ƒç”¨ Gemini API è·å–å­—å¹•ã€‚
    # è¿™æ˜¯ä¸€ä¸ªå†…éƒ¨å‡½æ•°ï¼Œç”±ä¸»è°ƒåº¦å™¨è°ƒç”¨ã€‚
    #
    thread_local_srt_list = []
    log_system_event("info", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å·²å¯åŠ¨...")
    try:
        # 1. æ„å»º REST API è¯·æ±‚ä½“ (payload)
        model_name = "gemini-1.5-flash-latest"
        
        # <<< æ ¸å¿ƒä¿®æ”¹ï¼šåŠ¨æ€æ„å»º URL >>>
        generate_url = f"{gemini_endpoint_prefix.rstrip('/')}/v1beta/models/{model_name}:generateContent"
        
        headers = {"x-goog-api-key": api_key, "Content-Type": "application/json"}
        
        parts = [{"text": prompt_for_task}]
        for chunk in chunk_group:
            encoded_data = read_and_encode_file_base64(chunk["path"])
            if not encoded_data:
                log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ–‡ä»¶ {chunk['path']} ç¼–ç å¤±è´¥ï¼Œè·³è¿‡æ­¤æ–‡ä»¶ã€‚")
                continue
            parts.append({"text": f"[AUDIO_INFO] {chunk['start_ms']} --> {chunk['end_ms']}"})
            parts.append({"inlineData": {"mime_type": "audio/wav", "data": encoded_data}})
        
        if len(parts) <= 1:
            log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ•´ä¸ªæ‰¹æ¬¡å‡æ— æ³•ç¼–ç ï¼Œæ”¾å¼ƒã€‚")
            return []

        payload = {
            "contents": [{"parts": parts}],
            "systemInstruction": {"parts": [{"text": system_instruction}]},
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
            ],
            "generationConfig": {"responseMimeType": "application/json"}
        }

        # 2. å‘é€è¯·æ±‚å¹¶å®ç°å…¨é¢çš„é‡è¯•é€»è¾‘
        log_system_event("info", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ•°æ®å‡†å¤‡å®Œæ¯•ï¼Œæ­£åœ¨è°ƒç”¨ Gemini API at {generate_url}...")
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(generate_url, headers=headers, json=payload, timeout=1000)
                
                if response.status_code in [429, 500, 503, 504]:
                    log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] é‡åˆ°å¯é‡è¯•çš„ API é”™è¯¯ (HTTP {response.status_code}, å°è¯• {attempt + 1}/{max_retries})ã€‚")
                    if attempt < max_retries - 1:
                        wait_time = 5 * (2 ** attempt)
                        log_system_event("info", f"{wait_time}ç§’åå°†è‡ªåŠ¨é‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        response.raise_for_status()

                response.raise_for_status()
                response_data = response.json()
                
                candidates = response_data.get("candidates")
                if not candidates:
                    raise ValueError(f"APIå“åº”ä¸­ç¼ºå°‘ 'candidates' å­—æ®µã€‚å“åº”: {response.text}")

                content = candidates[0].get("content")
                if not content:
                    finish_reason = candidates[0].get("finishReason", "æœªçŸ¥")
                    safety_ratings = candidates[0].get("safetyRatings", [])
                    raise ValueError(f"APIå“åº”å†…å®¹ä¸ºç©º(å¯èƒ½è¢«æ‹¦æˆª)ã€‚åŸå› : {finish_reason}, å®‰å…¨è¯„çº§: {safety_ratings}")

                json_text = content.get("parts", [{}])[0].get("text")
                if json_text is None:
                    raise ValueError(f"APIå“åº”çš„ 'parts' ä¸­ç¼ºå°‘ 'text' å­—æ®µã€‚å“åº”: {response.text}")

                parsed_result = BatchTranscriptionResult.model_validate_json(json_text)
                subtitles_count = len(parsed_result.subtitles)
                log_system_event("info", f"âœ… [å­—å¹•ä»»åŠ¡ {group_index+1}] æˆåŠŸï¼è·å¾— {subtitles_count} æ¡å­—å¹•ã€‚")
                
                for i, subtitle in enumerate(parsed_result.subtitles):
                    if subtitle.end_ms is None:
                        if i + 1 < subtitles_count:
                            subtitle.end_ms = parsed_result.subtitles[i+1].start_ms
                        else:
                            subtitle.end_ms = chunk_group[-1]['end_ms']
                    if subtitle.start_ms > subtitle.end_ms:
                        subtitle.end_ms = subtitle.start_ms + 250
                    
                    line = f"{ms_to_srt_time(subtitle.start_ms)} --> {ms_to_srt_time(subtitle.end_ms)}\n{subtitle.text.strip()}\n"
                    thread_local_srt_list.append({"start_ms": subtitle.start_ms, "srt_line": line})
                
                return thread_local_srt_list

            except requests.exceptions.RequestException as e:
                log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1: time.sleep(5 * (2 ** attempt))
            except pydantic.ValidationError as e:
                log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] AIè¿”å›çš„JSONæ ¼å¼éªŒè¯å¤±è´¥ï¼Œæ”¾å¼ƒã€‚é”™è¯¯: {e}")
                return []
            except Exception as e:
                log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œæ”¾å¼ƒã€‚é”™è¯¯: {e}")
                return []
        
        raise RuntimeError(f"æ‰¹æ¬¡ {group_index+1} åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥ã€‚")

    except Exception as e:
        log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
    
    return thread_local_srt_list


def run_subtitle_extraction_pipeline(subtitle_config: dict, chunk_files: list[dict], update_status_callback: callable) -> str:
    #
    # ä¸»è°ƒåº¦å™¨ï¼Œè´Ÿè´£å¹¶å‘å¤„ç†æ‰€æœ‰éŸ³é¢‘æ‰¹æ¬¡å¹¶ç”Ÿæˆæœ€ç»ˆçš„ SRT å†…å®¹ã€‚
    #
    
    # <<< æ ¸å¿ƒä¿®æ”¹ï¼šä»é…ç½®ä¸­è·å–æ‰€æœ‰éœ€è¦çš„å˜é‡ >>>
    api_keys = subtitle_config.get('GEMINI_API_KEYS', [])
    prompt_api_url = subtitle_config.get('PROMPT_API_URL', '')
    gemini_endpoint_prefix = subtitle_config.get('GEMINI_API_ENDPOINT_PREFIX', '')
    
    if not all([api_keys, prompt_api_url, gemini_endpoint_prefix]):
        raise ValueError("å­—å¹•é…ç½®ä¸­ç¼ºå°‘ 'GEMINI_API_KEYS', 'PROMPT_API_URL', æˆ– 'GEMINI_API_ENDPOINT_PREFIX'ã€‚")
    
    system_instruction, prompt_for_task = get_dynamic_prompts(prompt_api_url)

    total_chunks = len(chunk_files)
    if total_chunks == 0:
        log_system_event("warning", "æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆçš„è¯­éŸ³ç‰‡æ®µï¼Œæ— æ³•ç”Ÿæˆå­—å¹•ã€‚")
        return ""

    chunk_groups = [chunk_files[i:i + SUBTITLE_BATCH_SIZE] for i in range(0, total_chunks, SUBTITLE_BATCH_SIZE)]
    num_groups = len(chunk_groups)
    log_system_event("info", f"å·²å°†è¯­éŸ³ç‰‡æ®µåˆ†ä¸º {num_groups} ä¸ªæ‰¹æ¬¡ï¼Œå‡†å¤‡å¹¶å‘å¤„ç†ã€‚")
    update_status_callback(stage="subtitle_transcribing", details=f"å‡†å¤‡å¤„ç† {num_groups} ä¸ªå­—å¹•æ‰¹æ¬¡...")
    
    all_srt_blocks = []
    
    with ThreadPoolExecutor(max_workers=SUBTITLE_CONCURRENT_REQUESTS) as executor:
        futures = []
        delay_between_submissions = 60.0 / SUBTITLE_REQUESTS_PER_MINUTE
        
        for i, group in enumerate(chunk_groups):
            api_key_for_thread = api_keys[i % len(api_keys)]
            future = executor.submit(
                _process_subtitle_batch_with_ai,
                group,
                i,
                api_key_for_thread,
                gemini_endpoint_prefix, # <<< æ–°å¢å‚æ•°ä¼ é€’
                system_instruction,
                prompt_for_task
            )
            futures.append(future)
            if i < num_groups - 1:
                time.sleep(delay_between_submissions)
        
        for i, future in enumerate(futures):
            try:
                result = future.result()
                if result:
                    all_srt_blocks.extend(result)
                update_status_callback(stage="subtitle_transcribing", details=f"å·²å®Œæˆ {i+1}/{num_groups} ä¸ªå­—å¹•æ‰¹æ¬¡...")
            except Exception as e:
                log_system_event("error", f"ä¸€ä¸ªå­—å¹•çº¿ç¨‹ä»»åŠ¡åœ¨è·å–ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")

    log_system_event("info", "æ‰€æœ‰å¹¶å‘ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ­£åœ¨æ•´åˆå­—å¹•...")
    
    all_srt_blocks.sort(key=lambda x: x["start_ms"])
    final_srt_lines = [f"{i + 1}\n{block['srt_line']}" for i, block in enumerate(all_srt_blocks)]
    
    return "\n".join(final_srt_lines)

# =============================================================================
# --- ç¬¬ 5 æ­¥: MixFileCLI å®¢æˆ·ç«¯ ---
# =============================================================================

class MixFileCLIClient:
    # ä¸€ä¸ªç®€å•çš„ç”¨äºä¸ MixFileCLI åç«¯ API äº¤äº’çš„å®¢æˆ·ç«¯ã€‚
    def __init__(self, base_url: str):
        if not base_url.startswith("http"):
            raise ValueError("Base URL å¿…é¡»ä»¥ http æˆ– https å¼€å¤´")
        self.base_url = base_url
        self.session = requests.Session()

    def _make_request(self, method: str, url: str, **kwargs):
        # ç»Ÿä¸€çš„è¯·æ±‚å‘é€æ–¹æ³•ï¼ŒåŒ…å«é”™è¯¯å¤„ç†ã€‚
        try:
            # ç¡®ä¿è¯·æ±‚ä¸ä¼šè¢«ç¼“å­˜
            headers = kwargs.get('headers', {})
            headers.update({'Cache-Control': 'no-cache', 'Pragma': 'no-cache'})
            kwargs['headers'] = headers
            
            response = self.session.request(method, url, timeout=300, **kwargs)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            error_text = str(e)
            status_code = 500 # é»˜è®¤ä¸ºé€šç”¨æœåŠ¡å™¨é”™è¯¯
            if e.response is not None:
                error_text = e.response.text
                status_code = e.response.status_code
            return (status_code, error_text)

    def upload_file(self, local_file_path: str, progress_callback: callable = None):
        #
        # ä¸Šä¼ å•ä¸ªæ–‡ä»¶å¹¶è·å–åˆ†äº«ç ã€‚
        #
        # Args:
        #     local_file_path (str): æœ¬åœ°æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
        #     progress_callback (callable, optional): è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (bytes_uploaded, total_bytes)ã€‚
        #
        # Returns:
        #     requests.Response or tuple: æˆåŠŸæ—¶è¿”å› Response å¯¹è±¡ï¼Œå¤±è´¥æ—¶è¿”å› (status_code, error_text)ã€‚
        #
        filename = os.path.basename(local_file_path)
        upload_url = urljoin(self.base_url, f"/api/upload/{quote(filename)}")
        file_size = os.path.getsize(local_file_path)

        def file_reader_generator(file_handle):
            chunk_size = 1 * 1024 * 1024 # 1MB chunk
            bytes_read = 0
            while True:
                chunk = file_handle.read(chunk_size)
                if not chunk:
                    if progress_callback: # ç¡®ä¿æœ€å100%è¢«è°ƒç”¨
                        progress_callback(file_size, file_size)
                    break
                bytes_read += len(chunk)
                if progress_callback:
                    progress_callback(bytes_read, file_size)
                yield chunk

        with open(local_file_path, 'rb') as f:
            return self._make_request("PUT", upload_url, data=file_reader_generator(f))

# =============================================================================
# --- ç¬¬ 6 æ­¥: ç»Ÿä¸€çš„ Flask API æœåŠ¡ ---
# =============================================================================

# --- A. åº”ç”¨åˆå§‹åŒ–ä¸ä»»åŠ¡ç®¡ç† ---
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False

# ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡çš„çŠ¶æ€ï¼Œå¹¶ç”¨é”æ¥ä¿è¯çº¿ç¨‹å®‰å…¨
tasks = {}
tasks_lock = threading.Lock()

# å…¨å±€å˜é‡ï¼Œå°†åœ¨ main å‡½æ•°ä¸­è¢«åˆå§‹åŒ–
api_client = None 
subtitle_config_global = {}


# --- B. API è·¯ç”±å®šä¹‰ ---

@app.route("/api/upload", methods=["POST"])
def unified_upload_endpoint():
    #
    # ç»Ÿä¸€çš„åª’ä½“å¤„ç†å…¥å£ APIã€‚
    # æ¥æ”¶ä¸€ä¸ª URLï¼Œå¹¶æ ¹æ®å‚æ•°å†³å®šæ˜¯ä¸Šä¼ æ–‡ä»¶ã€æå–å­—å¹•ï¼Œè¿˜æ˜¯ä¸¤è€…éƒ½åšã€‚
    # å§‹ç»ˆä»¥å¼‚æ­¥æ¨¡å¼è¿è¡Œã€‚
    #
    data = request.get_json()
    if not data or "url" not in data:
        return jsonify({"error": "è¯·æ±‚ä½“ä¸­å¿…é¡»åŒ…å« 'url' å­—æ®µ"}), 400

    task_id = str(uuid.uuid4())
    
    # ä»è¯·æ±‚ä¸­æå–å‚æ•°ï¼Œå¹¶è®¾ç½®é»˜è®¤å€¼
    request_params = {
        "url": data["url"],
        "extract_subtitle": data.get("extract_subtitle", False),
        "upload_video": data.get("upload_video", True),
        "upload_subtitle": data.get("upload_subtitle", False),
    }

    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    with tasks_lock:
        tasks[task_id] = {
            "task_id": task_id,
            "status": "pending",
            "stage": "queue",
            "details": "ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…å¤„ç†",
            "progress": 0,
            "result": None
        }

    # åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨çœŸæ­£çš„ä»»åŠ¡å¤„ç†å™¨
    threading.Thread(
        target=process_unified_task,
        args=(task_id, request_params)
    ).start()
    
    log_system_event("info", f"å·²åˆ›å»ºæ–°ä»»åŠ¡ {task_id}")

    return jsonify({
        "task_id": task_id,
        "status_url": f"/api/tasks/{task_id}"
    }), 202


@app.route("/api/tasks/<task_id>", methods=["GET"])
def get_task_status_endpoint(task_id):
    # è·å–æŒ‡å®šä»»åŠ¡çš„å½“å‰çŠ¶æ€å’Œç»“æœã€‚
    with tasks_lock:
        task = tasks.get(task_id)
    
    if task:
        return jsonify(task)
    else:
        return jsonify({"error": "æœªæ‰¾åˆ°æŒ‡å®šçš„ task_id"}), 404


@app.route('/killer_status_frp', methods=['GET'])
def health_status_endpoint():
    # ç”¨äºå¤–éƒ¨å¥åº·æ£€æŸ¥çš„ç®€å•ç«¯ç‚¹ã€‚
    return jsonify({
        "status": "ok",
        "message": "ç»Ÿä¸€åª’ä½“å¤„ç† API æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶é€šè¿‡ FRP æš´éœ²ã€‚",
        "timestamp": time.time()
    }), 200


@app.route('/force_shutdown_notebook', methods=['GET'])
def force_shutdown_endpoint():
    #
    # è¿œç¨‹å…³é—­ APIï¼Œæ¥æ”¶ä¸€ä¸ª token è¿›è¡ŒéªŒè¯ã€‚
    #
    log_system_event("info", "API /force_shutdown_notebook è¢«è°ƒç”¨...")
    token_from_request = request.args.get('token')

    if token_from_request != KILLER_API_SHUTDOWN_TOKEN:
        log_system_event("error", "API Auth å¤±è´¥: Token æ— æ•ˆã€‚")
        return jsonify({"status": "error", "message": "Unauthorized"}), 401
    
    log_system_event("info", "API Auth æˆåŠŸã€‚æ­£åœ¨å®‰æ’åå°å…³é—­ä»»åŠ¡...")

    def delayed_full_shutdown():
        # Step 1: å…ˆå¹³æ»‘åœ°æ€æ­»å­è¿›ç¨‹
        log_system_event("info", "åå°å…³é—­ä»»åŠ¡ï¼šå¼€å§‹ç»ˆæ­¢å­è¿›ç¨‹...")
        _find_and_kill_targeted_processes(signal.SIGTERM)
        time.sleep(2)
        
        # Step 2: å¼ºåˆ¶ç»ˆæ­¢ä»ç„¶å­˜åœ¨çš„å­è¿›ç¨‹
        log_system_event("info", "åå°å…³é—­ä»»åŠ¡ï¼šå¼ºåˆ¶ç»ˆæ­¢ä»»ä½•æ®‹ç•™å­è¿›ç¨‹...")
        _find_and_kill_targeted_processes(signal.SIGKILL)
        time.sleep(1)

        # Step 3: ç»ˆæ­¢ä¸»å†…æ ¸
        _shutdown_notebook_kernel_immediately()
    
    # ç«‹å³å¯åŠ¨åå°çº¿ç¨‹ï¼Œç„¶åç«‹åˆ»è¿”å›å“åº”ï¼Œç¡®ä¿è°ƒç”¨æ–¹æ”¶åˆ°ç¡®è®¤
    threading.Thread(target=delayed_full_shutdown, daemon=True).start()
    
    return jsonify({
        "status": "shutdown_initiated",
        "message": "å…³é—­ä¿¡å·å·²æ¥æ”¶ã€‚åå°æ­£åœ¨æ‰§è¡Œæ¸…ç†å’Œå†…æ ¸å…³é—­æ“ä½œã€‚",
    }), 200

# =============================================================================
# --- ç¬¬ 7 æ­¥: é«˜å®¹é”™çš„ç»Ÿä¸€ä»»åŠ¡å¤„ç†å™¨ ---
# =============================================================================

def process_unified_task(task_id: str, params: dict):
    #
    # å¤„ç†ä¸€ä¸ªå®Œæ•´çš„åª’ä½“ä»»åŠ¡ï¼ŒåŒ…å«ä¸‹è½½ã€å­—å¹•æå–å’Œä¸Šä¼ ç­‰æ­¥éª¤ã€‚
    # è¿™ä¸ªå‡½æ•°å®ç°äº†æ–¹æ¡ˆ2çš„æ ¸å¿ƒé€»è¾‘ï¼šå­ä»»åŠ¡çš„ç‹¬ç«‹å¤±è´¥å¤„ç†ã€‚
    #
    
    # --- 1. åˆå§‹åŒ– ---
    temp_dir = Path(f"/kaggle/working/task_{task_id}")
    temp_dir.mkdir(exist_ok=True)
    
    # å®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•°æ¥æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼Œç®€åŒ–ä»£ç 
    def _update_status(status, stage, details, progress=None):
        with tasks_lock:
            tasks[task_id]['status'] = status
            tasks[task_id]['stage'] = stage
            tasks[task_id]['details'] = details
            if progress is not None:
                tasks[task_id]['progress'] = progress
    
    # åˆå§‹åŒ–æœ€ç»ˆç»“æœç»“æ„
    final_result = {
        "video_file": {"status": "pending"},
        "subtitle_file": {"status": "pending"}
    }

    try:
        # --- 2. å­ä»»åŠ¡: ä¸‹è½½æºæ–‡ä»¶ ---
        _update_status("running", "download", "å‡†å¤‡ä» URL ä¸‹è½½æ–‡ä»¶...", 0)
        
        file_url = params['url']
        # ä»URLçŒœæµ‹æ–‡ä»¶åï¼Œå¹¶è¿›è¡Œæ¸…ç†
        filename_encoded = file_url.split("/")[-1].split("?")[0] or f"downloaded_file_{task_id}"
        filename = unquote(filename_encoded)
        local_file_path = temp_dir / filename
        final_result["video_file"]["filename"] = filename

        bytes_downloaded = 0
        with requests.get(file_url, stream=True, timeout=60) as r:
            r.raise_for_status()
            total_size = int(r.headers.get('content-length', 0))
            with open(local_file_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
                    bytes_downloaded += len(chunk)
                    if total_size > 0:
                        progress = round((bytes_downloaded / total_size) * 100, 2)
                        _update_status("running", "download", f"å·²ä¸‹è½½ {bytes_downloaded}/{total_size} å­—èŠ‚", progress)
        
        # --- 3. å­ä»»åŠ¡: åª’ä½“ç±»å‹éªŒè¯ ---
        _update_status("running", "validate", "æ­£åœ¨éªŒè¯æ–‡ä»¶ç±»å‹...", 100)
        is_video = False
        mime_type, _ = mimetypes.guess_type(local_file_path)
        if mime_type and mime_type.startswith("video"):
            is_video = True
            log_system_event("info", f"æ–‡ä»¶ {filename} è¢«è¯†åˆ«ä¸ºè§†é¢‘ ({mime_type})ã€‚")
        else:
            log_system_event("warning", f"æ–‡ä»¶ {filename} ä¸æ˜¯æ ‡å‡†è§†é¢‘æ ¼å¼ ({mime_type})ã€‚")

        # --- 4. å­ä»»åŠ¡: å­—å¹•æå– ---
        srt_content = None
        if params["extract_subtitle"]:
            if is_video:
                try:
                    _update_status("running", "subtitle_extraction", "å­—å¹•æå–æµç¨‹å·²å¯åŠ¨...", 0)
                    # è°ƒç”¨å®Œæ•´çš„å­—å¹•æå–ç®¡é“
                    audio_chunks = preprocess_audio_for_subtitles(local_file_path, temp_dir, 
                        lambda stage, details: _update_status("running", stage, details))
                    srt_content = run_subtitle_extraction_pipeline(subtitle_config_global, audio_chunks, 
                        lambda stage, details: _update_status("running", stage, details))
                    
                    if srt_content:
                        srt_filename = local_file_path.stem + ".srt"
                        final_result["subtitle_file"]["filename"] = srt_filename
                        final_result["subtitle_file"]["status"] = "success"
                        
                        # è¿”å›Base64ç¼–ç çš„SRTæ–‡ä»¶
                        srt_base64 = base64.b64encode(srt_content.encode('utf-8')).decode('utf-8')
                        final_result["subtitle_file"]["content_base64"] = srt_base64
                        
                        # å°†SRTæ–‡ä»¶ä¿å­˜åˆ°æœ¬åœ°ä»¥å¤‡ä¸Šä¼ 
                        with open(temp_dir / srt_filename, "w", encoding="utf-8") as f:
                            f.write(srt_content)
                    else:
                        raise RuntimeError("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³ï¼Œæ— æ³•ç”Ÿæˆå­—å¹•ã€‚")

                except Exception as e:
                    log_system_event("error", f"ä»»åŠ¡ {task_id} çš„å­—å¹•æå–å¤±è´¥: {e}")
                    final_result["subtitle_file"]["status"] = "failed"
                    final_result["subtitle_file"]["details"] = f"å­—å¹•æå–å¤±è´¥: {str(e)}"
            else:
                final_result["subtitle_file"]["status"] = "skipped"
                final_result["subtitle_file"]["details"] = "æºæ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„è§†é¢‘æ ¼å¼"
        else:
            final_result["subtitle_file"]["status"] = "skipped"
            final_result["subtitle_file"]["details"] = "æœªè¯·æ±‚æå–å­—å¹•"

        # --- 5. å­ä»»åŠ¡: æ–‡ä»¶ä¸Šä¼  (ä½¿ç”¨å¹¶å‘) ---
        _update_status("running", "uploading", "å‡†å¤‡ä¸Šä¼ æ–‡ä»¶åˆ° MixFile...", 0)
        
        upload_tasks = []
        with ThreadPoolExecutor(max_workers=2) as executor:
            # æäº¤è§†é¢‘ä¸Šä¼ ä»»åŠ¡
            if params["upload_video"]:
                upload_tasks.append(executor.submit(api_client.upload_file, str(local_file_path)))
            else:
                final_result["video_file"]["status"] = "skipped"

            # æäº¤å­—å¹•ä¸Šä¼ ä»»åŠ¡
            if params["upload_subtitle"] and srt_content:
                srt_path = temp_dir / final_result["subtitle_file"]["filename"]
                upload_tasks.append(executor.submit(api_client.upload_file, str(srt_path)))
        
        # å¤„ç†ä¸Šä¼ ç»“æœ
        for future in upload_tasks:
            try:
                # å‡è®¾ upload_file æˆåŠŸæ—¶è¿”å› Responseï¼Œå¤±è´¥æ—¶è¿”å› tuple
                response = future.result()
                
                # ä»å“åº”å¤´ä¸­è§£æå‡ºä¸Šä¼ çš„æ˜¯å“ªä¸ªæ–‡ä»¶ (éœ€è¦MixFileCLIæ”¯æŒæˆ–é€šè¿‡å…¶ä»–æ–¹å¼åˆ¤æ–­)
                # ç®€åŒ–å¤„ç†ï¼šæˆ‘ä»¬é€šè¿‡æ–‡ä»¶æ‰©å±•åæ¥åˆ¤æ–­
                # è¿™æ˜¯ä¸€ä¸ªä¸å®Œç¾çš„å‡è®¾ï¼Œä½†å¯¹äºå½“å‰åœºæ™¯è¶³å¤Ÿ
                is_srt_upload = False
                if hasattr(response, 'request') and response.request is not None and response.request.url is not None:
                     if ".srt" in unquote(response.request.url):
                        is_srt_upload = True

                if isinstance(response, requests.Response) and response.ok:
                    share_code = response.text.strip()
                    if is_srt_upload:
                        target_result = final_result["subtitle_file"]
                    else:
                        target_result = final_result["video_file"]
                    
                    target_result["status"] = "success"
                    target_result["share_code"] = share_code
                    target_result["share_link"] = f"http://{FRP_SERVER_ADDR}:{MIXFILE_REMOTE_PORT}/api/download/{quote(target_result['filename'])}?s={share_code}"
                else:
                    status_code, error_text = response if isinstance(response, tuple) else (response.status_code, response.text)
                    raise RuntimeError(f"ä¸Šä¼ å¤±è´¥ã€‚çŠ¶æ€ç : {status_code}, é”™è¯¯: {error_text}")

            except Exception as e:
                # è¿™é‡Œæ•è·ä¸Šä¼ å¤±è´¥ï¼Œä½†æˆ‘ä»¬ä¸çŸ¥é“æ˜¯å“ªä¸ªæ–‡ä»¶å¤±è´¥äº†
                # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å¦¥åï¼Œå®é™…ç”Ÿäº§ç³»ç»Ÿéœ€è¦æ›´å¤æ‚çš„è¿½è¸ª
                log_system_event("error", f"ä»»åŠ¡ {task_id} çš„æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
                # å°†ä¸¤ä¸ªå¾…å®šä¸Šä¼ éƒ½æ ‡è®°ä¸ºå¤±è´¥
                if final_result["video_file"]["status"] == "pending":
                    final_result["video_file"]["status"] = "failed"
                    final_result["video_file"]["details"] = str(e)
                if final_result["subtitle_file"].get("status") == "success" and "share_code" not in final_result["subtitle_file"]:
                    final_result["subtitle_file"]["status"] = "upload_failed"
                    final_result["subtitle_file"]["details"] = str(e)

        # --- 6. ä»»åŠ¡å®Œæˆ ---
        with tasks_lock:
            tasks[task_id]['status'] = 'success'
            tasks[task_id]['stage'] = 'completed'
            tasks[task_id]['details'] = 'ä»»åŠ¡å¤„ç†å®Œæˆ'
            tasks[task_id]['progress'] = 100
            tasks[task_id]['result'] = final_result
            
    except Exception as e:
        log_system_event("error", f"å¤„ç†ä»»åŠ¡ {task_id} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        _update_status("failed", "error", str(e))
        with tasks_lock:
             tasks[task_id]['result'] = final_result # å³ä½¿å¤±è´¥ä¹Ÿè¿”å›éƒ¨åˆ†ç»“æœ
    finally:
        # --- 7. æ¸…ç†ä¸´æ—¶æ–‡ä»¶ ---
        try:
            shutil.rmtree(temp_dir)
            log_system_event("info", f"å·²æ¸…ç†ä»»åŠ¡ {task_id} çš„ä¸´æ—¶ç›®å½•ã€‚")
        except Exception as e:
            log_system_event("error", f"æ¸…ç†ä»»åŠ¡ {task_id} çš„ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")


# =============================================================================
# --- ç¬¬ 8 æ­¥: ä¸»ç¨‹åºä¸æœåŠ¡å¯åŠ¨ ---
# =============================================================================

def main():
    #
    # ä¸»æ‰§è¡Œå‡½æ•°ï¼Œè´Ÿè´£åˆå§‹åŒ–å’Œå¯åŠ¨æ‰€æœ‰æœåŠ¡ã€‚
    #
    global api_client, subtitle_config_global, FRP_SERVER_ADDR, MIXFILE_REMOTE_PORT
    
    try:
        # --- 1. å¯åŠ¨å‰å‡†å¤‡ ---
        log_system_event("info", "æœåŠ¡æ­£åœ¨å¯åŠ¨...")
        run_command("pip install -q pydantic pydub faster-whisper@https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz denoiser google-generativeai").wait()
        check_environment()

        # --- 2. è§£å¯†é…ç½® ---
        frp_config = get_decrypted_config(ENCRYPTED_FRP_CONFIG, "FRP")
        subtitle_config_global = get_decrypted_config(ENCRYPTED_SUBTITLE_CONFIG, "Subtitle")
        
        FRP_SERVER_ADDR = frp_config['FRP_SERVER_ADDR']
        FRP_SERVER_PORT = frp_config['FRP_SERVER_PORT']
        FRP_TOKEN = frp_config['FRP_TOKEN']
        
        # --- 3. åˆå§‹åŒ– MixFile å®¢æˆ·ç«¯ ---
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ localhostï¼Œå› ä¸º Flask å’Œ MixFileCLI éƒ½åœ¨ Kaggle å®¹å™¨å†…è¿è¡Œ
        api_client_base_url = f"http://127.0.0.1:{MIXFILE_LOCAL_PORT}"
        api_client = MixFileCLIClient(base_url=api_client_base_url)

        # --- 4. å¯åŠ¨ MixFileCLI æœåŠ¡ ---
        log_system_event("info", "æ­£åœ¨åˆ›å»º MixFileCLI config.yml...")
        with open("config.yml", "w") as f: f.write(mixfile_config_yaml)
        
        log_system_event("info", "æ­£åœ¨ä¸‹è½½å¹¶å¯åŠ¨ MixFileCLI...")
        if not os.path.exists("mixfile-cli.jar"):
            run_command("wget -q --show-progress https://github.com/HelloWorldWinning/mixfile-cli/releases/download/2.0.1/mixfile-cli-2.0.1.jar -O mixfile-cli.jar").wait()
        run_command("java -jar mixfile-cli.jar", "mixfile.log")
        if not wait_for_port(MIXFILE_LOCAL_PORT):
            raise RuntimeError("MixFileCLI æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ mixfile.logã€‚")

        # --- 5. å¯åŠ¨ Flask API æœåŠ¡ ---
        def run_flask_app():
            app.run(host='0.0.0.0', port=FLASK_API_LOCAL_PORT, debug=False, use_reloader=False)
        log_system_event("info", "æ­£åœ¨åå°å¯åŠ¨ Flask API æœåŠ¡...")
        threading.Thread(target=run_flask_app, daemon=True).start()
        time.sleep(3) # ç­‰å¾… Flask å¯åŠ¨

        # --- 6. å¯åŠ¨ frpc å®¢æˆ·ç«¯ ---
        log_system_event("info", "æ­£åœ¨å‡†å¤‡ frpc å®¢æˆ·ç«¯...")
        if not os.path.exists("/kaggle/working/frpc"):
            run_command("wget -q https://github.com/fatedier/frp/releases/download/v0.54.0/frp_0.54.0_linux_amd64.tar.gz && tar -zxvf frp_0.54.0_linux_amd64.tar.gz && mv frp_0.54.0_linux_amd64/frpc /kaggle/working/frpc && chmod +x /kaggle/working/frpc").wait()
        
        frpc_ini = f"""
[common]
server_addr = {FRP_SERVER_ADDR}
server_port = {FRP_SERVER_PORT}
token = {FRP_TOKEN}
log_file = /kaggle/working/frpc.log
[mixfile_webdav_{MIXFILE_REMOTE_PORT}]
type = tcp
local_ip = 127.0.0.1
local_port = {MIXFILE_LOCAL_PORT}
remote_port = {MIXFILE_REMOTE_PORT}
[flask_api_{FLASK_API_REMOTE_PORT}]
type = tcp
local_ip = 127.0.0.1
local_port = {FLASK_API_LOCAL_PORT}
remote_port = {FLASK_API_REMOTE_PORT}
"""
        with open('frpc.ini', 'w') as f: f.write(frpc_ini)
        run_command('/kaggle/working/frpc -c ./frpc.ini')
        log_system_event("info", "frpc å®¢æˆ·ç«¯å·²åœ¨åå°å¯åŠ¨ã€‚")
        time.sleep(3)
        
        # --- 7. æœ€ç»ˆçŠ¶æ€æŠ¥å‘Šä¸ä¿æ´» ---
        public_api_base_url = f"http://{FRP_SERVER_ADDR}:{FLASK_API_REMOTE_PORT}"
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æœåŠ¡å‡å·²æˆåŠŸå¯åŠ¨ï¼æ‚¨çš„ç»Ÿä¸€ API å·²ä¸Šçº¿ã€‚")
        print(f"  -> API å…¥å£ (POST) : {public_api_base_url}/api/upload")
        print(f"  -> ä»»åŠ¡çŠ¶æ€ (GET)  : {public_api_base_url}/api/tasks/<task_id>")
        print(f"  -> å¥åº·æ£€æŸ¥ (GET)  : {public_api_base_url}/killer_status_frp")
        print(f"  -> è¿œç¨‹å…³é—­ (GET)  : {public_api_base_url}/force_shutdown_notebook?token={KILLER_API_SHUTDOWN_TOKEN}")
        print("="*60)
        
        while True:
            time.sleep(300)
            log_system_event("info", f"æœåŠ¡æŒç»­è¿è¡Œä¸­... ({time.ctime()})")
            
    except (DecryptionError, RuntimeError, ValueError) as e:
        log_system_event("critical", f"ç¨‹åºå¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
        log_system_event("critical", "æœåŠ¡æ— æ³•å¯åŠ¨ï¼Œç¨‹åºå°†ç»ˆæ­¢ã€‚")
    except KeyboardInterrupt:
        log_system_event("info", "æœåŠ¡å·²æ‰‹åŠ¨åœæ­¢ã€‚")
    except Exception as e:
        log_system_event("critical", f"å‘ç”ŸæœªçŸ¥çš„è‡´å‘½é”™è¯¯: {e}")

if __name__ == '__main__':
    main()