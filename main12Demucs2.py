# =============================================================================
#         Kaggle æŒä¹…åŒ–åª’ä½“å¤„ç†æœåŠ¡ - ä¸»åº”ç”¨ v2.1 (æ–¹æ¡ˆ2 - å¤šè¿›ç¨‹éš”ç¦»ç‰ˆ)
# =============================================================================
#
# åŠŸèƒ½:
#   - å¯åŠ¨ MixFileCLI ä½œä¸ºæ–‡ä»¶å­˜å‚¨åç«¯ã€‚
#   - æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ Flask APIï¼Œç”¨äº:
#     1. ä» URL ä¸‹è½½æ–‡ä»¶å¹¶ä¸Šä¼ åˆ° MixFileã€‚
#     2. ä»è§†é¢‘ URL ä¸­æå–é«˜è´¨é‡å­—å¹• (å¯é€‰)ã€‚
#     3. å°†åŸå§‹è§†é¢‘å’Œç”Ÿæˆçš„å­—å¹•çµæ´»åœ°ä¸Šä¼ åˆ° MixFileã€‚
#   - é€šè¿‡ FRP å°†å†…éƒ¨æœåŠ¡å®‰å…¨åœ°æš´éœ²åˆ°å…¬ç½‘ã€‚
#   - æ‰€æœ‰æ•æ„Ÿé…ç½® (FRP, APIå¯†é’¥) å‡é€šè¿‡åŠ å¯†æ–¹å¼ç®¡ç†ã€‚
#   - å®ç°é«˜å®¹é”™çš„ä»»åŠ¡å¤„ç†é€»è¾‘ï¼Œå­ä»»åŠ¡å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹ã€‚
#   - **V2.1 å˜æ›´: é‡‡ç”¨å¤šè¿›ç¨‹æ¶æ„ï¼Œå°†é‡é‡çº§çš„åª’ä½“å¤„ç†ä»»åŠ¡æ”¾åˆ°ç‹¬ç«‹çš„å­è¿›ç¨‹
#     ä¸­æ‰§è¡Œï¼Œä»¥è§£å†³ä¸»è¿›ç¨‹ä¸­å› æœåŠ¡ç¯å¢ƒå†²çªå¯¼è‡´çš„AIæ¨¡å‹åŠ è½½å¤±è´¥é—®é¢˜ã€‚**
#
# =============================================================================

# --- æ ¸å¿ƒ Python åº“ ---
import os
import subprocess
import threading
import time
import uuid
import socket
import json
import base64
import hashlib
import signal
import sys
import shutil
import mimetypes
from pathlib import Path
from datetime import timedelta
from urllib.parse import urljoin, quote, unquote
from concurrent.futures import ThreadPoolExecutor
# --- å¤šè¿›ç¨‹ä¸é˜Ÿåˆ— ---
import multiprocessing
from queue import Empty as QueueEmpty

# --- Web æ¡†æ¶ä¸ HTTP å®¢æˆ·ç«¯ ---
from flask import Flask, request, jsonify, Response
import requests



# =============================================================================
# --- æ–°å¢ï¼šé«˜çº§éŸ³é¢‘å¤„ç†åº“å¯¼å…¥ ---
# =============================================================================
try:
    from demucs.separate import S
    import tensorflow as tf
    import tensorflow_hub as hub
    from speechbrain.pretrained import SepformerLSTMMaskNet as SpeechEnhancer
except ImportError as e:
    print(f"[è­¦å‘Š] é¢„åŠ è½½é«˜çº§éŸ³é¢‘å¤„ç†åº“å¤±è´¥: {e}ã€‚è¿™äº›åº“å°†åœ¨è¿è¡Œæ—¶å®‰è£…ã€‚")
    S, tf, hub, SpeechEnhancer = None, None, None, None # ä¿æŒå•è¡Œèµ‹å€¼
# =============================================================================



# --- åŠ å¯†ä¸å¯†é’¥ç®¡ç† ---
# å°è¯•å¯¼å…¥å…³é”®åº“ï¼Œå¦‚æœå¤±è´¥åˆ™åœ¨åç»­æ£€æŸ¥ä¸­å¤„ç†
try:
    from kaggle_secrets import UserSecretsClient
    from cryptography.fernet import Fernet
except ImportError:
    UserSecretsClient = None
    Fernet = None

# --- å­—å¹•æå–ç›¸å…³åº“ (å°†åœ¨ check_environment ä¸­éªŒè¯) ---
try:
    import torch
    import torchaudio
    from pydub import AudioSegment
    import pydantic
except ImportError as e:
    # å…è®¸å¯åŠ¨æ—¶å¯¼å…¥å¤±è´¥ï¼Œåç»­ç¯å¢ƒæ£€æŸ¥ä¼šæ•è·
    print(f"[è­¦å‘Š] é¢„åŠ è½½éƒ¨åˆ†åº“å¤±è´¥: {e}ã€‚å°†åœ¨ç¯å¢ƒæ£€æŸ¥ä¸­ç¡®è®¤ã€‚")
    torch = None
    torchaudio = None
    AudioSegment = None
    pydantic = None

# =============================================================================
# --- ç¬¬ 1 æ­¥: å…¨å±€é…ç½® ---
# =============================================================================

# -- A. MixFileCLI é…ç½® --
mixfile_config_yaml = """
uploader: "çº¿è·¯A2"
upload_task: 10
upload_retry: 10
download_task: 5
chunk_size: 1024
port: 4719
host: "0.0.0.0"
password: ""
webdav_path: "data.mix_dav"
history: "history.mix_list"
"""

# -- B. åŠ å¯†çš„æœåŠ¡é…ç½® --
# !!! é‡è¦ !!!
# åœ¨è¿™é‡Œç²˜è´´ä½ ä» encrypt_util.py å·¥å…·ä¸­è·å¾—çš„åŠ å¯†é…ç½®å­—ç¬¦ä¸²
# ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²ç”¨äº FRP (åå‘ä»£ç†)
ENCRYPTED_FRP_CONFIG = "gAAAAABouqe8kDRGZxX7I43TbmNqmy2TZNRJK0f5GqrXwKwW0VR3TtfyfFmDvHCtPbfMzcBkT1qgfMPzhPWDuHRmwibWSmVOgi4nZe_J1TTmLAWMSzQOieGMWE2LXRIIKbf-dLMCnjPDKRkLmZBuWtreN5QgZC41Px3hRU7pxqw51edAkOmvZiE7nf9G0AQ0IvCAR9WHMDg6"

# ç¬¬äºŒä¸ªå­—ç¬¦ä¸²ç”¨äºå­—å¹•æœåŠ¡ (Gemini API å¯†é’¥ç­‰)
ENCRYPTED_SUBTITLE_CONFIG = "gAAAAABouqgGsJh7-osSg3oc2OhtcGotCfGufGZLgeV_4hF6mr5qrhh2wKCjalhFRuPdeUPMNHWqz5CRju55o4CPcr_Deprf-hktkuQ9sDhtRI0mWMkqqwlqPO28AClTO-q0bKXZ74BEPcTCXPVeHZaG_gTolEMblHXXuNK7Mm3VlJV7S5PE1WuvjG4uhne7UtbzYWx5TBE46df09vAxEXb7Tyo05B2jSjkm8NVM3JRmOfSWFpvv9r-J7n3ZCyPCHDj6hc0-6IBkqnX4il41zdgovtxXVugNeploPQylxPI240L1nm6zRKfSlplkNhZ3k1reH3LjDyY1zfp5aEhNKqQXBUrQY_4c--Km2LbQml9BbjRhOnJmUF2uo7Aa7c8mZUM4-jWNDU8KD0dCJFrWkeUQoW7bPlicrw==" # ç¤ºä¾‹ï¼Œè¯·æ›¿æ¢

# -- C. æœ¬åœ°æœåŠ¡ä¸ç«¯å£é…ç½® --
MIXFILE_LOCAL_PORT = 4719
FLASK_API_LOCAL_PORT = 5000
MIXFILE_REMOTE_PORT = 20000  # æ˜ å°„åˆ°å…¬ç½‘çš„ MixFile ç«¯å£
FLASK_API_REMOTE_PORT = 20001  # æ˜ å°„åˆ°å…¬ç½‘çš„ Flask API ç«¯å£

# -- D. Killer API & è¿›ç¨‹ç®¡ç†é…ç½® --
KILLER_API_SHUTDOWN_TOKEN = "change-this-to-a-secure-random-string" # !!! å¼ºçƒˆå»ºè®®ä¿®æ”¹ !!!
PROCESS_KEYWORDS_TO_KILL = ["java", "frpc", "python -c"] # æ–°å¢pythonå­è¿›ç¨‹å…³é”®è¯
EXCLUDE_KEYWORDS_FROM_KILL = ["jupyter", "kernel", "ipykernel", "conda", "grep"]

# -- E. å­—å¹•æå–æµç¨‹é…ç½® --
# è¿™äº›å€¼æœªæ¥ä¹Ÿå¯ä»¥åŠ å…¥åˆ°åŠ å¯†é…ç½®ä¸­
SUBTITLE_CHUNK_DURATION_MS = 10 * 60 * 1000  # 10åˆ†é’Ÿ
SUBTITLE_BATCH_SIZE = 40
SUBTITLE_CONCURRENT_REQUESTS = 8
SUBTITLE_REQUESTS_PER_MINUTE = 8

# =============================================================================
# --- ç¬¬ 2 æ­¥: è§£å¯†ä¸é…ç½®åŠ è½½æ¨¡å— ---
# =============================================================================

class DecryptionError(Exception):
    # è‡ªå®šä¹‰å¼‚å¸¸ï¼Œç”¨äºè¡¨ç¤ºè§£å¯†è¿‡ç¨‹ä¸­çš„ç‰¹å®šå¤±è´¥ã€‚
    pass

def _get_decryption_cipher():
    #
    # è¾…åŠ©å‡½æ•°ï¼šä» Kaggle Secrets è·å–å¯†ç å¹¶ç”Ÿæˆ Fernet è§£å¯†å™¨ã€‚
    # è¿™æ˜¯ä¸€ä¸ªå…±äº«é€»è¾‘ï¼Œè¢«å…¶ä»–è§£å¯†å‡½æ•°è°ƒç”¨ã€‚
    #
    if not UserSecretsClient or not Fernet:
        raise DecryptionError("å…³é”®åº“ (kaggle_secrets, cryptography) æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥ã€‚")
    
    print("ğŸ” æ­£åœ¨ä» Kaggle Secrets è·å–è§£å¯†å¯†é’¥ 'FRP_DECRYPTION_KEY'...")
    try:
        secrets = UserSecretsClient()
        decryption_key_password = secrets.get_secret("FRP_DECRYPTION_KEY")
        if not decryption_key_password:
             raise ValueError("Kaggle Secret 'FRP_DECRYPTION_KEY' çš„å€¼ä¸ºç©ºã€‚")
    except Exception as e:
        print(f"âŒ æ— æ³•ä» Kaggle Secrets è·å–å¯†é’¥ï¼è¯·ç¡®ä¿ä½ å·²æ­£ç¡®è®¾ç½®äº†åä¸º 'FRP_DECRYPTION_KEY' çš„ Secretã€‚")
        raise DecryptionError(f"è·å– Kaggle Secret å¤±è´¥: {e}") from e

    # ä½¿ç”¨ SHA256 ä»ç”¨æˆ·å¯†ç æ´¾ç”Ÿä¸€ä¸ªç¡®å®šæ€§çš„ã€å®‰å…¨çš„32å­—èŠ‚å¯†é’¥
    key = base64.urlsafe_b64encode(hashlib.sha256(decryption_key_password.encode()).digest())
    return Fernet(key)

def get_decrypted_config(encrypted_string: str, config_name: str) -> dict:
    #
    # é€šç”¨çš„è§£å¯†å‡½æ•°ï¼Œç”¨äºè§£å¯†ä»»ä½•ç»™å®šçš„åŠ å¯†å­—ç¬¦ä¸²ã€‚
    #
    # Args:
    #     encrypted_string (str): ä» encrypt_util.py è·å–çš„ Base64 ç¼–ç çš„åŠ å¯†å­—ç¬¦ä¸²ã€‚
    #     config_name (str): é…ç½®çš„åç§°ï¼Œç”¨äºæ—¥å¿—è¾“å‡º (ä¾‹å¦‚ "FRP", "Subtitle")ã€‚
    #
    # Returns:
    #     dict: è§£å¯†å¹¶è§£æåçš„é…ç½®å­—å…¸ã€‚
    # 
    # Raises:
    #     DecryptionError: å¦‚æœè§£å¯†è¿‡ç¨‹çš„ä»»ä½•æ­¥éª¤å¤±è´¥ã€‚
    #
    print(f"ğŸ”‘ æ­£åœ¨å‡†å¤‡è§£å¯† {config_name} é…ç½®...")
    try:
        if "PASTE_YOUR_ENCRYPTED" in encrypted_string:
             raise ValueError(f"æ£€æµ‹åˆ°å ä½ç¬¦åŠ å¯†å­—ç¬¦ä¸²ï¼Œè¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®é…ç½®ã€‚")
        
        cipher = _get_decryption_cipher()
        decrypted_bytes = cipher.decrypt(encrypted_string.encode('utf-8'))
        config = json.loads(decrypted_bytes.decode('utf-8'))
        print(f"âœ… {config_name} é…ç½®è§£å¯†æˆåŠŸï¼")
        return config
    except ValueError as e:
        print(f"âŒ è§£å¯† {config_name} é…ç½®å¤±è´¥: {e}")
        raise DecryptionError(f"é…ç½®å­—ç¬¦ä¸²æ— æ•ˆ: {e}")
    except Exception as e:
        print(f"âŒ è§£å¯† {config_name} é…ç½®å¤±è´¥ï¼")
        print("   è¿™é€šå¸¸æ„å‘³ç€ä½ çš„ Kaggle Secret (å¯†ç ) ä¸åŠ å¯†æ—¶ä½¿ç”¨çš„å¯†ç ä¸åŒ¹é…ï¼Œ")
        print("   æˆ–è€…åŠ å¯†å­—ç¬¦ä¸²å·²æŸåã€‚")
        raise DecryptionError(f"è§£å¯†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}") from e
        
# =============================================================================
# --- ç¬¬ 3 æ­¥: ç¯å¢ƒæ£€æŸ¥ã€è¾…åŠ©å‡½æ•°ä¸ Killer API ---
# =============================================================================

def check_environment():
    #
    # æ£€æŸ¥è¿è¡Œæ‰€éœ€çš„æ‰€æœ‰å…³é”®ä¾èµ–å’Œåº“æ˜¯å¦éƒ½å·²æ­£ç¡®å®‰è£…ã€‚
    # å¦‚æœç¼ºå°‘å…³é”®ç»„ä»¶ï¼Œåˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œä½¿ç¨‹åºæå‰å¤±è´¥ã€‚
    #
    print("\n" + "="*60)
    print("ğŸ”¬ æ­£åœ¨æ‰§è¡Œç¯å¢ƒä¾èµ–æ£€æŸ¥...")
    print("="*60)
    
    # æ£€æŸ¥åŸºç¡€åº“
    if not UserSecretsClient or not Fernet:
        raise RuntimeError("å…³é”®å®‰å…¨åº“ 'kaggle_secrets' æˆ– 'cryptography' æœªæ‰¾åˆ°ã€‚æ— æ³•ç»§ç»­ã€‚")
    print("âœ… å®‰å…¨åº“ (kaggle_secrets, cryptography) - æ­£å¸¸")
    
    # æ£€æŸ¥å­—å¹•æå–æ ¸å¿ƒåº“
    critical_subtitle_libs = {
        "torch": torch,
        "torchaudio": torchaudio,
        "pydub": AudioSegment,
        "pydantic": pydantic
    }
    for name, lib in critical_subtitle_libs.items():
        if not lib:
            raise RuntimeError(f"å­—å¹•æå–æ ¸å¿ƒåº“ '{name}' æœªæ‰¾åˆ°æˆ–å¯¼å…¥å¤±è´¥ã€‚è¯·æ£€æŸ¥ Kaggle ç¯å¢ƒã€‚")
        print(f"âœ… å­—å¹•åº“ ({name}) - æ­£å¸¸")

    # æ£€æŸ¥å‘½ä»¤è¡Œå·¥å…·
    if not shutil.which("ffmpeg"):
        raise RuntimeError("'ffmpeg' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¿™æ˜¯æå–éŸ³é¢‘æ‰€å¿…éœ€çš„ã€‚")
    print("âœ… å‘½ä»¤è¡Œå·¥å…· (ffmpeg) - æ­£å¸¸")
    
    if not shutil.which("java"):
        raise RuntimeError("'java' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¿™æ˜¯è¿è¡Œ MixFileCLI æ‰€å¿…éœ€çš„ã€‚")
    print("âœ… å‘½ä»¤è¡Œå·¥å…· (java) - æ­£å¸¸")

    print("\nâœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼æ‰€æœ‰å…³é”®ä¾èµ–å‡å·²å°±ç»ªã€‚\n")


def log_system_event(level: str, message: str, in_worker=False):
    # ä¸€ä¸ªç®€å•çš„å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—è®°å½•å™¨ã€‚
    # æ–°å¢ in_worker å‚æ•°ä»¥åŒºåˆ†æ—¥å¿—æ¥æº
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
    worker_tag = "[WORKER]" if in_worker else "[SYSTEM]"
    prefix = f"[{timestamp} {worker_tag} {level.upper()}]"
    print(f"{prefix} {message}", flush=True)


def ms_to_srt_time(ms: int) -> str:
    # å°†æ¯«ç§’è½¬æ¢ä¸º SRT å­—å¹•æ–‡ä»¶çš„æ—¶é—´æ ¼å¼ (HH:MM:SS,ms)ã€‚
    try:
        ms = int(ms)
    except (ValueError, TypeError):
        ms = 0
    td = timedelta(milliseconds=ms)
    hours, remainder = divmod(td.seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    milliseconds = td.microseconds // 1000
    return f"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}"


def run_command(command: str, log_file: str = None) -> subprocess.Popen:
    #
    # åœ¨åå°ä»¥éé˜»å¡æ–¹å¼æ‰§è¡Œä¸€ä¸ª shell å‘½ä»¤ã€‚
    #
    # Args:
    #     command (str): è¦æ‰§è¡Œçš„å‘½ä»¤ã€‚
    #     log_file (str, optional): å°† stdout å’Œ stderr é‡å®šå‘åˆ°çš„æ—¥å¿—æ–‡ä»¶åã€‚
    #
    # Returns:
    #     subprocess.Popen: æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹å¯¹è±¡ã€‚
    #
    log_system_event("info", f"æ‰§è¡Œå‘½ä»¤: {command}")
    stdout_dest = None
    stderr_dest = None
    if log_file:
        log_handle = open(log_file, 'w', encoding='utf-8')
        stdout_dest = log_handle
        stderr_dest = log_handle
    
    # ä½¿ç”¨ Popen ä»¥éé˜»å¡æ–¹å¼å¯åŠ¨è¿›ç¨‹
    process = subprocess.Popen(
        command,
        shell=True,
        stdout=stdout_dest,
        stderr=stderr_dest,
        encoding='utf-8',
        errors='ignore'
    )
    return process


def wait_for_port(port: int, host: str = '127.0.0.1', timeout: float = 60.0) -> bool:
    #
    # ç­‰å¾…æŒ‡å®šçš„æœ¬åœ°ç«¯å£å˜ä¸ºå¯ç”¨çŠ¶æ€ã€‚
    #
    # Args:
    #     port (int): è¦æ£€æŸ¥çš„ç«¯å£å·ã€‚
    #     host (str, optional): ç›‘å¬çš„ä¸»æœºåœ°å€ã€‚é»˜è®¤ä¸º '127.0.0.1'ã€‚
    #     timeout (float, optional): æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ã€‚é»˜è®¤ä¸º 60.0ã€‚
    #
    # Returns:
    #     bool: å¦‚æœç«¯å£åœ¨è¶…æ—¶å‰å˜ä¸ºå¯ç”¨ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    #
    log_system_event("info", f"æ­£åœ¨ç­‰å¾…ç«¯å£ {host}:{port} å¯åŠ¨...")
    start_time = time.perf_counter()
    while True:
        try:
            with socket.create_connection((host, port), timeout=1):
                log_system_event("info", f"âœ… ç«¯å£ {port} å·²æˆåŠŸå¯åŠ¨ï¼")
                return True
        except (socket.timeout, ConnectionRefusedError):
            time.sleep(1)
            if time.perf_counter() - start_time >= timeout:
                log_system_event("error", f"âŒ ç­‰å¾…ç«¯å£ {port} è¶…æ—¶ ({timeout}ç§’)ã€‚")
                return False

# --- Killer API é€»è¾‘ (ç”¨äºè¿œç¨‹å…³é—­) ---

def _find_and_kill_targeted_processes(signal_to_send=signal.SIGTERM):
    # æŸ¥æ‰¾å¹¶ç»ˆæ­¢æ­¤è„šæœ¬å¯åŠ¨çš„æ‰€æœ‰å…³é”®å­è¿›ç¨‹ (java, frpc, python -c ...)ã€‚
    killed_pids_info = []
    log_system_event("info", f"æŸ¥æ‰¾å¹¶å°è¯•ç»ˆæ­¢ä¸ '{PROCESS_KEYWORDS_TO_KILL}' ç›¸å…³çš„è¿›ç¨‹...")
    
    current_kernel_pid = os.getpid()
    pids_to_target = set()

    try:
        # ä½¿ç”¨ ps å‘½ä»¤è·å–æ‰€æœ‰è¿›ç¨‹ä¿¡æ¯
        ps_output = subprocess.check_output(['ps', '-eo', 'pid,ppid,args'], text=True)
        
        for line in ps_output.splitlines()[1:]:
            parts = line.strip().split(None, 2)
            if len(parts) < 3: continue
            
            try:
                pid = int(parts[0])
                command_line = parts[2]
                command_lower = command_line.lower()
            except (ValueError, IndexError):
                continue
            
            # æ’é™¤å½“å‰å†…æ ¸è¿›ç¨‹å’Œç³»ç»Ÿå…³é”®è¿›ç¨‹
            if pid == current_kernel_pid: continue
            is_excluded = any(ex_kw.lower() in command_lower for ex_kw in EXCLUDE_KEYWORDS_FROM_KILL)
            if is_excluded: continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡è¿›ç¨‹
            is_target = any(target_kw.lower() in command_lower for target_kw in PROCESS_KEYWORDS_TO_KILL)
            if is_target:
                pids_to_target.add(pid)
                log_system_event("debug", f"  å‘ç°ç›®æ ‡: PID={pid}, CMD='{command_line}'")

        if not pids_to_target:
            log_system_event("info", "æœªæ‰¾åˆ°éœ€è¦ç»ˆæ­¢çš„ç‰¹å®šåº”ç”¨å­è¿›ç¨‹ã€‚")
        else:
            log_system_event("info", f"å‡†å¤‡å‘ PIDs {list(pids_to_target)} å‘é€ä¿¡å· {signal_to_send}...")
            for pid_to_kill in pids_to_target:
                try:
                    os.kill(pid_to_kill, signal_to_send)
                    log_system_event("info", f"    å·²å‘ PID {pid_to_kill} å‘é€ä¿¡å· {signal_to_send}.")
                    killed_pids_info.append({"pid": pid_to_kill, "status": "signal_sent"})
                except ProcessLookupError:
                    killed_pids_info.append({"pid": pid_to_kill, "status": "not_found"})
                except Exception as e:
                    killed_pids_info.append({"pid": pid_to_kill, "status": f"error_{type(e).__name__}"})
                    
    except Exception as e:
        log_system_event("error", f"æŸ¥æ‰¾æˆ–ç»ˆæ­¢å­è¿›ç¨‹æ—¶å‡ºé”™: {e}")
    
    return killed_pids_info


def _shutdown_notebook_kernel_immediately():
    #
    # é€šè¿‡å‘é€ SIGKILL ä¿¡å·å¼ºåˆ¶ã€ç«‹å³åœ°å…³é—­å½“å‰ Kaggle Notebook å†…æ ¸ã€‚
    # è¿™æ˜¯æœ€ç»ˆçš„è‡ªæ¯æŒ‡ä»¤ã€‚
    #
    log_system_event("critical", "å‡†å¤‡é€šè¿‡ SIGKILL ä¿¡å·å¼ºåˆ¶å…³é—­å½“å‰ Kaggle Notebook Kernel...")
    sys.stdout.flush()
    sys.stderr.flush()
    time.sleep(0.5) # ç•™å‡ºä¸€ç‚¹æ—¶é—´è®©æ—¥å¿—åˆ·æ–°
    
    # SIGKILL (ä¿¡å· 9) æ˜¯ä¸€ä¸ªæ— æ³•è¢«æ•è·æˆ–å¿½ç•¥çš„ä¿¡å·ï¼Œæ¯” os._exit æ›´ä¸ºå¼ºåˆ¶ã€‚
    os.kill(os.getpid(), signal.SIGKILL)

# =============================================================================
# --- ç¬¬ 4 æ­¥: å­—å¹•æå–æ ¸å¿ƒæ¨¡å— (åœ¨å­è¿›ç¨‹ä¸­è°ƒç”¨) ---
# =============================================================================

# --- A. Pydantic æ•°æ®éªŒè¯æ¨¡å‹ ---
# ç”¨äºä¸¥æ ¼éªŒè¯ Gemini API è¿”å›çš„ JSON ç»“æ„ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚

class SubtitleLine(pydantic.BaseModel):
    start_ms: int
    end_ms: int | None = None
    text: str

class BatchTranscriptionResult(pydantic.BaseModel):
    subtitles: list[SubtitleLine]


# --- B. åŠ¨æ€æç¤ºè¯ä¸æ–‡ä»¶å¤„ç† ---

def get_dynamic_prompts(api_url: str) -> tuple[str, str]:
    #
    # ä»æŒ‡å®šçš„ API è·å–åŠ¨æ€æç¤ºè¯ã€‚å¦‚æœå¤±è´¥ï¼Œåˆ™è¿”å›ç¡¬ç¼–ç çš„å¤‡ç”¨æç¤ºè¯ã€‚
    #
    log_system_event("info", "æ­£åœ¨å°è¯•ä» API è·å–åŠ¨æ€æç¤ºè¯...", in_worker=True)
    try:
        response = requests.get(api_url, timeout=10)
        response.raise_for_status()
        prompts = response.json()
        if "system_instruction" in prompts and "prompt_for_task" in prompts:
            log_system_event("info", "âœ… æˆåŠŸä» API è·å–åŠ¨æ€æç¤ºè¯ã€‚", in_worker=True)
            return prompts['system_instruction'], prompts['prompt_for_task']
        else:
            raise ValueError("API å“åº”ä¸­ç¼ºå°‘å¿…è¦çš„é”®ã€‚")
    except Exception as e:
        log_system_event("warning", f"è·å–åŠ¨æ€æç¤ºè¯å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨å¤‡ç”¨æç¤ºè¯ã€‚", in_worker=True)
        # --- å¤‡ç”¨æç¤ºè¯ (Fallback Prompts) ---
        fallback_system = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­—å¹•ç¿»è¯‘æ¨¡å‹ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æä¾›çš„ä»»ä½•è¯­è¨€çš„éŸ³é¢‘å†…å®¹ç¿»è¯‘æˆ**ç®€ä½“ä¸­æ–‡**ã€‚"
            "åœ¨ä½ çš„æ‰€æœ‰è¾“å‡ºä¸­ï¼Œ`text` å­—æ®µçš„å€¼**å¿…é¡»æ˜¯ç®€ä½“ä¸­æ–‡**ã€‚"
            "**ç»å¯¹ç¦æ­¢**åœ¨ `text` å­—æ®µä¸­è¿”å›ä»»ä½•åŸå§‹è¯­è¨€ï¼ˆå¦‚æ—¥è¯­ï¼‰çš„æ–‡æœ¬ã€‚"
        )
        fallback_task = (
            "æˆ‘å°†ä¸ºä½ æä¾›ä¸€ç³»åˆ—éŸ³é¢‘ç‰‡æ®µå’Œå®ƒä»¬åœ¨è§†é¢‘ä¸­çš„ç»å¯¹æ—¶é—´ `[AUDIO_INFO] start_ms --> end_ms`ã€‚\n"
            "è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n"
            "1.  **å¬å–éŸ³é¢‘**å¹¶ç†è§£å…¶å†…å®¹ã€‚\n"
            "2.  **åˆ›å»ºå†…éƒ¨æ—¶é—´è½´**: å°†**ç¿»è¯‘æˆä¸­æ–‡å**çš„æ–‡æœ¬ï¼Œæ ¹æ®è¯­éŸ³åœé¡¿åˆ†å‰²æˆæ›´çŸ­çš„å­—å¹•è¡Œï¼Œå¹¶ä¸ºæ¯ä¸€è¡Œä¼°ç®—ä¸€ä¸ªç²¾ç¡®çš„ `start_ms` å’Œ `end_ms`ã€‚\n"
            "3.  **æ ¼å¼åŒ–è¾“å‡º**: ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªæ ¼å¼å®Œå…¨æ­£ç¡®çš„ JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½• markdown æ ‡è®°ã€‚ç»“æ„å¦‚ä¸‹ï¼Œå…¶ä¸­ `text` å­—æ®µçš„å€¼å¿…é¡»æ˜¯ä½ ç¿»è¯‘åçš„**ç®€ä½“ä¸­æ–‡**ï¼š\n"
            '{\n'
            '  "subtitles": [\n'
            '    { "start_ms": 12345, "end_ms": 13456, "text": "è¿™æ˜¯ç¿»è¯‘åçš„ç¬¬ä¸€å¥å­—å¹•" },\n'
            '    { "start_ms": 13600, "text": "è¿™æ˜¯ç¬¬äºŒå¥" }\n'
            '  ]\n'
            '}\n'
        )
        return fallback_system, fallback_task


def read_and_encode_file_base64(filepath: str) -> str | None:
    # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è¿”å› Base64 ç¼–ç çš„å­—ç¬¦ä¸²ã€‚
    try:
        with open(filepath, "rb") as f:
            binary_data = f.read()
        return base64.b64encode(binary_data).decode('utf-8')
    except Exception as e:
        log_system_event("error", f"æ— æ³•è¯»å–æˆ–ç¼–ç æ–‡ä»¶: {filepath}. é”™è¯¯: {e}", in_worker=True)
        return None

# --- C. éŸ³é¢‘é¢„å¤„ç†ç®¡é“ ---


def preprocess_audio_for_subtitles(
    video_path: Path,
    temp_dir: Path,
    update_status_callback: callable
) -> list[dict]:
    
    # ã€æ ¸å¿ƒä¿®å¤ã€‘ï¼šåœ¨å­è¿›ç¨‹å‡½æ•°å†…éƒ¨ï¼Œå¼ºåˆ¶é‡æ–°å¯¼å…¥å¯èƒ½åœ¨å¯åŠ¨æ—¶ä¸ºNoneçš„æ¨¡å—
    # è¿™ç¡®ä¿äº†å³ä½¿çˆ¶è¿›ç¨‹ä¸­è¿™äº›å˜é‡æ˜¯Noneï¼Œå­è¿›ç¨‹ä¹Ÿèƒ½è·å–åˆ°æ­£ç¡®çš„æ¨¡å—
    from demucs.separate import S
    import tensorflow_hub as hub
    import tensorflow as tf
    from speechbrain.pretrained import SepformerLSTMMaskNet as SpeechEnhancer
    from faster_whisper.audio import decode_audio
    from faster_whisper.vad import VadOptions, get_speech_timestamps

    # --- STAGE 0: åˆå§‹åŒ–æ¨¡å‹ ---
    update_status_callback(stage="subtitle_init_models", details="æ­£åœ¨åˆå§‹åŒ–é«˜çº§éŸ³é¢‘å¤„ç†æ¨¡å‹...")

    # åŠ è½½ SpeechBrain è¯­éŸ³å¢å¼ºæ¨¡å‹
    try:
        speech_enhancer = SpeechEnhancer.from_hparams(
            source="speechbrain/sepformer-dns4-16k-enhancement",
            savedir=str(temp_dir / "pretrained_models/sepformer-dns4-16k-enhancement"),
            run_opts={"device": "cuda"} # æ˜ç¡®æŒ‡å®šè®¾å¤‡
        )
        log_system_event("info", "âœ… SpeechBrain è¯­éŸ³å¢å¼ºæ¨¡å‹åŠ è½½æˆåŠŸã€‚", in_worker=True)
    except Exception as e:
        log_system_event("warning", f"åŠ è½½ SpeechBrain æ¨¡å‹å¤±è´¥ï¼Œå°†è·³è¿‡è¯­éŸ³å¢å¼ºã€‚é”™è¯¯: {e}", in_worker=True)
        speech_enhancer = None
        
    # åŠ è½½ YAMNet å£°éŸ³äº‹ä»¶æ£€æµ‹æ¨¡å‹
    try:
        yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')
        yamnet_class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')
        yamnet_class_names = [name.strip('"') for name in open(yamnet_class_map_path).read().strip().split('\n')]
        log_system_event("info", "âœ… YAMNet å£°éŸ³äº‹ä»¶æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸã€‚", in_worker=True)
    except Exception as e:
        log_system_event("warning", f"åŠ è½½ YAMNet æ¨¡å‹å¤±è´¥ï¼Œå°†è·³è¿‡å†…å®¹ç”„åˆ«ã€‚é”™è¯¯: {e}", in_worker=True)
        yamnet_model = None

    # --- STAGE 1: æå–ä¸ç²—åˆ†ç¦» (Demucs) ---
    update_status_callback(stage="subtitle_extract_audio", details="æ­£åœ¨æå–åŸå§‹éŸ³é¢‘...")
    raw_audio_path = temp_dir / "raw_audio.wav"
    try:
        command = [ "ffmpeg", "-i", str(video_path), "-ac", "2", "-ar", "44100", "-vn", "-y", "-loglevel", "error", str(raw_audio_path) ]
        subprocess.run(command, check=True, capture_output=True, text=True)
    except subprocess.CalledProcessError as e: raise RuntimeError(f"FFmpeg æå–éŸ³é¢‘å¤±è´¥: {e.stderr}")

    update_status_callback(stage="subtitle_demucs_separation", details="æ­£åœ¨ä½¿ç”¨ Demucs åˆ†ç¦»äººå£°...")
    demucs_args = [ "-n", "htdemucs_ft", "--two-stems", "vocals", "-d", "cuda", "--out", str(temp_dir / "demucs_output"), str(raw_audio_path) ]
    old_stdout, sys.stdout = sys.stdout, StringIO()
    try:
        S.sub_run(demucs_args)
    except Exception as demucs_error:
        # å¢åŠ å¯¹ Demucs å¤±è´¥çš„æ•è·ï¼Œé˜²æ­¢æ•´ä¸ªä»»åŠ¡ä¸­æ–­
        log_system_event("error", f"Demucs æ‰§è¡Œå¤±è´¥: {demucs_error}", in_worker=True)
    finally:
        sys.stdout = old_stdout
    
    vocals_path = temp_dir / "demucs_output" / "htdemucs_ft" / raw_audio_path.stem / "vocals.wav"
    if not vocals_path.exists():
        log_system_event("warning", "Demucsæœªèƒ½åˆ†ç¦»äººå£°ï¼Œå°†ä½¿ç”¨åŸå§‹éŸ³é¢‘ã€‚", in_worker=True)
        processing_audio_path = raw_audio_path
    else:
        log_system_event("info", "âœ… Demucs æˆåŠŸåˆ†ç¦»äººå£°ã€‚", in_worker=True)
        processing_audio_path = vocals_path
        
    # --- STAGE 2: ç»Ÿä¸€æ ¼å¼å¹¶è¿›è¡Œç²¾ç»†è¯­éŸ³å¢å¼º (SpeechBrain) ---
    update_status_callback(stage="subtitle_speech_enhancement", details="æ­£åœ¨è¿›è¡Œæ·±åº¦è¯­éŸ³å¢å¼º...")
    # è½¬æ¢åˆ°æ¨¡å‹æ‰€éœ€çš„ 16kHz å•å£°é“
    enhanced_audio_path = temp_dir / "enhanced_audio.wav"
    ffmpeg_resample_cmd = [ "ffmpeg", "-i", str(processing_audio_path), "-ac", "1", "-ar", "16000", "-y", "-loglevel", "error", str(enhanced_audio_path)]
    subprocess.run(ffmpeg_resample_cmd, check=True)
    
    if speech_enhancer:
        try:
            # SpeechBrain v0.5.16 çš„ API éœ€è¦ tensor è¾“å…¥
            waveform, sr = torchaudio.load(str(enhanced_audio_path))
            enhanced_wav = speech_enhancer.enhance_batch(waveform.cuda(), lengths=torch.tensor([1.0]).cuda())
            torchaudio.save(str(enhanced_audio_path), enhanced_wav.squeeze(0).cpu(), 16000)
            log_system_event("info", "âœ… è¯­éŸ³å¢å¼ºå¤„ç†å®Œæˆã€‚", in_worker=True)
        except Exception as e:
            log_system_event("warning", f"SpeechBrain å¢å¼ºå¤±è´¥ï¼Œå°†ä½¿ç”¨æœªå¢å¼ºçš„éŸ³é¢‘ã€‚é”™è¯¯: {e}", in_worker=True)

    # --- STAGE 3: å†…å®¹ç”„åˆ« (VAD + YAMNet) ---
    update_status_callback(stage="subtitle_content_filtering", details="æ­£åœ¨æ£€æµ‹è¯­éŸ³å¹¶è¿‡æ»¤éè¯´è¯å£°...")
    vad_parameters = { "threshold": 0.4, "min_speech_duration_ms": 250, "max_speech_duration_s": 15.0, "min_silence_duration_ms": 1000, "speech_pad_ms": 150 }
    audio_data = decode_audio(str(enhanced_audio_path), sampling_rate=16000)
    speech_timestamps = get_speech_timestamps(audio_data, vad_options=VadOptions(**vad_parameters))

    original_audio = AudioSegment.from_wav(enhanced_audio_path)
    chunk_files = []
    chunks_dir = temp_dir / "audio_chunks"
    chunks_dir.mkdir(exist_ok=True)
    
    for speech in speech_timestamps:
        start_ms = int(speech["start"] / 16000 * 1000)
        end_ms = int(speech["end"] / 16000 * 1000)
        segment_audio = original_audio[start_ms:end_ms]
        
        is_speech_flag = True
        
        if yamnet_model and len(segment_audio) > 0:
            segment_samples = np.array(segment_audio.get_array_of_samples()).astype(np.float32) / (1 << 15)
            scores, embeddings, spectrogram = yamnet_model(segment_samples)
            scores_np = scores.numpy().mean(axis=0)
            top5_indices = np.argsort(scores_np)[-5:][::-1]
            top_classes = [yamnet_class_names[i] for i in top5_indices]

            undesired_sounds = {"Shout", "Yell", "Groan", "Sigh", "Screaming", "Crying, sobbing"}
            
            # æ”¹è¿›çš„ç”„åˆ«é€»è¾‘ï¼šå¦‚æœæœ€é«˜åˆ†çš„åˆ†ç±»æ˜¯ä¸æœŸæœ›çš„å£°éŸ³ï¼Œ
            # å¹¶ä¸”"Speech"ä¸åœ¨å‰ä¸¤åçš„åˆ†ç±»ä¸­ï¼Œåˆ™è¿‡æ»¤æ‰ã€‚
            if top_classes[0] in undesired_sounds and "Speech" not in top_classes[:2]:
                log_system_event("info", f"è¿‡æ»¤æ‰ç‰‡æ®µ {start_ms}ms - {end_ms}ms (æ£€æµ‹ä¸º: {top_classes[0]})", in_worker=True)
                is_speech_flag = False

        if is_speech_flag:
            final_chunk_path = chunks_dir / f"chunk_{start_ms}.wav"
            segment_audio.export(str(final_chunk_path), format="wav")
            chunk_files.append({ "path": str(final_chunk_path), "start_ms": start_ms, "end_ms": end_ms })

    log_system_event("info", f"éŸ³é¢‘çº¯åŒ–å¤„ç†å®Œæˆï¼Œæ€»å…±è·å¾— {len(chunk_files)} ä¸ªæœ‰æ•ˆè¯´è¯ç‰‡æ®µã€‚", in_worker=True)
    return chunk_files

# --- D. AI äº¤äº’ä¸å¹¶å‘è°ƒåº¦ ---

def _process_subtitle_batch_with_ai(
    chunk_group: list[dict],
    group_index: int,
    api_key: str,
    gemini_endpoint_prefix: str,
    prompt_api_url: str  # <--- æ ¸å¿ƒä¿®æ”¹ï¼šä¸å†æ¥æ”¶æç¤ºè¯æœ¬èº«ï¼Œè€Œæ˜¯æ¥æ”¶APIçš„URL
) -> list[dict]:
    #
    # å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„éŸ³é¢‘å—ï¼Œè°ƒç”¨ Gemini API è·å–å­—å¹•ã€‚
    # è¿™æ˜¯ä¸€ä¸ªå†…éƒ¨å‡½æ•°ï¼Œç”±ä¸»è°ƒåº¦å™¨è°ƒç”¨ã€‚
    #
    thread_local_srt_list = []
    log_system_event("info", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å·²å¯åŠ¨...", in_worker=True)
    try:
        # ã€æ–°é€»è¾‘ã€‘ï¼šåœ¨æ¯ä¸ªæ‰¹æ¬¡å¤„ç†å¼€å§‹æ—¶ï¼Œç‹¬ç«‹è·å–åŠ¨æ€æç¤ºè¯
        system_instruction, prompt_for_task = get_dynamic_prompts(prompt_api_url)

        # 1. æ„å»º REST API è¯·æ±‚ä½“ (payload) - åç»­é€»è¾‘ä¿æŒä¸å˜
        model_name = "gemini-2.5-flash"
        
        generate_url = f"{gemini_endpoint_prefix.rstrip('/')}/v1beta/models/{model_name}:generateContent"
        
        headers = {"x-goog-api-key": api_key, "Content-Type": "application/json"}
        
        parts = [{"text": prompt_for_task}]
        for chunk in chunk_group:
            encoded_data = read_and_encode_file_base64(chunk["path"])
            if not encoded_data:
                log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ–‡ä»¶ {chunk['path']} ç¼–ç å¤±è´¥ï¼Œè·³è¿‡æ­¤æ–‡ä»¶ã€‚", in_worker=True)
                continue
            parts.append({"text": f"[AUDIO_INFO] {chunk['start_ms']} --> {chunk['end_ms']}"})
            parts.append({"inlineData": {"mime_type": "audio/wav", "data": encoded_data}})
        
        if len(parts) <= 1:
            log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ•´ä¸ªæ‰¹æ¬¡å‡æ— æ³•ç¼–ç ï¼Œæ”¾å¼ƒã€‚", in_worker=True)
            return []

        payload = {
            "contents": [{"parts": parts}],
            "systemInstruction": {"parts": [{"text": system_instruction}]},
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
            ],
            "generationConfig": {"responseMimeType": "application/json"}
        }

        # 2. å‘é€è¯·æ±‚å¹¶å®ç°å…¨é¢çš„é‡è¯•é€»è¾‘
        log_system_event("info", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ•°æ®å‡†å¤‡å®Œæ¯•ï¼Œæ­£åœ¨è°ƒç”¨ Gemini API at {generate_url}...", in_worker=True)
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(generate_url, headers=headers, json=payload, timeout=1000)
                
                if response.status_code in [429, 500, 503, 504]:
                    log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] é‡åˆ°å¯é‡è¯•çš„ API é”™è¯¯ (HTTP {response.status_code}, å°è¯• {attempt + 1}/{max_retries})ã€‚", in_worker=True)
                    if attempt < max_retries - 1:
                        wait_time = 5 * (2 ** attempt)
                        log_system_event("info", f"{wait_time}ç§’åå°†è‡ªåŠ¨é‡è¯•...", in_worker=True)
                        time.sleep(wait_time)
                        continue
                    else:
                        response.raise_for_status()

                response.raise_for_status()
                response_data = response.json()
                
                candidates = response_data.get("candidates")
                if not candidates:
                    raise ValueError(f"APIå“åº”ä¸­ç¼ºå°‘ 'candidates' å­—æ®µã€‚å“åº”: {response.text}")

                content = candidates[0].get("content")
                if not content:
                    finish_reason = candidates[0].get("finishReason", "æœªçŸ¥")
                    safety_ratings = candidates[0].get("safetyRatings", [])
                    raise ValueError(f"APIå“åº”å†…å®¹ä¸ºç©º(å¯èƒ½è¢«æ‹¦æˆª)ã€‚åŸå› : {finish_reason}, å®‰å…¨è¯„çº§: {safety_ratings}")

                json_text = content.get("parts", [{}])[0].get("text")
                if json_text is None:
                    raise ValueError(f"APIå“åº”çš„ 'parts' ä¸­ç¼ºå°‘ 'text' å­—æ®µã€‚å“åº”: {response.text}")

                parsed_result = BatchTranscriptionResult.model_validate_json(json_text)
                subtitles_count = len(parsed_result.subtitles)
                log_system_event("info", f"âœ… [å­—å¹•ä»»åŠ¡ {group_index+1}] æˆåŠŸï¼è·å¾— {subtitles_count} æ¡å­—å¹•ã€‚", in_worker=True)
                
                for i, subtitle in enumerate(parsed_result.subtitles):
                    if subtitle.end_ms is None:
                        if i + 1 < subtitles_count:
                            subtitle.end_ms = parsed_result.subtitles[i+1].start_ms
                        else:
                            subtitle.end_ms = chunk_group[-1]['end_ms']
                    if subtitle.start_ms > subtitle.end_ms:
                        subtitle.end_ms = subtitle.start_ms + 250
                    
                    line = f"{ms_to_srt_time(subtitle.start_ms)} --> {ms_to_srt_time(subtitle.end_ms)}\n{subtitle.text.strip()}\n"
                    thread_local_srt_list.append({"start_ms": subtitle.start_ms, "srt_line": line})
                
                return thread_local_srt_list

            except requests.exceptions.RequestException as e:
                log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}", in_worker=True)
                if attempt < max_retries - 1: time.sleep(5 * (2 ** attempt))
            except pydantic.ValidationError as e:
                log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] AIè¿”å›çš„JSONæ ¼å¼éªŒè¯å¤±è´¥ï¼Œæ”¾å¼ƒã€‚é”™è¯¯: {e}", in_worker=True)
                return []
            except Exception as e:
                log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œæ”¾å¼ƒã€‚é”™è¯¯: {e}", in_worker=True)
                return []
        
        raise RuntimeError(f"æ‰¹æ¬¡ {group_index+1} åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥ã€‚")

    except Exception as e:
        log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
    
    return thread_local_srt_list

def run_subtitle_extraction_pipeline(subtitle_config: dict, chunk_files: list[dict], update_status_callback: callable) -> str:
    #
    # ä¸»è°ƒåº¦å™¨ï¼Œè´Ÿè´£å¹¶å‘å¤„ç†æ‰€æœ‰éŸ³é¢‘æ‰¹æ¬¡å¹¶ç”Ÿæˆæœ€ç»ˆçš„ SRT å†…å®¹ã€‚
    #
    
    api_keys = subtitle_config.get('GEMINI_API_KEYS', [])
    prompt_api_url = subtitle_config.get('PROMPT_API_URL', '')
    gemini_endpoint_prefix = subtitle_config.get('GEMINI_API_ENDPOINT_PREFIX', '')
    
    if not all([api_keys, prompt_api_url, gemini_endpoint_prefix]):
        raise ValueError("å­—å¹•é…ç½®ä¸­ç¼ºå°‘ 'GEMINI_API_KEYS', 'PROMPT_API_URL', æˆ– 'GEMINI_API_ENDPOINT_PREFIX'ã€‚")
    
    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ï¼šç§»é™¤æ­¤å¤„çš„ get_dynamic_prompts è°ƒç”¨
    # system_instruction, prompt_for_task = get_dynamic_prompts(prompt_api_url)

    total_chunks = len(chunk_files)
    if total_chunks == 0:
        log_system_event("warning", "æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆçš„è¯­éŸ³ç‰‡æ®µï¼Œæ— æ³•ç”Ÿæˆå­—å¹•ã€‚", in_worker=True)
        return ""

    chunk_groups = [chunk_files[i:i + SUBTITLE_BATCH_SIZE] for i in range(0, total_chunks, SUBTITLE_BATCH_SIZE)]
    num_groups = len(chunk_groups)
    log_system_event("info", f"å·²å°†è¯­éŸ³ç‰‡æ®µåˆ†ä¸º {num_groups} ä¸ªæ‰¹æ¬¡ï¼Œå‡†å¤‡å¹¶å‘å¤„ç†ã€‚", in_worker=True)
    update_status_callback(stage="subtitle_transcribing", details=f"å‡†å¤‡å¤„ç† {num_groups} ä¸ªå­—å¹•æ‰¹æ¬¡...")
    
    all_srt_blocks = []
    
    with ThreadPoolExecutor(max_workers=SUBTITLE_CONCURRENT_REQUESTS) as executor:
        futures = []
        delay_between_submissions = 60.0 / SUBTITLE_REQUESTS_PER_MINUTE
        
        for i, group in enumerate(chunk_groups):
            api_key_for_thread = api_keys[i % len(api_keys)]
            future = executor.submit(
                _process_subtitle_batch_with_ai,
                group,
                i,
                api_key_for_thread,
                gemini_endpoint_prefix,
                prompt_api_url  # <--- æ ¸å¿ƒä¿®æ”¹ï¼šä¼ é€’ URL è€Œä¸æ˜¯æç¤ºè¯å†…å®¹
            )
            futures.append(future)
            if i < num_groups - 1:
                time.sleep(delay_between_submissions)
        
        for i, future in enumerate(futures):
            try:
                result = future.result()
                if result:
                    all_srt_blocks.extend(result)
                update_status_callback(stage="subtitle_transcribing", details=f"å·²å®Œæˆ {i+1}/{num_groups} ä¸ªå­—å¹•æ‰¹æ¬¡...")
            except Exception as e:
                log_system_event("error", f"ä¸€ä¸ªå­—å¹•çº¿ç¨‹ä»»åŠ¡åœ¨è·å–ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}", in_worker=True)

    log_system_event("info", "æ‰€æœ‰å¹¶å‘ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ­£åœ¨æ•´åˆå­—å¹•...", in_worker=True)
    
    all_srt_blocks.sort(key=lambda x: x["start_ms"])
    final_srt_lines = [f"{i + 1}\n{block['srt_line']}" for i, block in enumerate(all_srt_blocks)]
    
    return "\n".join(final_srt_lines)

# =============================================================================
# --- ç¬¬ 5 æ­¥: MixFileCLI å®¢æˆ·ç«¯ ---
# =============================================================================

class MixFileCLIClient:
    # ä¸€ä¸ªç®€å•çš„ç”¨äºä¸ MixFileCLI åç«¯ API äº¤äº’çš„å®¢æˆ·ç«¯ã€‚
    def __init__(self, base_url: str):
        if not base_url.startswith("http"):
            raise ValueError("Base URL å¿…é¡»ä»¥ http æˆ– https å¼€å¤´")
        self.base_url = base_url
        self.session = requests.Session()

    def _make_request(self, method: str, url: str, **kwargs):
        # ç»Ÿä¸€çš„è¯·æ±‚å‘é€æ–¹æ³•ï¼ŒåŒ…å«é”™è¯¯å¤„ç†ã€‚
        try:
            # ç¡®ä¿è¯·æ±‚ä¸ä¼šè¢«ç¼“å­˜
            headers = kwargs.get('headers', {})
            headers.update({'Cache-Control': 'no-cache', 'Pragma': 'no-cache'})
            kwargs['headers'] = headers
            
            response = self.session.request(method, url, timeout=300, **kwargs)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            error_text = str(e)
            status_code = 500 # é»˜è®¤ä¸ºé€šç”¨æœåŠ¡å™¨é”™è¯¯
            if e.response is not None:
                error_text = e.response.text
                status_code = e.response.status_code
            return (status_code, error_text)

    def upload_file(self, local_file_path: str, progress_callback: callable = None):
        #
        # ä¸Šä¼ å•ä¸ªæ–‡ä»¶å¹¶è·å–åˆ†äº«ç ã€‚
        #
        # Args:
        #     local_file_path (str): æœ¬åœ°æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
        #     progress_callback (callable, optional): è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (bytes_uploaded, total_bytes)ã€‚
        #
        # Returns:
        #     requests.Response or tuple: æˆåŠŸæ—¶è¿”å› Response å¯¹è±¡ï¼Œå¤±è´¥æ—¶è¿”å› (status_code, error_text)ã€‚
        #
        filename = os.path.basename(local_file_path)
        upload_url = urljoin(self.base_url, f"/api/upload/{quote(filename)}")
        file_size = os.path.getsize(local_file_path)

        def file_reader_generator(file_handle):
            chunk_size = 1 * 1024 * 1024 # 1MB chunk
            bytes_read = 0
            while True:
                chunk = file_handle.read(chunk_size)
                if not chunk:
                    if progress_callback: # ç¡®ä¿æœ€å100%è¢«è°ƒç”¨
                        progress_callback(file_size, file_size)
                    break
                bytes_read += len(chunk)
                if progress_callback:
                    progress_callback(bytes_read, file_size)
                yield chunk

        with open(local_file_path, 'rb') as f:
            return self._make_request("PUT", upload_url, data=file_reader_generator(f))

# =============================================================================
# --- ç¬¬ 6 æ­¥: ç»Ÿä¸€çš„ Flask API æœåŠ¡ (ä¸»è¿›ç¨‹) ---
# =============================================================================

# --- A. åº”ç”¨åˆå§‹åŒ–ä¸ä»»åŠ¡ç®¡ç† ---
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False

# ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡çš„çŠ¶æ€ï¼Œå¹¶ç”¨é”æ¥ä¿è¯çº¿ç¨‹å®‰å…¨
tasks = {}
tasks_lock = threading.Lock()

# å…¨å±€å˜é‡ï¼Œå°†åœ¨ main å‡½æ•°ä¸­è¢«åˆå§‹åŒ–
api_client = None 
subtitle_config_global = {}
FRP_SERVER_ADDR = None # å£°æ˜å…¨å±€å˜é‡
TASK_QUEUE = None # å¤šè¿›ç¨‹ä»»åŠ¡é˜Ÿåˆ—
RESULT_QUEUE = None # å¤šè¿›ç¨‹ç»“æœé˜Ÿåˆ—

# --- B. API è·¯ç”±å®šä¹‰ ---

@app.route("/api/upload", methods=["POST"])
def unified_upload_endpoint():
    #
    # ç»Ÿä¸€çš„åª’ä½“å¤„ç†å…¥å£ APIã€‚
    # æ¥æ”¶ä¸€ä¸ª URLï¼Œå¹¶æ ¹æ®å‚æ•°å†³å®šæ˜¯ä¸Šä¼ æ–‡ä»¶ã€æå–å­—å¹•ï¼Œè¿˜æ˜¯ä¸¤è€…éƒ½åšã€‚
    # å§‹ç»ˆä»¥å¼‚æ­¥æ¨¡å¼è¿è¡Œã€‚
    #
    data = request.get_json()
    if not data or "url" not in data:
        return jsonify({"error": "è¯·æ±‚ä½“ä¸­å¿…é¡»åŒ…å« 'url' å­—æ®µ"}), 400

    task_id = str(uuid.uuid4())
    
    # ä»è¯·æ±‚ä¸­æå–å‚æ•°ï¼Œå¹¶è®¾ç½®é»˜è®¤å€¼
    request_params = {
        "url": data["url"],
        "extract_subtitle": data.get("extract_subtitle", False),
        "upload_video": data.get("upload_video", True),
        "upload_subtitle": data.get("upload_subtitle", False),
    }

    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    with tasks_lock:
        tasks[task_id] = {
            "task_id": task_id,
            "status": "pending",
            "stage": "queue",
            "details": "ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…å¤„ç†",
            "progress": 0,
            "result": None
        }

    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘å°†ä»»åŠ¡æ•°æ®æ”¾å…¥é˜Ÿåˆ—ï¼Œç”±å­è¿›ç¨‹å¤„ç†
    task_data = {
        'task_id': task_id,
        'params': request_params,
        'subtitle_config': subtitle_config_global, # ä¼ é€’å¿…è¦çš„é…ç½®
        'api_client_base_url': api_client.base_url,
        'frp_server_addr': FRP_SERVER_ADDR # ä¼ é€’FRPåœ°å€ç”¨äºæ„å»ºé“¾æ¥
    }
    TASK_QUEUE.put(task_data)
    
    log_system_event("info", f"å·²åˆ›å»ºæ–°ä»»åŠ¡ {task_id} å¹¶æ¨å…¥å¤„ç†é˜Ÿåˆ—ã€‚")

    return jsonify({
        "task_id": task_id,
        "status_url": f"/api/tasks/{task_id}"
    }), 202


@app.route("/api/tasks/<task_id>", methods=["GET"])
def get_task_status_endpoint(task_id):
    # è·å–æŒ‡å®šä»»åŠ¡çš„å½“å‰çŠ¶æ€å’Œç»“æœã€‚
    with tasks_lock:
        task = tasks.get(task_id)
    
    if task:
        return jsonify(task)
    else:
        return jsonify({"error": "æœªæ‰¾åˆ°æŒ‡å®šçš„ task_id"}), 404


@app.route('/killer_status_frp', methods=['GET'])
def health_status_endpoint():
    # ç”¨äºå¤–éƒ¨å¥åº·æ£€æŸ¥çš„ç®€å•ç«¯ç‚¹ã€‚
    return jsonify({
        "status": "ok",
        "message": "ç»Ÿä¸€åª’ä½“å¤„ç† API æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶é€šè¿‡ FRP æš´éœ²ã€‚",
        "timestamp": time.time()
    }), 200


@app.route('/force_shutdown_notebook', methods=['GET'])
def force_shutdown_endpoint():
    #
    # è¿œç¨‹å…³é—­ APIï¼Œæ¥æ”¶ä¸€ä¸ª token è¿›è¡ŒéªŒè¯ã€‚
    #
    log_system_event("info", "API /force_shutdown_notebook è¢«è°ƒç”¨...")
    token_from_request = request.args.get('token')

    if token_from_request != KILLER_API_SHUTDOWN_TOKEN:
        log_system_event("error", "API Auth å¤±è´¥: Token æ— æ•ˆã€‚")
        return jsonify({"status": "error", "message": "Unauthorized"}), 401
    
    log_system_event("info", "API Auth æˆåŠŸã€‚æ­£åœ¨å®‰æ’åå°å…³é—­ä»»åŠ¡...")

    def delayed_full_shutdown():
        # Step 1: å…ˆå¹³æ»‘åœ°æ€æ­»å­è¿›ç¨‹
        log_system_event("info", "åå°å…³é—­ä»»åŠ¡ï¼šå¼€å§‹ç»ˆæ­¢å­è¿›ç¨‹...")
        _find_and_kill_targeted_processes(signal.SIGTERM)
        time.sleep(2)
        
        # Step 2: å¼ºåˆ¶ç»ˆæ­¢ä»ç„¶å­˜åœ¨çš„å­è¿›ç¨‹
        log_system_event("info", "åå°å…³é—­ä»»åŠ¡ï¼šå¼ºåˆ¶ç»ˆæ­¢ä»»ä½•æ®‹ç•™å­è¿›ç¨‹...")
        _find_and_kill_targeted_processes(signal.SIGKILL)
        time.sleep(1)

        # Step 3: ç»ˆæ­¢ä¸»å†…æ ¸
        _shutdown_notebook_kernel_immediately()
    
    # ç«‹å³å¯åŠ¨åå°çº¿ç¨‹ï¼Œç„¶åç«‹åˆ»è¿”å›å“åº”ï¼Œç¡®ä¿è°ƒç”¨æ–¹æ”¶åˆ°ç¡®è®¤
    threading.Thread(target=delayed_full_shutdown, daemon=True).start()
    
    return jsonify({
        "status": "shutdown_initiated",
        "message": "å…³é—­ä¿¡å·å·²æ¥æ”¶ã€‚åå°æ­£åœ¨æ‰§è¡Œæ¸…ç†å’Œå†…æ ¸å…³é—­æ“ä½œã€‚",
    }), 200

# =============================================================================
# --- ç¬¬ 7 æ­¥: é«˜å®¹é”™çš„ç»Ÿä¸€ä»»åŠ¡å¤„ç†å™¨ (åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œ) ---
# =============================================================================

def process_unified_task(task_data: dict, result_queue: multiprocessing.Queue):
    #
    # å¤„ç†ä¸€ä¸ªå®Œæ•´çš„åª’ä½“ä»»åŠ¡ï¼ŒåŒ…å«ä¸‹è½½ã€å­—å¹•æå–å’Œä¸Šä¼ ç­‰æ­¥éª¤ã€‚
    # è¿™ä¸ªå‡½æ•°å®ç°äº†æ–¹æ¡ˆ2çš„æ ¸å¿ƒé€»è¾‘ï¼šå­ä»»åŠ¡çš„ç‹¬ç«‹å¤±è´¥å¤„ç†ã€‚
    #
    
    # --- 1. åˆå§‹åŒ– ---
    task_id = task_data['task_id']
    params = task_data['params']
    subtitle_config = task_data['subtitle_config']
    api_client_base_url = task_data['api_client_base_url']
    frp_server_addr = task_data['frp_server_addr']
    
    # åœ¨å­è¿›ç¨‹ä¸­é‡æ–°åˆ›å»ºapi_client
    local_api_client = MixFileCLIClient(base_url=api_client_base_url)

    temp_dir = Path(f"/kaggle/working/task_{task_id}")
    temp_dir.mkdir(exist_ok=True)
    
    # å®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•°æ¥æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼Œé€šè¿‡é˜Ÿåˆ—å‘é€ç»™ä¸»è¿›ç¨‹
    def _update_status(status, stage, details, progress=None):
        update_data = {
            'type': 'status_update',
            'task_id': task_id,
            'payload': {
                'status': status,
                'stage': stage,
                'details': details,
            }
        }
        if progress is not None:
            update_data['payload']['progress'] = progress
        result_queue.put(update_data)

    
    # åˆå§‹åŒ–æœ€ç»ˆç»“æœç»“æ„
    final_result = {
        "video_file": {"status": "pending"},
        "subtitle_file": {"status": "pending"}
    }

    try:
        # --- 2. å­ä»»åŠ¡: ä¸‹è½½æºæ–‡ä»¶ ---
        _update_status("running", "download", "å‡†å¤‡ä» URL ä¸‹è½½æ–‡ä»¶...", 0)
        
        file_url = params['url']
        # ä»URLçŒœæµ‹æ–‡ä»¶åï¼Œå¹¶è¿›è¡Œæ¸…ç†
        filename_encoded = file_url.split("/")[-1].split("?")[0] or f"downloaded_file_{task_id}"
        filename = unquote(filename_encoded)
        local_file_path = temp_dir / filename
        final_result["video_file"]["filename"] = filename

        bytes_downloaded = 0
        with requests.get(file_url, stream=True, timeout=60) as r:
            r.raise_for_status()
            total_size = int(r.headers.get('content-length', 0))
            with open(local_file_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
                    bytes_downloaded += len(chunk)
                    if total_size > 0:
                        progress = round((bytes_downloaded / total_size) * 100, 2)
                        _update_status("running", "download", f"å·²ä¸‹è½½ {bytes_downloaded}/{total_size} å­—èŠ‚", progress)
        
        # --- 3. å­ä»»åŠ¡: åª’ä½“ç±»å‹éªŒè¯ ---
        _update_status("running", "validate", "æ­£åœ¨éªŒè¯æ–‡ä»¶ç±»å‹...", 100)
        is_video = False
        mime_type, _ = mimetypes.guess_type(local_file_path)
        if mime_type and mime_type.startswith("video"):
            is_video = True
            log_system_event("info", f"æ–‡ä»¶ {filename} è¢«è¯†åˆ«ä¸ºè§†é¢‘ ({mime_type})ã€‚", in_worker=True)
        else:
            log_system_event("warning", f"æ–‡ä»¶ {filename} ä¸æ˜¯æ ‡å‡†è§†é¢‘æ ¼å¼ ({mime_type})ã€‚", in_worker=True)

        # --- 4. å­ä»»åŠ¡: å­—å¹•æå– ---
        srt_content = None
        if params["extract_subtitle"]:
            if is_video:
                try:
                    _update_status("running", "subtitle_extraction", "å­—å¹•æå–æµç¨‹å·²å¯åŠ¨...", 0)
                    # è°ƒç”¨å®Œæ•´çš„å­—å¹•æå–ç®¡é“
                    # åˆ›å»ºä¸€ä¸ªå±€éƒ¨lambdaï¼Œå› ä¸ºå®ƒä¸èƒ½è¢«pickleä¼ é€’
                    update_callback_for_sub = lambda stage, details: _update_status("running", stage, details)
                    audio_chunks = preprocess_audio_for_subtitles(local_file_path, temp_dir, update_callback_for_sub)
                    srt_content = run_subtitle_extraction_pipeline(subtitle_config, audio_chunks, update_callback_for_sub)
                    
                    if srt_content:
                        srt_filename = local_file_path.stem + ".srt"
                        final_result["subtitle_file"]["filename"] = srt_filename
                        final_result["subtitle_file"]["status"] = "success"
                        
                        # è¿”å›Base64ç¼–ç çš„SRTæ–‡ä»¶
                        srt_base64 = base64.b64encode(srt_content.encode('utf-8')).decode('utf-8')
                        final_result["subtitle_file"]["content_base64"] = srt_base64
                        
                        # å°†SRTæ–‡ä»¶ä¿å­˜åˆ°æœ¬åœ°ä»¥å¤‡ä¸Šä¼ 
                        with open(temp_dir / srt_filename, "w", encoding="utf-8") as f:
                            f.write(srt_content)
                    else:
                        raise RuntimeError("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³ï¼Œæ— æ³•ç”Ÿæˆå­—å¹•ã€‚")

                except Exception as e:
                    log_system_event("error", f"ä»»åŠ¡ {task_id} çš„å­—å¹•æå–å¤±è´¥: {e}", in_worker=True)
                    final_result["subtitle_file"]["status"] = "failed"
                    final_result["subtitle_file"]["details"] = f"å­—å¹•æå–å¤±è´¥: {str(e)}"
            else:
                final_result["subtitle_file"]["status"] = "skipped"
                final_result["subtitle_file"]["details"] = "æºæ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„è§†é¢‘æ ¼å¼"
        else:
            final_result["subtitle_file"]["status"] = "skipped"
            final_result["subtitle_file"]["details"] = "æœªè¯·æ±‚æå–å­—å¹•"

        # --- 5. å­ä»»åŠ¡: æ–‡ä»¶ä¸Šä¼  (ä½¿ç”¨å¹¶å‘) ---
        _update_status("running", "uploading", "å‡†å¤‡ä¸Šä¼ æ–‡ä»¶åˆ° MixFile...", 0)
        
        upload_tasks = []
        with ThreadPoolExecutor(max_workers=2) as executor:
            # æäº¤è§†é¢‘ä¸Šä¼ ä»»åŠ¡
            if params["upload_video"]:
                upload_tasks.append(executor.submit(local_api_client.upload_file, str(local_file_path)))
            else:
                final_result["video_file"]["status"] = "skipped"

            # æäº¤å­—å¹•ä¸Šä¼ ä»»åŠ¡
            if params["upload_subtitle"] and srt_content:
                srt_path = temp_dir / final_result["subtitle_file"]["filename"]
                upload_tasks.append(executor.submit(local_api_client.upload_file, str(srt_path)))
        
        # å¤„ç†ä¸Šä¼ ç»“æœ
        for future in upload_tasks:
            try:
                # å‡è®¾ upload_file æˆåŠŸæ—¶è¿”å› Responseï¼Œå¤±è´¥æ—¶è¿”å› tuple
                response = future.result()
                
                is_srt_upload = False
                if hasattr(response, 'request') and response.request is not None and response.request.url is not None:
                     if ".srt" in unquote(response.request.url):
                        is_srt_upload = True

                if isinstance(response, requests.Response) and response.ok:
                    share_code = response.text.strip()
                    if is_srt_upload:
                        target_result = final_result["subtitle_file"]
                    else:
                        target_result = final_result["video_file"]
                    
                    target_result["status"] = "success"
                    target_result["share_code"] = share_code
                    target_result["share_link"] = f"http://{frp_server_addr}:{MIXFILE_REMOTE_PORT}/api/download/{quote(target_result['filename'])}?s={share_code}"
                else:
                    status_code, error_text = response if isinstance(response, tuple) else (response.status_code, response.text)
                    raise RuntimeError(f"ä¸Šä¼ å¤±è´¥ã€‚çŠ¶æ€ç : {status_code}, é”™è¯¯: {error_text}")

            except Exception as e:
                log_system_event("error", f"ä»»åŠ¡ {task_id} çš„æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}", in_worker=True)
                if final_result["video_file"]["status"] == "pending":
                    final_result["video_file"]["status"] = "failed"
                    final_result["video_file"]["details"] = str(e)
                if final_result["subtitle_file"].get("status") == "success" and "share_code" not in final_result["subtitle_file"]:
                    final_result["subtitle_file"]["status"] = "upload_failed"
                    final_result["subtitle_file"]["details"] = str(e)

        # --- 6. ä»»åŠ¡å®Œæˆ ---
        result_data = {
            'type': 'task_result',
            'task_id': task_id,
            'status': 'success',
            'result': final_result,
            'details': 'ä»»åŠ¡å¤„ç†å®Œæˆ'
        }
        result_queue.put(result_data)
            
    except Exception as e:
        log_system_event("error", f"å¤„ç†ä»»åŠ¡ {task_id} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
        result_data = {
            'type': 'task_result',
            'task_id': task_id,
            'status': 'failed',
            'result': final_result, # å³ä½¿å¤±è´¥ä¹Ÿè¿”å›éƒ¨åˆ†ç»“æœ
            'details': str(e)
        }
        result_queue.put(result_data)
    finally:
        # --- 7. æ¸…ç†ä¸´æ—¶æ–‡ä»¶ ---
        try:
            shutil.rmtree(temp_dir)
            log_system_event("info", f"å·²æ¸…ç†ä»»åŠ¡ {task_id} çš„ä¸´æ—¶ç›®å½•ã€‚", in_worker=True)
        except Exception as e:
            log_system_event("error", f"æ¸…ç†ä»»åŠ¡ {task_id} çš„ä¸´æ—¶ç›®å½•å¤±è´¥: {e}", in_worker=True)

# =============================================================================
# --- ç¬¬ 8 æ­¥: å¤šè¿›ç¨‹ Worker ä¸ä¸»ç¨‹åº ---
# =============================================================================

def worker_process_loop(task_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue):
    """
    è¿™æ˜¯ä¸€ä¸ªåœ¨ç‹¬ç«‹å­è¿›ç¨‹ä¸­è¿è¡Œçš„å¾ªç¯ã€‚
    å®ƒæŒç»­ä»ä»»åŠ¡é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡ï¼Œè°ƒç”¨å¤„ç†å™¨ï¼Œå¹¶å°†ç»“æœæ”¾å…¥ç»“æœé˜Ÿåˆ—ã€‚
    """
    log_system_event("info", "åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å·²å¯åŠ¨ã€‚", in_worker=True)
    while True:
        try:
            task_data = task_queue.get()
            if task_data is None: # æ”¶åˆ°ç»ˆæ­¢ä¿¡å·
                break
            log_system_event("info", f"å·¥ä½œè¿›ç¨‹æ¥æ”¶åˆ°æ–°ä»»åŠ¡: {task_data['task_id']}", in_worker=True)
            process_unified_task(task_data, result_queue)
        except KeyboardInterrupt:
            break
        except Exception as e:
            # æ•è·å¾ªç¯ä¸­çš„æ„å¤–é”™è¯¯ï¼Œé˜²æ­¢å­è¿›ç¨‹å´©æºƒ
            log_system_event("critical", f"å·¥ä½œè¿›ç¨‹å¾ªç¯å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
    log_system_event("info", "åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å·²å…³é—­ã€‚", in_worker=True)

def result_processor_thread_loop(result_queue: multiprocessing.Queue):
    """
    è¿™æ˜¯ä¸€ä¸ªåœ¨ä¸»è¿›ç¨‹ä¸­è¿è¡Œçš„åå°çº¿ç¨‹ã€‚
    å®ƒæŒç»­ä»ç»“æœé˜Ÿåˆ—ä¸­è·å–æ¥è‡ªå­è¿›ç¨‹çš„æ•°æ®ï¼Œå¹¶æ›´æ–°ä¸»è¿›ç¨‹ä¸­çš„ä»»åŠ¡çŠ¶æ€å­—å…¸ã€‚
    """
    log_system_event("info", "ç»“æœå¤„ç†çº¿ç¨‹å·²å¯åŠ¨ã€‚")
    while True:
        try:
            # ä½¿ç”¨å¸¦è¶…æ—¶çš„getï¼Œé˜²æ­¢æ°¸ä¹…é˜»å¡ï¼Œå…è®¸æœªæ¥åŠ å…¥ä¼˜é›…é€€å‡ºé€»è¾‘
            result_data = result_queue.get(timeout=3600) 
            
            task_id = result_data['task_id']
            with tasks_lock:
                if task_id not in tasks:
                    continue # å¦‚æœä»»åŠ¡å·²è¢«æ¸…é™¤ï¼Œåˆ™å¿½ç•¥

                data_type = result_data['type']
                if data_type == 'status_update':
                    payload = result_data['payload']
                    tasks[task_id].update(payload)

                elif data_type == 'task_result':
                    tasks[task_id]['status'] = result_data['status']
                    tasks[task_id]['result'] = result_data['result']
                    tasks[task_id]['details'] = result_data['details']
                    tasks[task_id]['progress'] = 100
                    log_system_event("info", f"ä»»åŠ¡ {task_id} å·²å®Œæˆï¼ŒçŠ¶æ€: {result_data['status']}.")
        
        except QueueEmpty:
            continue # è¶…æ—¶æ˜¯æ­£å¸¸æƒ…å†µï¼Œç»§ç»­å¾ªç¯
        except Exception as e:
            log_system_event("error", f"ç»“æœå¤„ç†çº¿ç¨‹å‘ç”Ÿé”™è¯¯: {e}")

def main():
    #
    # ä¸»æ‰§è¡Œå‡½æ•°ï¼Œè´Ÿè´£åˆå§‹åŒ–å’Œå¯åŠ¨æ‰€æœ‰æœåŠ¡ã€‚
    #
    global api_client, subtitle_config_global, FRP_SERVER_ADDR, TASK_QUEUE, RESULT_QUEUE
    
    try:

        # --- 1. å¯åŠ¨å‰å‡†å¤‡ ---
        log_system_event("info", "æœåŠ¡æ­£åœ¨å¯åŠ¨...")


        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘åœ¨ main() å‡½æ•°ä¸­ï¼Œæ›¿æ¢æ•´ä¸ª pip install é€»è¾‘

        # ç¬¬ä¸€æ­¥ï¼šå®‰è£…ä¸Kaggle CUDA 12.1å…¼å®¹çš„ç‰¹å®šç‰ˆæœ¬PyTorch (ä¿æŒä¸å˜)
        install_torch_cmd = (
            "pip uninstall -y torch torchvision torchaudio && "
            "pip install torch==2.3.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
        )
        log_system_event("info", "æ­£åœ¨å®‰è£…å…¼å®¹çš„ PyTorch ç‰ˆæœ¬...")
        install_proc = subprocess.run(install_torch_cmd, shell=True, capture_output=True, text=True)
        if install_proc.returncode != 0:
            log_system_event("error", f"PyTorch å®‰è£…å¤±è´¥ï¼\nStdout: {install_proc.stdout}\nStderr: {install_proc.stderr}")
            raise RuntimeError("æœªèƒ½å®‰è£…å…¼å®¹çš„ PyTorch ç‰ˆæœ¬ã€‚")
        log_system_event("info", "âœ… å…¼å®¹çš„ PyTorch å®‰è£…å®Œæˆã€‚")

        # ç¬¬äºŒæ­¥ï¼šå®‰è£…æ‰€æœ‰å…¶ä»–åº“ï¼ŒåŒ…æ‹¬æ–°å¢çš„ speechbrain, tensorflow, demucs ç­‰
        install_other_cmd = (
            "pip install -q pydantic pydub demucs "
            "speechbrain==0.5.16 "  # å›ºå®š SpeechBrain ç‰ˆæœ¬ä»¥ä¿è¯å…¼å®¹æ€§
            "tensorflow tensorflow_hub "
            "faster-whisper@https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz "
            "google-generativeai requests psutil"
        )
        log_system_event("info", "æ­£åœ¨å®‰è£…å…¶ä½™ä¾èµ–åº“...")
        subprocess.run(install_other_cmd, shell=True, check=True)
        log_system_event("info", "âœ… å…¶ä½™ä¾èµ–åº“å®‰è£…å®Œæˆã€‚")


        check_environment()
        
        # --- è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³• ---
        # Kaggleç¯å¢ƒæ¨èä½¿ç”¨ 'fork'
        multiprocessing.set_start_method('fork', force=True)

        # --- 2. è§£å¯†é…ç½® ---
        frp_config = get_decrypted_config(ENCRYPTED_FRP_CONFIG, "FRP")
        subtitle_config_global = get_decrypted_config(ENCRYPTED_SUBTITLE_CONFIG, "Subtitle")
        
        FRP_SERVER_ADDR = frp_config['FRP_SERVER_ADDR']
        FRP_SERVER_PORT = frp_config['FRP_SERVER_PORT']
        FRP_TOKEN = frp_config['FRP_TOKEN']
        
        # --- 3. åˆå§‹åŒ– MixFile å®¢æˆ·ç«¯ ---
        api_client_base_url = f"http://127.0.0.1:{MIXFILE_LOCAL_PORT}"
        api_client = MixFileCLIClient(base_url=api_client_base_url)

        # --- 4. å¯åŠ¨ MixFileCLI æœåŠ¡ ---
        log_system_event("info", "æ­£åœ¨åˆ›å»º MixFileCLI config.yml...")
        with open("config.yml", "w") as f: f.write(mixfile_config_yaml)
        
        log_system_event("info", "æ­£åœ¨ä¸‹è½½å¹¶å¯åŠ¨ MixFileCLI...")
        if not os.path.exists("mixfile-cli.jar"):
            run_command("wget -q --show-progress https://raw.githubusercontent.com/jornhand/kagglewithmixfile/refs/heads/main/mixfile-cli-2.0.1.jar -O mixfile-cli.jar").wait()
        run_command("java -jar mixfile-cli.jar", "mixfile.log")
        if not wait_for_port(MIXFILE_LOCAL_PORT):
            raise RuntimeError("MixFileCLI æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ mixfile.logã€‚")

        # --- 5. åˆå§‹åŒ–å¤šè¿›ç¨‹é˜Ÿåˆ—å’Œå·¥ä½œè¿›ç¨‹ ---
        TASK_QUEUE = multiprocessing.Queue()
        RESULT_QUEUE = multiprocessing.Queue()
        
        worker = multiprocessing.Process(
            target=worker_process_loop,
            args=(TASK_QUEUE, RESULT_QUEUE),
            daemon=True
        )
        worker.start()
        
        # å¯åŠ¨ç»“æœå¤„ç†çº¿ç¨‹
        result_thread = threading.Thread(
            target=result_processor_thread_loop,
            args=(RESULT_QUEUE,),
            daemon=True
        )
        result_thread.start()


        # --- 6. å¯åŠ¨ Flask API æœåŠ¡ (å¿…é¡»åœ¨ if __name__ == '__main__': å—å†…å¯åŠ¨) ---
        def run_flask_app():
            app.run(host='0.0.0.0', port=FLASK_API_LOCAL_PORT, debug=False, use_reloader=False)
        log_system_event("info", "æ­£åœ¨åå°å¯åŠ¨ Flask API æœåŠ¡...")
        threading.Thread(target=run_flask_app, daemon=True).start()
        if not wait_for_port(FLASK_API_LOCAL_PORT):
             raise RuntimeError("Flask æœåŠ¡å¯åŠ¨å¤±è´¥ã€‚")

        # --- 7. å¯åŠ¨ frpc å®¢æˆ·ç«¯ ---
        log_system_event("info", "æ­£åœ¨å‡†å¤‡ frpc å®¢æˆ·ç«¯...")
        if not os.path.exists("/kaggle/working/frpc"):
            run_command("wget -q https://github.com/fatedier/frp/releases/download/v0.54.0/frp_0.54.0_linux_amd64.tar.gz && tar -zxvf frp_0.54.0_linux_amd64.tar.gz && mv frp_0.54.0_linux_amd64/frpc /kaggle/working/frpc && chmod +x /kaggle/working/frpc").wait()
        
        frpc_ini = f"""
[common]
server_addr = {FRP_SERVER_ADDR}
server_port = {FRP_SERVER_PORT}
token = {FRP_TOKEN}
log_file = /kaggle/working/frpc.log
[mixfile_webdav_{MIXFILE_REMOTE_PORT}]
type = tcp
local_ip = 127.0.0.1
local_port = {MIXFILE_LOCAL_PORT}
remote_port = {MIXFILE_REMOTE_PORT}
[flask_api_{FLASK_API_REMOTE_PORT}]
type = tcp
local_ip = 127.0.0.1
local_port = {FLASK_API_LOCAL_PORT}
remote_port = {FLASK_API_REMOTE_PORT}
"""
        with open('frpc.ini', 'w') as f: f.write(frpc_ini)
        run_command('/kaggle/working/frpc -c ./frpc.ini')
        log_system_event("info", "frpc å®¢æˆ·ç«¯å·²åœ¨åå°å¯åŠ¨ã€‚")
        time.sleep(3)
        
        # --- 8. æœ€ç»ˆçŠ¶æ€æŠ¥å‘Šä¸ä¿æ´» ---
        public_api_base_url = f"http://{FRP_SERVER_ADDR}:{FLASK_API_REMOTE_PORT}"
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æœåŠ¡å‡å·²æˆåŠŸå¯åŠ¨ï¼æ‚¨çš„ç»Ÿä¸€ API å·²ä¸Šçº¿ã€‚")
        print(f"  -> API å…¥å£ (POST) : {public_api_base_url}/api/upload")
        print(f"  -> ä»»åŠ¡çŠ¶æ€ (GET)  : {public_api_base_url}/api/tasks/<task_id>")
        print(f"  -> å¥åº·æ£€æŸ¥ (GET)  : {public_api_base_url}/killer_status_frp")
        print(f"  -> è¿œç¨‹å…³é—­ (GET)  : {public_api_base_url}/force_shutdown_notebook?token={KILLER_API_SHUTDOWN_TOKEN}")
        print("="*60)
        
        while True:
            time.sleep(300)
            log_system_event("info", f"æœåŠ¡æŒç»­è¿è¡Œä¸­... ({time.ctime()}) å·¥ä½œè¿›ç¨‹çŠ¶æ€: {'å­˜æ´»' if worker.is_alive() else 'å·²é€€å‡º'}")
            
    except (DecryptionError, RuntimeError, ValueError) as e:
        log_system_event("critical", f"ç¨‹åºå¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
        log_system_event("critical", "æœåŠ¡æ— æ³•å¯åŠ¨ï¼Œç¨‹åºå°†ç»ˆæ­¢ã€‚")
    except KeyboardInterrupt:
        log_system_event("info", "æœåŠ¡å·²æ‰‹åŠ¨åœæ­¢ã€‚")
        if 'TASK_QUEUE' in globals() and TASK_QUEUE is not None:
            TASK_QUEUE.put(None) # å‘é€ä¿¡å·è®©å­è¿›ç¨‹é€€å‡º
    except Exception as e:
        log_system_event("critical", f"å‘ç”ŸæœªçŸ¥çš„è‡´å‘½é”™è¯¯: {e}")

if __name__ == '__main__':
    main()