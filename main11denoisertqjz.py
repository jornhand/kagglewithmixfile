# =============================================================================
#         Kaggle æŒä¹…åŒ–åª’ä½“å¤„ç†æœåŠ¡ - ä¸»åº”ç”¨ v2.1 (æ–¹æ¡ˆ2 - å¤šè¿›ç¨‹éš”ç¦»ç‰ˆ)
# =============================================================================
#
# åŠŸèƒ½:
#   - å¯åŠ¨ MixFileCLI ä½œä¸ºæ–‡ä»¶å­˜å‚¨åç«¯ã€‚
#   - æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ Flask APIï¼Œç”¨äº:
#     1. ä» URL ä¸‹è½½æ–‡ä»¶å¹¶ä¸Šä¼ åˆ° MixFileã€‚
#     2. ä»è§†é¢‘ URL ä¸­æå–é«˜è´¨é‡å­—å¹• (å¯é€‰)ã€‚
#     3. å°†åŸå§‹è§†é¢‘å’Œç”Ÿæˆçš„å­—å¹•çµæ´»åœ°ä¸Šä¼ åˆ° MixFileã€‚
#   - é€šè¿‡ FRP å°†å†…éƒ¨æœåŠ¡å®‰å…¨åœ°æš´éœ²åˆ°å…¬ç½‘ã€‚
#   - æ‰€æœ‰æ•æ„Ÿé…ç½® (FRP, APIå¯†é’¥) å‡é€šè¿‡åŠ å¯†æ–¹å¼ç®¡ç†ã€‚
#   - å®ç°é«˜å®¹é”™çš„ä»»åŠ¡å¤„ç†é€»è¾‘ï¼Œå­ä»»åŠ¡å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹ã€‚
#   - **V2.1 å˜æ›´: é‡‡ç”¨å¤šè¿›ç¨‹æ¶æ„ï¼Œå°†é‡é‡çº§çš„åª’ä½“å¤„ç†ä»»åŠ¡æ”¾åˆ°ç‹¬ç«‹çš„å­è¿›ç¨‹
#     ä¸­æ‰§è¡Œï¼Œä»¥è§£å†³ä¸»è¿›ç¨‹ä¸­å› æœåŠ¡ç¯å¢ƒå†²çªå¯¼è‡´çš„AIæ¨¡å‹åŠ è½½å¤±è´¥é—®é¢˜ã€‚**
#
# =============================================================================

# --- æ ¸å¿ƒ Python åº“ ---
import os
import subprocess
import threading
import time
import uuid
import socket
import json
import base64
import hashlib
import signal
import sys
import shutil
import mimetypes
from pathlib import Path
from datetime import timedelta
from urllib.parse import urljoin, quote, unquote
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed, TimeoutError


# --- å¤šè¿›ç¨‹ä¸é˜Ÿåˆ— ---
import multiprocessing
from queue import Empty as QueueEmpty

# --- Web æ¡†æ¶ä¸ HTTP å®¢æˆ·ç«¯ ---
from flask import Flask, request, jsonify, Response
import requests





# =============================================================================
# --- (æ–°å¢æ¨¡å—) ç¬¬ 3.5 æ­¥: V2Ray ä»£ç†ç®¡ç†å™¨ ---
# =============================================================================
import base64
import json
import random
import string
from urllib.parse import urlparse, parse_qs




# --- åŠ å¯†ä¸å¯†é’¥ç®¡ç† ---
# å°è¯•å¯¼å…¥å…³é”®åº“ï¼Œå¦‚æœå¤±è´¥åˆ™åœ¨åç»­æ£€æŸ¥ä¸­å¤„ç†
try:
    from kaggle_secrets import UserSecretsClient
    from cryptography.fernet import Fernet
except ImportError:
    UserSecretsClient = None
    Fernet = None

# --- å­—å¹•æå–ç›¸å…³åº“ (å°†åœ¨ check_environment ä¸­éªŒè¯) ---
try:
    import torch
    import torchaudio
    from pydub import AudioSegment
    import pydantic
except ImportError as e:
    # å…è®¸å¯åŠ¨æ—¶å¯¼å…¥å¤±è´¥ï¼Œåç»­ç¯å¢ƒæ£€æŸ¥ä¼šæ•è·
    print(f"[è­¦å‘Š] é¢„åŠ è½½éƒ¨åˆ†åº“å¤±è´¥: {e}ã€‚å°†åœ¨ç¯å¢ƒæ£€æŸ¥ä¸­ç¡®è®¤ã€‚")
    torch = None
    torchaudio = None
    AudioSegment = None
    pydantic = None

# =============================================================================
# --- ç¬¬ 1 æ­¥: å…¨å±€é…ç½® ---
# =============================================================================

# -- A. MixFileCLI é…ç½® --
mixfile_config_yaml = """
uploader: "çº¿è·¯A2"
upload_task: 10
upload_retry: 10
download_task: 5
chunk_size: 1024
port: 4719
host: "0.0.0.0"
password: ""
webdav_path: "data.mix_dav"
history: "history.mix_list"
"""

# -- B. åŠ å¯†çš„æœåŠ¡é…ç½® --
# !!! é‡è¦ !!!
# åœ¨è¿™é‡Œç²˜è´´ä½ ä» encrypt_util.py å·¥å…·ä¸­è·å¾—çš„åŠ å¯†é…ç½®å­—ç¬¦ä¸²
# ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²ç”¨äº FRP (åå‘ä»£ç†)
ENCRYPTED_FRP_CONFIG = "gAAAAABovr0nLhcvqidm1uzqWvVToI9StZZAAa9vgtqoxlXSKuMosiX8yEzEIwPrDghud3lGK0zRB0pLsc14E8Bvbk8OxSQVL_NT31PHrXsz6ulKWJYo6fNU3ycOuBoeSWLkou4-HP33INu5SyWH5rnLqvGW8KuFcZtV9qtM-A3zX-dDzDMdhBo5NeS_u5yHpCKy478r029Y"

# ç¬¬äºŒä¸ªå­—ç¬¦ä¸²ç”¨äºå­—å¹•æœåŠ¡ (Gemini API å¯†é’¥ç­‰)
ENCRYPTED_SUBTITLE_CONFIG = "gAAAAABovr1QY48Fe_jcPlBAYeYyO8Dx2woxKVlQcukRmw-Uh0mVw63YT0OBVdmXMaF5Ksj-zTZSdjAvJtpht7sjx7WtjMSXeYzB_dzqrb43X7zqGN_T-blYp285D33QUnr0tfCVR4j9BuTP7KquQNm7yrFhV0zOXOnherA2OoP9htPWJ3Z45igpAj_LxYh4cOXRtPMCGdRza5vnBjlgWyuQVTg6p-1kYfiPi9lGzwgGoqoSsfJkK69TFWUM9IRrrIIz56urbQOsvYP7wtM1pqJ0ReKKRSWq6A4mXVJKMJj0Su1tuQTPSX8otiM3Y2EYwFy6J7JXdVs6zbTSRFzzQ05JXSpEl0eZ62G91ZnBtmpOArrUvqh0JsWfWVUk3D2-ijxfBnVcJ3xu1y6slW2jPtS7J2PBI_lgi939-aTecrKYl-yPj7XbxeAvSZecl_tJj8l0p1XMDmFWNol_A4UeRxI8exMCQd9QCXyTT07kIo2CF5r32FF0yPIlCdKmw-0petX2wnYxnkrG" # ç¤ºä¾‹ï¼Œè¯·æ›¿æ¢

# -- C. æœ¬åœ°æœåŠ¡ä¸ç«¯å£é…ç½® --
MIXFILE_LOCAL_PORT = 4719
FLASK_API_LOCAL_PORT = 5000
MIXFILE_REMOTE_PORT = 20000  # æ˜ å°„åˆ°å…¬ç½‘çš„ MixFile ç«¯å£
FLASK_API_REMOTE_PORT = 20001  # æ˜ å°„åˆ°å…¬ç½‘çš„ Flask API ç«¯å£

# -- D. Killer API & è¿›ç¨‹ç®¡ç†é…ç½® --
KILLER_API_SHUTDOWN_TOKEN = "123456" # !!! å¼ºçƒˆå»ºè®®ä¿®æ”¹ !!!
PROCESS_KEYWORDS_TO_KILL = ["java", "frpc", "python -c"] # æ–°å¢pythonå­è¿›ç¨‹å…³é”®è¯
EXCLUDE_KEYWORDS_FROM_KILL = ["jupyter", "kernel", "ipykernel", "conda", "grep"]

# -- E. å­—å¹•æå–æµç¨‹é…ç½® --
# è¿™äº›å€¼æœªæ¥ä¹Ÿå¯ä»¥åŠ å…¥åˆ°åŠ å¯†é…ç½®ä¸­
SUBTITLE_CHUNK_DURATION_MS = 10 * 60 * 1000  # 10åˆ†é’Ÿ
SUBTITLE_BATCH_SIZE = 40
SUBTITLE_CONCURRENT_REQUESTS = 8
SUBTITLE_REQUESTS_PER_MINUTE = 8

# =============================================================================
# --- ç¬¬ 2 æ­¥: è§£å¯†ä¸é…ç½®åŠ è½½æ¨¡å— ---
# =============================================================================

class DecryptionError(Exception):
    # è‡ªå®šä¹‰å¼‚å¸¸ï¼Œç”¨äºè¡¨ç¤ºè§£å¯†è¿‡ç¨‹ä¸­çš„ç‰¹å®šå¤±è´¥ã€‚
    pass

def _get_decryption_cipher():
    #
    # è¾…åŠ©å‡½æ•°ï¼šä» Kaggle Secrets è·å–å¯†ç å¹¶ç”Ÿæˆ Fernet è§£å¯†å™¨ã€‚
    # è¿™æ˜¯ä¸€ä¸ªå…±äº«é€»è¾‘ï¼Œè¢«å…¶ä»–è§£å¯†å‡½æ•°è°ƒç”¨ã€‚
    #
    if not UserSecretsClient or not Fernet:
        raise DecryptionError("å…³é”®åº“ (kaggle_secrets, cryptography) æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥ã€‚")
    
    print("ğŸ” æ­£åœ¨ä» Kaggle Secrets è·å–è§£å¯†å¯†é’¥ 'FRP_DECRYPTION_KEY'...")
    try:
        secrets = UserSecretsClient()
        decryption_key_password = secrets.get_secret("FRP_DECRYPTION_KEY")
        if not decryption_key_password:
             raise ValueError("Kaggle Secret 'FRP_DECRYPTION_KEY' çš„å€¼ä¸ºç©ºã€‚")
    except Exception as e:
        print(f"âŒ æ— æ³•ä» Kaggle Secrets è·å–å¯†é’¥ï¼è¯·ç¡®ä¿ä½ å·²æ­£ç¡®è®¾ç½®äº†åä¸º 'FRP_DECRYPTION_KEY' çš„ Secretã€‚")
        raise DecryptionError(f"è·å– Kaggle Secret å¤±è´¥: {e}") from e

    # ä½¿ç”¨ SHA256 ä»ç”¨æˆ·å¯†ç æ´¾ç”Ÿä¸€ä¸ªç¡®å®šæ€§çš„ã€å®‰å…¨çš„32å­—èŠ‚å¯†é’¥
    key = base64.urlsafe_b64encode(hashlib.sha256(decryption_key_password.encode()).digest())
    return Fernet(key)

def get_decrypted_config(encrypted_string: str, config_name: str) -> dict:
    #
    # é€šç”¨çš„è§£å¯†å‡½æ•°ï¼Œç”¨äºè§£å¯†ä»»ä½•ç»™å®šçš„åŠ å¯†å­—ç¬¦ä¸²ã€‚
    #
    # Args:
    #     encrypted_string (str): ä» encrypt_util.py è·å–çš„ Base64 ç¼–ç çš„åŠ å¯†å­—ç¬¦ä¸²ã€‚
    #     config_name (str): é…ç½®çš„åç§°ï¼Œç”¨äºæ—¥å¿—è¾“å‡º (ä¾‹å¦‚ "FRP", "Subtitle")ã€‚
    #
    # Returns:
    #     dict: è§£å¯†å¹¶è§£æåçš„é…ç½®å­—å…¸ã€‚
    # 
    # Raises:
    #     DecryptionError: å¦‚æœè§£å¯†è¿‡ç¨‹çš„ä»»ä½•æ­¥éª¤å¤±è´¥ã€‚
    #
    print(f"ğŸ”‘ æ­£åœ¨å‡†å¤‡è§£å¯† {config_name} é…ç½®...")
    try:
        if "PASTE_YOUR_ENCRYPTED" in encrypted_string:
             raise ValueError(f"æ£€æµ‹åˆ°å ä½ç¬¦åŠ å¯†å­—ç¬¦ä¸²ï¼Œè¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®é…ç½®ã€‚")
        
        cipher = _get_decryption_cipher()
        decrypted_bytes = cipher.decrypt(encrypted_string.encode('utf-8'))
        config = json.loads(decrypted_bytes.decode('utf-8'))
        print(f"âœ… {config_name} é…ç½®è§£å¯†æˆåŠŸï¼")
        return config
    except ValueError as e:
        print(f"âŒ è§£å¯† {config_name} é…ç½®å¤±è´¥: {e}")
        raise DecryptionError(f"é…ç½®å­—ç¬¦ä¸²æ— æ•ˆ: {e}")
    except Exception as e:
        print(f"âŒ è§£å¯† {config_name} é…ç½®å¤±è´¥ï¼")
        print("   è¿™é€šå¸¸æ„å‘³ç€ä½ çš„ Kaggle Secret (å¯†ç ) ä¸åŠ å¯†æ—¶ä½¿ç”¨çš„å¯†ç ä¸åŒ¹é…ï¼Œ")
        print("   æˆ–è€…åŠ å¯†å­—ç¬¦ä¸²å·²æŸåã€‚")
        raise DecryptionError(f"è§£å¯†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}") from e
        
# =============================================================================
# --- ç¬¬ 3 æ­¥: ç¯å¢ƒæ£€æŸ¥ã€è¾…åŠ©å‡½æ•°ä¸ Killer API ---
# =============================================================================

def check_environment():
    #
    # æ£€æŸ¥è¿è¡Œæ‰€éœ€çš„æ‰€æœ‰å…³é”®ä¾èµ–å’Œåº“æ˜¯å¦éƒ½å·²æ­£ç¡®å®‰è£…ã€‚
    # å¦‚æœç¼ºå°‘å…³é”®ç»„ä»¶ï¼Œåˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œä½¿ç¨‹åºæå‰å¤±è´¥ã€‚
    #
    print("\n" + "="*60)
    print("ğŸ”¬ æ­£åœ¨æ‰§è¡Œç¯å¢ƒä¾èµ–æ£€æŸ¥...")
    print("="*60)
    
    # æ£€æŸ¥åŸºç¡€åº“
    if not UserSecretsClient or not Fernet:
        raise RuntimeError("å…³é”®å®‰å…¨åº“ 'kaggle_secrets' æˆ– 'cryptography' æœªæ‰¾åˆ°ã€‚æ— æ³•ç»§ç»­ã€‚")
    print("âœ… å®‰å…¨åº“ (kaggle_secrets, cryptography) - æ­£å¸¸")
    
    # æ£€æŸ¥å­—å¹•æå–æ ¸å¿ƒåº“
    critical_subtitle_libs = {
        "torch": torch,
        "torchaudio": torchaudio,
        "pydub": AudioSegment,
        "pydantic": pydantic
    }
    for name, lib in critical_subtitle_libs.items():
        if not lib:
            raise RuntimeError(f"å­—å¹•æå–æ ¸å¿ƒåº“ '{name}' æœªæ‰¾åˆ°æˆ–å¯¼å…¥å¤±è´¥ã€‚è¯·æ£€æŸ¥ Kaggle ç¯å¢ƒã€‚")
        print(f"âœ… å­—å¹•åº“ ({name}) - æ­£å¸¸")

    # æ£€æŸ¥å‘½ä»¤è¡Œå·¥å…·
    if not shutil.which("ffmpeg"):
        raise RuntimeError("'ffmpeg' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¿™æ˜¯æå–éŸ³é¢‘æ‰€å¿…éœ€çš„ã€‚")
    print("âœ… å‘½ä»¤è¡Œå·¥å…· (ffmpeg) - æ­£å¸¸")
    
    if not shutil.which("java"):
        raise RuntimeError("'java' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¿™æ˜¯è¿è¡Œ MixFileCLI æ‰€å¿…éœ€çš„ã€‚")
    print("âœ… å‘½ä»¤è¡Œå·¥å…· (java) - æ­£å¸¸")

    print("\nâœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼æ‰€æœ‰å…³é”®ä¾èµ–å‡å·²å°±ç»ªã€‚\n")


def log_system_event(level: str, message: str, in_worker=False):
    # ä¸€ä¸ªç®€å•çš„å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—è®°å½•å™¨ã€‚
    # æ–°å¢ in_worker å‚æ•°ä»¥åŒºåˆ†æ—¥å¿—æ¥æº
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
    worker_tag = "[WORKER]" if in_worker else "[SYSTEM]"
    prefix = f"[{timestamp} {worker_tag} {level.upper()}]"
    print(f"{prefix} {message}", flush=True)


def ms_to_srt_time(ms: int) -> str:
    # å°†æ¯«ç§’è½¬æ¢ä¸º SRT å­—å¹•æ–‡ä»¶çš„æ—¶é—´æ ¼å¼ (HH:MM:SS,ms)ã€‚
    try:
        ms = int(ms)
    except (ValueError, TypeError):
        ms = 0
    td = timedelta(milliseconds=ms)
    hours, remainder = divmod(td.seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    milliseconds = td.microseconds // 1000
    return f"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}"


def run_command(command: str, log_file: str = None) -> subprocess.Popen:
    #
    # åœ¨åå°ä»¥éé˜»å¡æ–¹å¼æ‰§è¡Œä¸€ä¸ª shell å‘½ä»¤ã€‚
    #
    # Args:
    #     command (str): è¦æ‰§è¡Œçš„å‘½ä»¤ã€‚
    #     log_file (str, optional): å°† stdout å’Œ stderr é‡å®šå‘åˆ°çš„æ—¥å¿—æ–‡ä»¶åã€‚
    #
    # Returns:
    #     subprocess.Popen: æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹å¯¹è±¡ã€‚
    #
    log_system_event("info", f"æ‰§è¡Œå‘½ä»¤: {command}")
    stdout_dest = None
    stderr_dest = None
    if log_file:
        log_handle = open(log_file, 'w', encoding='utf-8')
        stdout_dest = log_handle
        stderr_dest = log_handle
    
    # ä½¿ç”¨ Popen ä»¥éé˜»å¡æ–¹å¼å¯åŠ¨è¿›ç¨‹
    process = subprocess.Popen(
        command,
        shell=True,
        stdout=stdout_dest,
        stderr=stderr_dest,
        encoding='utf-8',
        errors='ignore'
    )
    return process


def wait_for_port(port: int, host: str = '127.0.0.1', timeout: float = 60.0) -> bool:
    #
    # ç­‰å¾…æŒ‡å®šçš„æœ¬åœ°ç«¯å£å˜ä¸ºå¯ç”¨çŠ¶æ€ã€‚
    #
    # Args:
    #     port (int): è¦æ£€æŸ¥çš„ç«¯å£å·ã€‚
    #     host (str, optional): ç›‘å¬çš„ä¸»æœºåœ°å€ã€‚é»˜è®¤ä¸º '127.0.0.1'ã€‚
    #     timeout (float, optional): æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ã€‚é»˜è®¤ä¸º 60.0ã€‚
    #
    # Returns:
    #     bool: å¦‚æœç«¯å£åœ¨è¶…æ—¶å‰å˜ä¸ºå¯ç”¨ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    #
    log_system_event("info", f"æ­£åœ¨ç­‰å¾…ç«¯å£ {host}:{port} å¯åŠ¨...")
    start_time = time.perf_counter()
    while True:
        try:
            with socket.create_connection((host, port), timeout=1):
                log_system_event("info", f"âœ… ç«¯å£ {port} å·²æˆåŠŸå¯åŠ¨ï¼")
                return True
        except (socket.timeout, ConnectionRefusedError):
            time.sleep(1)
            if time.perf_counter() - start_time >= timeout:
                log_system_event("error", f"âŒ ç­‰å¾…ç«¯å£ {port} è¶…æ—¶ ({timeout}ç§’)ã€‚")
                return False

# --- Killer API é€»è¾‘ (ç”¨äºè¿œç¨‹å…³é—­) ---

def _find_and_kill_targeted_processes(signal_to_send=signal.SIGTERM):
    # æŸ¥æ‰¾å¹¶ç»ˆæ­¢æ­¤è„šæœ¬å¯åŠ¨çš„æ‰€æœ‰å…³é”®å­è¿›ç¨‹ (java, frpc, python -c ...)ã€‚
    killed_pids_info = []
    log_system_event("info", f"æŸ¥æ‰¾å¹¶å°è¯•ç»ˆæ­¢ä¸ '{PROCESS_KEYWORDS_TO_KILL}' ç›¸å…³çš„è¿›ç¨‹...")
    
    current_kernel_pid = os.getpid()
    pids_to_target = set()

    try:
        # ä½¿ç”¨ ps å‘½ä»¤è·å–æ‰€æœ‰è¿›ç¨‹ä¿¡æ¯
        ps_output = subprocess.check_output(['ps', '-eo', 'pid,ppid,args'], text=True)
        
        for line in ps_output.splitlines()[1:]:
            parts = line.strip().split(None, 2)
            if len(parts) < 3: continue
            
            try:
                pid = int(parts[0])
                command_line = parts[2]
                command_lower = command_line.lower()
            except (ValueError, IndexError):
                continue
            
            # æ’é™¤å½“å‰å†…æ ¸è¿›ç¨‹å’Œç³»ç»Ÿå…³é”®è¿›ç¨‹
            if pid == current_kernel_pid: continue
            is_excluded = any(ex_kw.lower() in command_lower for ex_kw in EXCLUDE_KEYWORDS_FROM_KILL)
            if is_excluded: continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡è¿›ç¨‹
            is_target = any(target_kw.lower() in command_lower for target_kw in PROCESS_KEYWORDS_TO_KILL)
            if is_target:
                pids_to_target.add(pid)
                log_system_event("debug", f"  å‘ç°ç›®æ ‡: PID={pid}, CMD='{command_line}'")

        if not pids_to_target:
            log_system_event("info", "æœªæ‰¾åˆ°éœ€è¦ç»ˆæ­¢çš„ç‰¹å®šåº”ç”¨å­è¿›ç¨‹ã€‚")
        else:
            log_system_event("info", f"å‡†å¤‡å‘ PIDs {list(pids_to_target)} å‘é€ä¿¡å· {signal_to_send}...")
            for pid_to_kill in pids_to_target:
                try:
                    os.kill(pid_to_kill, signal_to_send)
                    log_system_event("info", f"    å·²å‘ PID {pid_to_kill} å‘é€ä¿¡å· {signal_to_send}.")
                    killed_pids_info.append({"pid": pid_to_kill, "status": "signal_sent"})
                except ProcessLookupError:
                    killed_pids_info.append({"pid": pid_to_kill, "status": "not_found"})
                except Exception as e:
                    killed_pids_info.append({"pid": pid_to_kill, "status": f"error_{type(e).__name__}"})
                    
    except Exception as e:
        log_system_event("error", f"æŸ¥æ‰¾æˆ–ç»ˆæ­¢å­è¿›ç¨‹æ—¶å‡ºé”™: {e}")
    
    return killed_pids_info


def _shutdown_notebook_kernel_immediately():
    #
    # é€šè¿‡å‘é€ SIGKILL ä¿¡å·å¼ºåˆ¶ã€ç«‹å³åœ°å…³é—­å½“å‰ Kaggle Notebook å†…æ ¸ã€‚
    # è¿™æ˜¯æœ€ç»ˆçš„è‡ªæ¯æŒ‡ä»¤ã€‚
    #
    log_system_event("critical", "å‡†å¤‡é€šè¿‡ SIGKILL ä¿¡å·å¼ºåˆ¶å…³é—­å½“å‰ Kaggle Notebook Kernel...")
    sys.stdout.flush()
    sys.stderr.flush()
    time.sleep(0.5) # ç•™å‡ºä¸€ç‚¹æ—¶é—´è®©æ—¥å¿—åˆ·æ–°
    
    # SIGKILL (ä¿¡å· 9) æ˜¯ä¸€ä¸ªæ— æ³•è¢«æ•è·æˆ–å¿½ç•¥çš„ä¿¡å·ï¼Œæ¯” os._exit æ›´ä¸ºå¼ºåˆ¶ã€‚
    os.kill(os.getpid(), signal.SIGKILL)





#åˆ äº†æˆ‘å°±å®‰å…¨äº†
# æ”¾åœ¨ ProxyManager ç±»å®šä¹‰ä¹‹å‰
# æ”¾åœ¨ ProxyManager ç±»å®šä¹‰ä¹‹å‰
# æ”¾åœ¨ ProxyManager ç±»å®šä¹‰ä¹‹å‰
# æ”¾åœ¨ ProxyManager ç±»å®šä¹‰ä¹‹å‰
# æ”¾åœ¨ ProxyManager ç±»å®šä¹‰ä¹‹å‰
# æ”¾åœ¨ ProxyManager ç±»å®šä¹‰ä¹‹å‰
# æ”¾åœ¨ ProxyManager ç±»å®šä¹‰ä¹‹å‰
class WarpManager:
    """ã€å®‰è£…æºä¿®å¤ç‰ˆã€‘ç»•è¿‡aptï¼Œæ‰‹åŠ¨ä¸‹è½½å¹¶å®‰è£…æœ€æ–°ç‰ˆçš„ WARP å®¢æˆ·ç«¯ã€‚"""
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(WarpManager, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        self.warp_cli_path = Path("/usr/bin/warp-cli")
        self.is_connected = False
        self._initialized = True
        self._warp_svc_process = None
        self.warp_log_path = "/tmp/warp-svc.log"

    def _install_warp(self):
        if self.warp_cli_path.exists():
            # å³ä½¿æ–‡ä»¶å­˜åœ¨ï¼Œæˆ‘ä»¬ä¹Ÿæ£€æŸ¥ä¸€ä¸‹ç‰ˆæœ¬ï¼Œé˜²æ­¢æ˜¯æ—§ç‰ˆæœ¬æ®‹ç•™
            try:
                version_proc = subprocess.run("warp-cli --version", shell=True, capture_output=True, text=True)
                if "2025." in version_proc.stdout or "2024." in version_proc.stdout: # æ£€æŸ¥æ˜¯å¦æ˜¯è¾ƒæ–°çš„å¹´ä»½ç‰ˆæœ¬
                    log_system_event("info", "æ£€æµ‹åˆ°æ–°ç‰ˆ WARP å®¢æˆ·ç«¯å·²å®‰è£…ï¼Œè·³è¿‡ã€‚", in_worker=True)
                    return True
            except Exception:
                pass # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œå°±ç»§ç»­å®‰è£…

        log_system_event("info", "æ­£åœ¨æ‰‹åŠ¨ä¸‹è½½å¹¶å®‰è£…æœ€æ–°ç‰ˆ Cloudflare WARP å®¢æˆ·ç«¯...", in_worker=True)
        try:
            # ã€æ ¸å¿ƒä¿®å¤ã€‘ç›´æ¥ä»å®˜æ–¹å‘å¸ƒé¡µä¸‹è½½ .deb åŒ…
            warp_deb_url = "https://pkg.cloudflareclient.com/uploads/cloudflare-warp-arm64.deb" # <--- æ³¨æ„: ç¡®è®¤Kaggleçš„CPUæ¶æ„
            
            # æ£€æŸ¥Kaggleçš„CPUæ¶æ„
            arch_proc = subprocess.run("dpkg --print-architecture", shell=True, capture_output=True, text=True)
            arch = arch_proc.stdout.strip()
            if arch == "amd64":
                warp_deb_url = "https://pkg.cloudflareclient.com/uploads/cloudflare-warp_2024.5.263-1_amd64.deb"
            elif arch == "arm64":
                 warp_deb_url = "https://pkg.cloudflareclient.com/uploads/cloudflare-warp_2024.5.263-1_arm64.deb"
            else:
                log_system_event("error", f"ä¸æ”¯æŒçš„CPUæ¶æ„: {arch}", in_worker=True)
                return False

            log_system_event("info", f"æ£€æµ‹åˆ°æ¶æ„: {arch}, ä¸‹è½½URL: {warp_deb_url}", in_worker=True)

            install_cmd = (
                f"wget -q -O /tmp/cloudflare-warp.deb {warp_deb_url} && "
                "sudo dpkg -i /tmp/cloudflare-warp.deb"
            )
            
            env = os.environ.copy()
            env["DEBIAN_FRONTEND"] = "noninteractive"
            
            process = subprocess.run(install_cmd, shell=True, capture_output=True, text=True, env=env)
            
            # dpkg å®‰è£…å¯èƒ½ä¼šå› ä¸ºä¾èµ–é—®é¢˜è¿”å›é0ä»£ç ï¼Œä½†åªè¦ warp-cli å®‰è£…æˆåŠŸå°±è¡Œ
            if not self.warp_cli_path.exists():
                log_system_event("error", f"WARP æ‰‹åŠ¨å®‰è£…å¤±è´¥: {process.stderr}", in_worker=True)
                return False
            
            log_system_event("info", "âœ… WARP å®¢æˆ·ç«¯æ‰‹åŠ¨å®‰è£…/æ›´æ–°æˆåŠŸã€‚", in_worker=True)
            return True
        except Exception as e:
            log_system_event("error", f"æ‰‹åŠ¨å®‰è£… WARP æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", in_worker=True)
            return False

    # ... _start_warp_daemon æ–¹æ³•ä¿æŒä¸å˜ ...
    def _start_warp_daemon(self):
        try:
            subprocess.run(['pgrep', '-f', 'warp-svc'], check=True, capture_output=True)
            return True
        except subprocess.CalledProcessError:
            log_system_event("info", "æ­£åœ¨å¯åŠ¨ WARP å®ˆæŠ¤è¿›ç¨‹...", in_worker=True)
            self._warp_svc_process = run_command(f"/usr/bin/warp-svc > {self.warp_log_path} 2>&1")
            time.sleep(3)
            try:
                subprocess.run(['pgrep', '-f', 'warp-svc'], check=True, capture_output=True)
                log_system_event("info", "âœ… WARP å®ˆæŠ¤è¿›ç¨‹å¯åŠ¨æˆåŠŸã€‚", in_worker=True)
                return True
            except subprocess.CalledProcessError:
                log_system_event("error", "WARP å®ˆæŠ¤è¿›ç¨‹å¯åŠ¨å¤±è´¥ï¼", in_worker=True)
                return False

    # ... connect å’Œ disconnect æ–¹æ³•å¯ä»¥æ¢å¤ä½¿ç”¨ä¸Šä¸€ç‰ˆä¸ºæ–°ç‰ˆè®¾è®¡çš„å‘½ä»¤æµ ...
    def connect(self):
        if self.is_connected:
            return True
            
        if not self._install_warp() or not self._start_warp_daemon():
            return False

        log_system_event("info", "æ­£åœ¨é…ç½®å¹¶è¿æ¥åˆ° Cloudflare WARP (Proxy Mode)...", in_worker=True)
        try:
            # æ¢å¤ä½¿ç”¨ä¸ºæ–°ç‰ˆæœ¬è®¾è®¡çš„ã€å¥å£®çš„å‘½ä»¤åºåˆ—
            run_command("warp-cli --accept-tos disconnect").wait()
            time.sleep(1)
            
            run_command("warp-cli --accept-tos registration delete").wait()
            time.sleep(1)
            
            run_command("warp-cli --accept-tos registration new").wait()
            time.sleep(2)

            # ç°åœ¨ set-mode åº”è¯¥å­˜åœ¨äº†
            run_command("warp-cli set-mode proxy").wait()
            time.sleep(1)

            run_command("warp-cli set-proxy-port 40000").wait()
            time.sleep(1)

            run_command("warp-cli --accept-tos connect").wait()
            time.sleep(5)

            status_proc = subprocess.run("warp-cli status", shell=True, capture_output=True, text=True)
            if "Status: Connected" in status_proc.stdout and "Mode: Proxy" in status_proc.stdout:
                log_system_event("info", "âœ… WARP åœ¨ Proxy Mode ä¸‹è¿æ¥æˆåŠŸï¼", in_worker=True)
                self.is_connected = True
                return True
            else:
                log_system_event("error", f"WARP è¿æ¥æˆ–è¿›å…¥Proxy Modeå¤±è´¥ã€‚çŠ¶æ€è¯¦æƒ…: {status_proc.stdout.strip()}", in_worker=True)
                return False
        except Exception as e:
            log_system_event("error", f"è¿æ¥ WARP æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", in_worker=True)
            return False
            
    def disconnect(self):
        if self.is_connected and self.warp_cli_path.exists():
            log_system_event("info", "æ­£åœ¨æ–­å¼€ WARP è¿æ¥...", in_worker=True)
            run_command("warp-cli disconnect").wait()
            self.is_connected = False
# =============================================================================
# --- (æ–°å¢æ¨¡å—) ç¬¬ 3.5 æ­¥: V2Ray ä»£ç†ç®¡ç†å™¨ ---
# =============================================================================

class ProxyManager:
    """
    ã€WARP é›†æˆç‰ˆã€‘æŒ‰éœ€ä¸ºä¸Šä¼ ä»»åŠ¡å¯»æ‰¾æœ€ä¼˜ä»£ç†çº¿è·¯çš„å·¥å…·ç±»ï¼Œ
    å¹¶å°† Cloudflare WARP ä½œä¸ºä¸€ä¸ªç‰¹æ®Šçš„æµ‹é€ŸèŠ‚ç‚¹ã€‚
    """
    def __init__(self, sub_url=None):
        self.sub_url = sub_url
        self.xray_path = Path("/kaggle/working/xray")
        self.config_path = Path("/kaggle/working/xray_config.json")
        self.local_xray_socks_port = 10808
        self.local_warp_socks_port = 40000 # WARP é»˜è®¤çš„ SOCKS5 ç«¯å£
        self.geoip_path = Path("/kaggle/working/geoip.dat")
        self.geosite_path = Path("/kaggle/working/geosite.dat")
        self.warp_manager = WarpManager() # åˆå§‹åŒ– WARP ç®¡ç†å™¨


    def _ensure_xray_assets(self):
        """ç¡®ä¿ Xray æ ¸å¿ƒåŠæ•°æ®åº“æ–‡ä»¶å·²ä¸‹è½½ã€‚"""
        if self.xray_path.exists() and self.geoip_path.exists() and self.geosite_path.exists():
            return

        log_system_event("info", "æ£€æµ‹åˆ°Xrayç»„ä»¶ç¼ºå¤±ï¼Œæ­£åœ¨ä¸‹è½½...", in_worker=True)
        xray_url = "https://github.com/XTLS/Xray-core/releases/download/v1.8.10/Xray-linux-64.zip"
        geoip_url = "https://github.com/Loyalsoldier/v2ray-rules-dat/releases/latest/download/geoip.dat"
        geosite_url = "https://github.com/Loyalsoldier/v2ray-rules-dat/releases/latest/download/geosite.dat"
        zip_path = Path("/kaggle/working/xray.zip")
        
        try:
            if not self.xray_path.exists():
                log_system_event("info", "  -> ä¸‹è½½ Xray core...", in_worker=True)
                run_command(f"wget -q -O {zip_path} {xray_url}").wait()
                run_command(f"unzip -o {zip_path} xray -d /kaggle/working/").wait()
                self.xray_path.chmod(0o755)
                zip_path.unlink()
            if not self.geoip_path.exists():
                log_system_event("info", "  -> ä¸‹è½½ geoip.dat...", in_worker=True)
                run_command(f"wget -q -O {self.geoip_path} {geoip_url}").wait()
            if not self.geosite_path.exists():
                log_system_event("info", "  -> ä¸‹è½½ geosite.dat...", in_worker=True)
                run_command(f"wget -q -O {self.geosite_path} {geosite_url}").wait()
            log_system_event("info", "âœ… Xrayç»„ä»¶ä¸‹è½½å®Œæˆã€‚", in_worker=True)
        except Exception as e:
            raise RuntimeError(f"ä¸‹è½½ Xray ç»„ä»¶å¤±è´¥: {e}")

    def _fetch_and_parse_subscription(self):
        """è·å–å¹¶è§£æè®¢é˜…é“¾æ¥ï¼Œè¿”å›èŠ‚ç‚¹é“¾æ¥åˆ—è¡¨ã€‚"""
        if not self.sub_url:
            return []
        log_system_event("info", f"æ­£åœ¨ä» {self.sub_url[:30]}... è·å–è®¢é˜…...", in_worker=True)
        try:
            response = requests.get(self.sub_url, timeout=20)
            response.raise_for_status()
            decoded_content = base64.b64decode(response.content).decode('utf-8')
            return decoded_content.strip().split('\n')
        except Exception as e:
            log_system_event("error", f"è·å–æˆ–è§£æè®¢é˜…å¤±è´¥: {e}", in_worker=True)
            return []

    def _generate_node_config(self, node_url):
        """æ ¹æ®èŠ‚ç‚¹URLç”ŸæˆXrayçš„JSONé…ç½®ï¼ˆåŒ…å«è·¯ç”±ï¼‰ã€‚"""
        try:
            parsed_url = urlparse(node_url)
            node_name_raw = parsed_url.fragment
            node_name = unquote(node_name_raw) if node_name_raw else "Unnamed Node"
            
            # --- æ­¥éª¤ 1: æ„å»º Outbounds ---
            #
            # Proxy Outbound (tag: "proxy")
            protocol = parsed_url.scheme
            proxy_outbound = {"protocol": protocol, "settings": {}, "tag": "proxy"}
            
            if protocol == "vmess":
                try:
                    decoded_vmess_str = base64.b64decode(parsed_url.netloc).decode('utf-8')
                    decoded_vmess = json.loads(decoded_vmess_str)
                except Exception:
                     return None, f"Invalid VMess format for node {node_name}"

                proxy_outbound["settings"]["vnext"] = [{
                    "address": decoded_vmess["add"],
                    "port": int(decoded_vmess["port"]),
                    "users": [{"id": decoded_vmess["id"], "alterId": int(decoded_vmess["aid"]), "security": decoded_vmess.get("scy", "auto")}]
                }]
                stream_settings = {"network": decoded_vmess.get("net", "tcp")}
                if stream_settings["network"] == "ws":
                    ws_settings = {"path": decoded_vmess.get("path", "/")}
                    host = decoded_vmess.get("host")
                    if host:
                        ws_settings["headers"] = {"Host": host}
                    stream_settings["wsSettings"] = ws_settings
                if decoded_vmess.get("tls", "") == "tls":
                     stream_settings["security"] = "tls"
                     stream_settings["tlsSettings"] = {"serverName": decoded_vmess.get("sni", decoded_vmess.get("host", decoded_vmess["add"]))}
                proxy_outbound["streamSettings"] = stream_settings

            elif protocol == "vless":
                qs = parse_qs(parsed_url.query)
                user_obj = { "id": parsed_url.username, "encryption": "none", "flow": qs.get("flow", [None])[0], "alterId": 0, "security": "auto" }
                if user_obj["flow"] is None: del user_obj["flow"]
                proxy_outbound["settings"]["vnext"] = [{"address": parsed_url.hostname, "port": int(parsed_url.port), "users": [user_obj]}]
                stream_settings = {"network": qs.get("type", ["tcp"])[0]}
                if stream_settings["network"] == "ws":
                    ws_path = unquote(qs.get("path", ["/"])[0])
                    ws_host = unquote(qs.get("host", [""])[0])
                    ws_settings = {"path": ws_path}
                    if ws_host: ws_settings["headers"] = {"Host": ws_host}
                    stream_settings["wsSettings"] = ws_settings
                if qs.get("security", ["none"])[0] == "tls":
                    stream_settings["security"] = "tls"
                    tls_sni = unquote(qs.get("sni", [""])[0]) or unquote(qs.get("host", [""])[0]) or parsed_url.hostname
                    fp = qs.get("fp", ["random"])[0]
                    tls_settings = {"serverName": tls_sni, "fingerprint": fp, "allowInsecure": False, "show": False}
                    alpn = qs.get("alpn")
                    if alpn: tls_settings["alpn"] = [val for val in alpn[0].split(',') if val]
                    stream_settings["tlsSettings"] = tls_settings
                proxy_outbound["streamSettings"] = stream_settings
            else:
                return None, f"Unsupported protocol: {protocol}"

            # Direct Outbound (tag: "direct")
            direct_outbound = {"protocol": "freedom", "settings": {}, "tag": "direct"}
            
            # Block Outbound (tag: "block") - for ads, etc.
            block_outbound = {"protocol": "blackhole", "settings": {}, "tag": "block"}

            # --- æ­¥éª¤ 2: æ„å»ºæœ€ç»ˆé…ç½® ---
            config = {
                "log": {"loglevel": "warning"},
                "dns": { "servers": ["8.8.8.8", "1.1.1.1", "localhost"] },
                "inbounds": [{
                    "port": self.local_socks_port,
                    "protocol": "socks",
                    "listen": "127.0.0.1",
                    "settings": {"auth": "noauth", "udp": True},
                    "sniffing": { "enabled": True, "destOverride": ["http", "tls"] }
                }],
                "outbounds": [
                    proxy_outbound,
                    direct_outbound,
                    block_outbound
                ],
                "routing": {
                    "domainStrategy": "AsIs",
                    "rules": [
                        { "type": "field", "ip": ["geoip:private"], "outboundTag": "direct" },
                        { "type": "field", "domain": ["geosite:category-ads-all"], "outboundTag": "block" },
                    ]
                }
            }
            return config, node_name
        except Exception as e:
            import traceback
            traceback.print_exc()
            return None, f"Error parsing node '{node_name}': {e}"

    def _test_upload_speed(self, test_upload_url, proxies=None):
        """
        ã€çŠ¶æ€éš”ç¦»ä¿®å¤ç‰ˆã€‘æµ‹è¯•ä¸Šä¼ é€Ÿåº¦çš„æ ¸å¿ƒå‡½æ•°ã€‚
        å®ƒæ¥æ”¶ä¸€ä¸ªå®Œæ•´çš„ã€åŒ…å«å”¯ä¸€æ–‡ä»¶åçš„ URL è¿›è¡Œæµ‹è¯•ã€‚
        """
        try:
            # ä½¿ç”¨è¾ƒå°çš„æµ‹è¯•æ•°æ®ä»¥åŠ å¿«æµ‹é€Ÿè¿‡ç¨‹
            test_data_size = 3 * 1024 * 1024  # 1MB
            test_data = os.urandom(test_data_size)
            
            start_time = time.time()
            # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ã€å”¯ä¸€çš„ URL
            response = requests.put(test_upload_url, data=test_data, proxies=proxies, timeout=30)
            end_time = time.time()
            
            response.raise_for_status() # ç¡®ä¿ä¸Šä¼ æˆåŠŸ (è¿”å› 2xx çŠ¶æ€ç )

            # æ³¨æ„ï¼šæˆ‘ä»¬ä¸å†å°è¯•åˆ é™¤æµ‹é€Ÿæ–‡ä»¶ï¼Œå› ä¸ºå®ƒä»¬çš„æ–‡ä»¶åæ˜¯å”¯ä¸€çš„ï¼Œ
            # ç•™å­˜åœ¨æœåŠ¡å™¨ä¸Šä¸ä¼šå¯¹åç»­æ“ä½œäº§ç”Ÿå†²çªã€‚
            # MixFileCLI æœåŠ¡é‡å¯æˆ–æ‰‹åŠ¨æ¸…ç†å³å¯ã€‚

            duration = end_time - start_time
            if duration > 0:
                return (test_data_size / duration) / (1024 * 1024)  # MB/s
            else:
                # å¦‚æœæ—¶é—´è¿‡çŸ­ï¼Œç»™ä¸€ä¸ªæé«˜çš„é€Ÿåº¦å€¼ï¼Œè€Œä¸æ˜¯0ï¼Œä»¥è¡¨ç¤ºè¿æ¥éå¸¸å¿«
                return 999 
        except Exception as e:
            log_system_event("debug", f"æµ‹é€Ÿå¤±è´¥ (proxy: {bool(proxies)}): {type(e).__name__}", in_worker=True)
            return 0

    def get_best_proxy_for_upload(self, api_client_base_url):
        """
        ã€WARP é›†æˆç‰ˆã€‘æ‰§è¡Œå®Œæ•´çš„æŒ‰éœ€æµ‹é€Ÿæµç¨‹ï¼ŒåŒ…å«ç›´è¿ã€WARP å’Œ V2Ray èŠ‚ç‚¹ã€‚
        """
        log_system_event("info", "====== å¼€å§‹æŒ‰éœ€æµ‹é€Ÿ (å«WARP) ======", in_worker=True)
        
        # --- å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€çš„æµ‹è¯•URL ---
        def generate_unique_test_url():
            random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))
            test_filename = f"speed_test_{random_suffix}.tmp"
            return urljoin(api_client_base_url, f"/api/upload/{quote(test_filename)}")

        best_node_config = None
        best_node_speed = 0
        best_node_name = "None"
        best_proxies = None

        # 1. æµ‹è¯•ç›´è¿é€Ÿåº¦
        direct_test_url = generate_unique_test_url()
        direct_speed = self._test_upload_speed(direct_test_url)
        log_system_event("info", f"  -> ç›´è¿é€Ÿåº¦: {direct_speed:.2f} MB/s", in_worker=True)
        if direct_speed > best_node_speed:
            best_node_speed = direct_speed
            best_node_name = "Direct Connection"
            best_proxies = None
            best_node_config = None # ç›´è¿æ²¡æœ‰config

        # 2. æµ‹è¯• WARP é€Ÿåº¦
        if self.warp_manager.connect():
            warp_proxies = {'http': f'socks5h://127.0.0.1:{self.local_warp_socks_port}', 'https': f'socks5h://127.0.0.1:{self.local_warp_socks_port}'}
            warp_test_url = generate_unique_test_url()
            warp_speed = self._test_upload_speed(warp_test_url, proxies=warp_proxies)
            log_system_event("info", f"  -> WARP é€Ÿåº¦: {warp_speed:.2f} MB/s", in_worker=True)
            if warp_speed > best_node_speed:
                best_node_speed = warp_speed
                best_node_name = "Cloudflare WARP"
                best_proxies = warp_proxies
                best_node_config = "WARP" # ä½¿ç”¨ç‰¹æ®Šæ ‡è®°
        else:
            log_system_event("warning", "WARP è¿æ¥å¤±è´¥ï¼Œè·³è¿‡æµ‹é€Ÿã€‚", in_worker=True)
        
        # åœ¨å¼€å§‹Xrayæµ‹è¯•å‰ï¼Œç¡®ä¿WARPæ˜¯æ–­å¼€çš„ï¼Œé¿å…ç½‘ç»œå†²çª
        self.warp_manager.disconnect()

        # 3. å¾ªç¯æµ‹è¯• V2Ray/Xray èŠ‚ç‚¹
        self._ensure_xray_assets()
        node_urls = self._fetch_and_parse_subscription()
        if node_urls:
            num_to_test = 3 if len(node_urls) > 3 else len(node_urls)
            nodes_to_test = random.sample(node_urls, num_to_test)
            log_system_event("info", f"éšæœºé€‰æ‹© {len(nodes_to_test)} ä¸ª V2Ray èŠ‚ç‚¹è¿›è¡Œæµ‹é€Ÿ...", in_worker=True)

            for node_url in nodes_to_test:
                node_config, node_name = self._generate_node_config(node_url.strip())
                if not node_config: continue

                log_system_event("info", f"  -> æ­£åœ¨æµ‹è¯• V2Ray èŠ‚ç‚¹: {node_name}...", in_worker=True)
                with open(self.config_path, 'w') as f: json.dump(node_config, f)
                
                process = run_command(f"{self.xray_path} -c {self.config_path}")
                if not wait_for_port(self.local_xray_socks_port, host='127.0.0.1', timeout=10):
                    process.terminate(); process.wait()
                    continue
                
                xray_proxies = {'http': f'socks5h://127.0.0.1:{self.local_xray_socks_port}', 'https': f'socks5h://127.0.0.1:{self.local_xray_socks_port}'}
                node_test_url = generate_unique_test_url()
                node_speed = self._test_upload_speed(node_test_url, proxies=xray_proxies)
                log_system_event("info", f"     èŠ‚ç‚¹ {node_name} é€Ÿåº¦: {node_speed:.2f} MB/s", in_worker=True)

                if node_speed > best_node_speed:
                    best_node_speed = node_speed
                    best_node_name = node_name
                    best_proxies = xray_proxies
                    best_node_config = node_config

                process.terminate(); process.wait()
                time.sleep(1)

        log_system_event("info", "="*28, in_worker=True)
        log_system_event("info", f"  æœ€ä¼˜çº¿è·¯: {best_node_name}", in_worker=True)
        log_system_event("info", f"  æœ€é«˜é€Ÿåº¦: {best_node_speed:.2f} MB/s", in_worker=True)
        log_system_event("info", "="*28, in_worker=True)
        log_system_event("info", "====== æµ‹é€Ÿç»“æŸ ======", in_worker=True)

        return best_proxies, best_node_config

# =============================================================================
# --- ç¬¬ 4 æ­¥: å­—å¹•æå–æ ¸å¿ƒæ¨¡å— (åœ¨å­è¿›ç¨‹ä¸­è°ƒç”¨) ---
# =============================================================================

# --- A. Pydantic æ•°æ®éªŒè¯æ¨¡å‹ ---
# ç”¨äºä¸¥æ ¼éªŒè¯ Gemini API è¿”å›çš„ JSON ç»“æ„ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚

class SubtitleLine(pydantic.BaseModel):
    start_ms: int
    end_ms: int | None = None
    text: str

class BatchTranscriptionResult(pydantic.BaseModel):
    subtitles: list[SubtitleLine]

def load_ai_models():
    """
    åœ¨å·¥ä½œè¿›ç¨‹å¯åŠ¨æ—¶é¢„åŠ è½½æ‰€æœ‰éœ€è¦çš„ AI æ¨¡å‹ã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«å·²åŠ è½½æ¨¡å‹çš„å­—å…¸ã€‚
    """
    log_system_event("info", "å·¥ä½œè¿›ç¨‹æ­£åœ¨é¢„åŠ è½½ AI æ¨¡å‹...", in_worker=True)
    loaded_models = {
        "denoiser": None
    }
    
    # 1. å°è¯•åŠ è½½ AI é™å™ªæ¨¡å‹
    try:
        from denoiser import pretrained
        log_system_event("info", "æ­£åœ¨åŠ è½½ AI é™å™ªæ¨¡å‹ (denoiser)...", in_worker=True)
        denoiser_model = pretrained.dns64().cuda()
        loaded_models["denoiser"] = denoiser_model
        log_system_event("info", "âœ… AI é™å™ªæ¨¡å‹åŠ è½½æˆåŠŸã€‚", in_worker=True)
    except Exception as e:
        log_system_event("warning", f"é¢„åŠ è½½ AI é™å™ªæ¨¡å‹å¤±è´¥ï¼Œé™å™ªåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚é”™è¯¯: {e}", in_worker=True)

    # 2. VAD æ¨¡å‹è¯´æ˜
    # faster-whisper çš„ VAD åŠŸèƒ½ (get_speech_timestamps) æ˜¯ä¸€ä¸ªè½»é‡çº§å‡½æ•°ï¼Œ
    # ä¸éœ€è¦åƒé™å™ªæ¨¡å‹é‚£æ ·è¿›è¡Œé‡é‡çº§çš„é¢„åŠ è½½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡ŒåªåŠ è½½é™å™ªæ¨¡å‹ã€‚
    
    return loaded_models

# --- B. åŠ¨æ€æç¤ºè¯ä¸æ–‡ä»¶å¤„ç† ---

def get_dynamic_prompts(api_url: str) -> tuple[str, str]:
    #
    # ä»æŒ‡å®šçš„ API è·å–åŠ¨æ€æç¤ºè¯ã€‚å¦‚æœå¤±è´¥ï¼Œåˆ™è¿”å›ç¡¬ç¼–ç çš„å¤‡ç”¨æç¤ºè¯ã€‚
    #
    log_system_event("info", "æ­£åœ¨å°è¯•ä» API è·å–åŠ¨æ€æç¤ºè¯...", in_worker=True)
    try:
        response = requests.get(api_url, timeout=10)
        response.raise_for_status()
        prompts = response.json()
        if "system_instruction" in prompts and "prompt_for_task" in prompts:
            log_system_event("info", "âœ… æˆåŠŸä» API è·å–åŠ¨æ€æç¤ºè¯ã€‚", in_worker=True)
            return prompts['system_instruction'], prompts['prompt_for_task']
        else:
            raise ValueError("API å“åº”ä¸­ç¼ºå°‘å¿…è¦çš„é”®ã€‚")
    except Exception as e:
        log_system_event("warning", f"è·å–åŠ¨æ€æç¤ºè¯å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨å¤‡ç”¨æç¤ºè¯ã€‚", in_worker=True)
        # --- å¤‡ç”¨æç¤ºè¯ (Fallback Prompts) ---
        fallback_system = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­—å¹•ç¿»è¯‘æ¨¡å‹ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æä¾›çš„ä»»ä½•è¯­è¨€çš„éŸ³é¢‘å†…å®¹ç¿»è¯‘æˆ**ç®€ä½“ä¸­æ–‡**ã€‚"
            "åœ¨ä½ çš„æ‰€æœ‰è¾“å‡ºä¸­ï¼Œ`text` å­—æ®µçš„å€¼**å¿…é¡»æ˜¯ç®€ä½“ä¸­æ–‡**ã€‚"
            "**ç»å¯¹ç¦æ­¢**åœ¨ `text` å­—æ®µä¸­è¿”å›ä»»ä½•åŸå§‹è¯­è¨€ï¼ˆå¦‚æ—¥è¯­ï¼‰çš„æ–‡æœ¬ã€‚"
        )
        fallback_task = (
            "æˆ‘å°†ä¸ºä½ æä¾›ä¸€ç³»åˆ—éŸ³é¢‘ç‰‡æ®µå’Œå®ƒä»¬åœ¨è§†é¢‘ä¸­çš„ç»å¯¹æ—¶é—´ `[AUDIO_INFO] start_ms --> end_ms`ã€‚\n"
            "è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n"
            "1.  **å¬å–éŸ³é¢‘**å¹¶ç†è§£å…¶å†…å®¹ã€‚\n"
            "2.  **åˆ›å»ºå†…éƒ¨æ—¶é—´è½´**: å°†**ç¿»è¯‘æˆä¸­æ–‡å**çš„æ–‡æœ¬ï¼Œæ ¹æ®è¯­éŸ³åœé¡¿åˆ†å‰²æˆæ›´çŸ­çš„å­—å¹•è¡Œï¼Œå¹¶ä¸ºæ¯ä¸€è¡Œä¼°ç®—ä¸€ä¸ªç²¾ç¡®çš„ `start_ms` å’Œ `end_ms`ã€‚\n"
            "3.  **æ ¼å¼åŒ–è¾“å‡º**: ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªæ ¼å¼å®Œå…¨æ­£ç¡®çš„ JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½• markdown æ ‡è®°ã€‚ç»“æ„å¦‚ä¸‹ï¼Œå…¶ä¸­ `text` å­—æ®µçš„å€¼å¿…é¡»æ˜¯ä½ ç¿»è¯‘åçš„**ç®€ä½“ä¸­æ–‡**ï¼š\n"
            '{\n'
            '  "subtitles": [\n'
            '    { "start_ms": 12345, "end_ms": 13456, "text": "è¿™æ˜¯ç¿»è¯‘åçš„ç¬¬ä¸€å¥å­—å¹•" },\n'
            '    { "start_ms": 13600, "text": "è¿™æ˜¯ç¬¬äºŒå¥" }\n'
            '  ]\n'
            '}\n'
        )
        return fallback_system, fallback_task


def read_and_encode_file_base64(filepath: str) -> str | None:
    # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è¿”å› Base64 ç¼–ç çš„å­—ç¬¦ä¸²ã€‚
    try:
        with open(filepath, "rb") as f:
            binary_data = f.read()
        return base64.b64encode(binary_data).decode('utf-8')
    except Exception as e:
        log_system_event("error", f"æ— æ³•è¯»å–æˆ–ç¼–ç æ–‡ä»¶: {filepath}. é”™è¯¯: {e}", in_worker=True)
        return None

# --- C. éŸ³é¢‘é¢„å¤„ç†ç®¡é“ ---

def extract_audio_with_ffmpeg(
    video_path: Path,
    temp_dir: Path,
    update_status_callback: callable
) -> Path:
    """
    ã€æ–°å¢ã€‘åªè´Ÿè´£ä»è§†é¢‘ä¸­æå–éŸ³é¢‘çš„å‡½æ•°ã€‚
    è¿™ä¸ªè¿‡ç¨‹ä¼šé”å®šè§†é¢‘æ–‡ä»¶ï¼Œå®Œæˆåå³é‡Šæ”¾ã€‚
    è¿”å›æå–å‡ºçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ã€‚
    """
    update_status_callback(stage="subtitle_extract_audio", details="æ­£åœ¨ä»è§†é¢‘ä¸­æå–éŸ³é¢‘ (ffmpeg)...")
    raw_audio_path = temp_dir / "raw_audio.wav"
    try:
        command = [
            "ffmpeg", "-i", str(video_path),
            "-ac", "1", "-ar", "16000",
            "-vn", "-y", "-loglevel", "error", str(raw_audio_path)
        ]
        process = subprocess.run(command, check=True, capture_output=True, text=True)
        return raw_audio_path
    except subprocess.CalledProcessError as e:
        log_system_event("error", f"FFmpeg æå–éŸ³é¢‘å¤±è´¥ã€‚Stderr: {e.stderr}", in_worker=True)
        raise RuntimeError(f"FFmpeg æå–éŸ³é¢‘å¤±è´¥: {e.stderr}")

def preprocess_audio_for_subtitles(
    raw_audio_path: Path, # <--- è¾“å…¥å‚æ•°å˜æ›´
    temp_dir: Path,
    update_status_callback: callable,
    ai_models: dict
) -> list[dict]:
    """
    ã€ä¿®æ”¹åã€‘æ¥æ”¶ä¸€ä¸ªå·²æå–çš„éŸ³é¢‘æ–‡ä»¶ï¼Œæ‰§è¡Œåç»­çš„é™å™ªã€VADåˆ‡åˆ†ã€‚
    è¿™ä¸ªè¿‡ç¨‹ä¸å†æ¥è§¦åŸå§‹è§†é¢‘æ–‡ä»¶ã€‚
    """
    # 1. æ£€æŸ¥é™å™ªæ¨¡å‹
    denoiser_model = ai_models.get("denoiser")
    if denoiser_model:
        update_status_callback(stage="subtitle_denoise", details="AI é™å™ªæ¨¡å‹å·²åŠ è½½ï¼Œå‡†å¤‡å¤„ç†...")
        log_system_event("info", "å°†ä½¿ç”¨é¢„åŠ è½½çš„ AI é™å™ªæ¨¡å‹ã€‚", in_worker=True)
    else:
        log_system_event("warning", "é™å™ªæ¨¡å‹ä¸å¯ç”¨ï¼Œå°†è·³è¿‡é™å™ªæ­¥éª¤ã€‚", in_worker=True)

    # 2. åˆ†å—å¤„ç†éŸ³é¢‘ï¼šé™å™ª -> VAD
    update_status_callback(stage="subtitle_vad", details="æ­£åœ¨è¿›è¡ŒéŸ³é¢‘åˆ†å—ä¸è¯­éŸ³æ£€æµ‹...")
    try:
        original_audio = AudioSegment.from_wav(raw_audio_path)
    except Exception as e:
        raise RuntimeError(f"æ— æ³•åŠ è½½æå–å‡ºçš„éŸ³é¢‘æ–‡ä»¶ {raw_audio_path}: {e}")
        
    total_duration_ms = len(original_audio)
    chunk_files = []
    chunks_dir = temp_dir / "audio_chunks"
    chunks_dir.mkdir(exist_ok=True)
    
    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§éŸ³é¢‘å—
    for item in chunks_dir.iterdir():
        item.unlink()

    num_chunks = -(-total_duration_ms // SUBTITLE_CHUNK_DURATION_MS)

    for i in range(num_chunks):
        start_time_ms = i * SUBTITLE_CHUNK_DURATION_MS
        end_time_ms = min((i + 1) * SUBTITLE_CHUNK_DURATION_MS, total_duration_ms)
        
        log_system_event("info", f"æ­£åœ¨å¤„ç†éŸ³é¢‘æ€»å— {i+1}/{num_chunks}...", in_worker=True)
        
        audio_chunk = original_audio[start_time_ms:end_time_ms]
        temp_chunk_path = temp_dir / f"temp_chunk_{i}.wav"
        audio_chunk.export(temp_chunk_path, format="wav")
        
        processing_path = temp_chunk_path
        
        # 2.1 AI é™å™ª (å¦‚æœæ¨¡å‹åŠ è½½æˆåŠŸ)
        if denoiser_model:
            try:
                wav, sr = torchaudio.load(temp_chunk_path)
                wav = wav.cuda()
                with torch.no_grad():
                    denoised_wav = denoiser_model(wav[None])[0]
                denoised_chunk_path = temp_dir / f"denoised_chunk_{i}.wav"
                torchaudio.save(denoised_chunk_path, denoised_wav.cpu(), 16000)
                processing_path = denoised_chunk_path
            except Exception as e:
                log_system_event("warning", f"å½“å‰å—é™å™ªå¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹éŸ³é¢‘ã€‚é”™è¯¯: {e}", in_worker=True)
        
        # 2.2 VAD è¯­éŸ³æ£€æµ‹
        try:
            from faster_whisper.audio import decode_audio
            from faster_whisper.vad import VadOptions, get_speech_timestamps
            
            vad_parameters = { "threshold": 0.38, "min_speech_duration_ms": 150, "max_speech_duration_s": 15.0, "min_silence_duration_ms": 1500, "speech_pad_ms": 500 }
            sampling_rate = 16000
            audio_data = decode_audio(str(processing_path), sampling_rate=sampling_rate)
            speech_timestamps = get_speech_timestamps(audio_data, vad_options=VadOptions(**vad_parameters))
            
            # 2.3 æ ¹æ® VAD ç»“æœä»åŸå§‹éŸ³é¢‘ä¸­ç²¾ç¡®åˆ‡ç‰‡
            for speech in speech_timestamps:
                relative_start_ms = int(speech["start"] / sampling_rate * 1000)
                relative_end_ms = int(speech["end"] / sampling_rate * 1000)
                absolute_start_ms = start_time_ms + relative_start_ms
                absolute_end_ms = start_time_ms + relative_end_ms
                
                final_chunk = original_audio[absolute_start_ms:absolute_end_ms]
                final_chunk_path = chunks_dir / f"chunk_{absolute_start_ms}.wav"
                final_chunk.export(str(final_chunk_path), format="wav")
                chunk_files.append({ "path": str(final_chunk_path), "start_ms": absolute_start_ms, "end_ms": absolute_end_ms })
        except Exception as e:
            log_system_event("error", f"å½“å‰å— VAD å¤„ç†å¤±è´¥: {e}", in_worker=True)
    
    log_system_event("info", f"éŸ³é¢‘åˆ†å—å¤„ç†å®Œæˆï¼Œæ€»å…±åˆ‡åˆ†ä¸º {len(chunk_files)} ä¸ªæœ‰æ•ˆè¯­éŸ³ç‰‡æ®µã€‚", in_worker=True)
    return chunk_files

# --- D. AI äº¤äº’ä¸å¹¶å‘è°ƒåº¦ ---

def _process_subtitle_batch_with_ai(
    chunk_group: list[dict],
    group_index: int,
    api_key: str,
    gemini_endpoint_prefix: str,
    prompt_api_url: str
) -> list[dict]:
    """
    å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„éŸ³é¢‘å—ï¼Œè°ƒç”¨ Gemini API è·å–å­—å¹•ã€‚
    (ä¼˜åŒ–ç‰ˆ v2: æ˜ç¡®æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œä¸æ˜¯è¿”å›ç©ºåˆ—è¡¨ï¼Œä»¥ä¾¿ä¸Šå±‚æ•è·)
    """
    log_system_event("info", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å·²åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­å¯åŠ¨...", in_worker=True)
    
    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘: å°†æ•´ä¸ªå‡½æ•°é€»è¾‘åŒ…è£¹åœ¨ä¸€ä¸ªå¤§çš„ try...except å—ä¸­ã€‚
    # è¿™æ ·åšçš„ç›®çš„æ˜¯ï¼Œæ— è®ºå‘ç”Ÿä½•ç§ç±»å‹çš„é”™è¯¯ï¼ˆç½‘ç»œã€APIã€æ•°æ®è§£æç­‰ï¼‰ï¼Œ
    # éƒ½èƒ½ç¡®ä¿å®ƒè¢«é‡æ–°æŠ›å‡ºï¼Œè€Œä¸æ˜¯è¢«å‡½æ•°å†…éƒ¨æ¶ˆåŒ–æ‰ã€‚
    try:
        # 1. è·å–åŠ¨æ€æç¤ºè¯
        system_instruction, prompt_for_task = get_dynamic_prompts(prompt_api_url)

        # 2. æ„å»º REST API è¯·æ±‚ä½“ (payload)
        model_name = "gemini-2.5-flash"
        generate_url = f"{gemini_endpoint_prefix.rstrip('/')}/v1beta/models/{model_name}:generateContent"
        headers = {"x-goog-api-key": api_key, "Content-Type": "application/json"}
        
        parts = [{"text": prompt_for_task}]
        for chunk in chunk_group:
            encoded_data = read_and_encode_file_base64(chunk["path"])
            if not encoded_data:
                # å¦‚æœå•ä¸ªæ–‡ä»¶ç¼–ç å¤±è´¥ï¼Œè®°å½•è­¦å‘Šå¹¶è·³è¿‡ï¼Œä¸å½±å“æ•´ä¸ªæ‰¹æ¬¡
                log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ–‡ä»¶ {chunk['path']} ç¼–ç å¤±è´¥ï¼Œå°†è·³è¿‡æ­¤éŸ³é¢‘ç‰‡æ®µã€‚", in_worker=True)
                continue
            parts.append({"text": f"[AUDIO_INFO] {chunk['start_ms']} --> {chunk['end_ms']}"})
            parts.append({"inlineData": {"mime_type": "audio/wav", "data": encoded_data}})
        
        if len(parts) <= 1:
            # å¦‚æœæ•´ä¸ªæ‰¹æ¬¡çš„æ‰€æœ‰æ–‡ä»¶éƒ½æ— æ³•ç¼–ç ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡´å‘½é”™è¯¯ï¼Œåº”æŠ›å‡ºå¼‚å¸¸
            raise ValueError(f"æ‰¹æ¬¡ {group_index+1} ä¸­çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶å‡æ— æ³•ç¼–ç ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚")

        payload = {
            "contents": [{"parts": parts}],
            "systemInstruction": {"parts": [{"text": system_instruction}]},
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
            ],
            "generationConfig": {"responseMimeType": "application/json"}
        }

        # 3. å‘é€è¯·æ±‚å¹¶å®ç°å…¨é¢çš„é‡è¯•é€»è¾‘
        log_system_event("info", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ•°æ®å‡†å¤‡å®Œæ¯•ï¼Œæ­£åœ¨è°ƒç”¨ Gemini API...", in_worker=True)
        max_retries = 3
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                # ä½¿ç”¨ç¨é•¿çš„è¶…æ—¶æ—¶é—´ï¼Œå› ä¸ºå¯èƒ½åŒ…å«å¤§é‡éŸ³é¢‘æ•°æ®
                response = requests.post(generate_url, headers=headers, json=payload, timeout=300) 
                
                # å¯é‡è¯•çš„æœåŠ¡å™¨ç«¯é”™è¯¯
                if response.status_code in [429, 500, 503, 504]:
                    log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] é‡åˆ°å¯é‡è¯•çš„ API é”™è¯¯ (HTTP {response.status_code}, å°è¯• {attempt + 1}/{max_retries})ã€‚", in_worker=True)
                    # è§¦å‘ä¸‹ä¸€æ¬¡å¾ªç¯çš„é‡è¯•
                    raise requests.exceptions.HTTPError(f"Server error: {response.status_code}")

                # ä»»ä½•å…¶ä»–é2xxçš„çŠ¶æ€ç ï¼Œéƒ½è§†ä¸ºä¸å¯é‡è¯•çš„å¤±è´¥
                response.raise_for_status()
                
                # --- å¦‚æœè¯·æ±‚æˆåŠŸï¼Œåˆ™å¤„ç†å“åº” ---
                response_data = response.json()
                
                candidates = response_data.get("candidates")
                if not candidates:
                    raise ValueError(f"APIå“åº”ä¸­ç¼ºå°‘ 'candidates' å­—æ®µã€‚å“åº”: {response.text}")

                content = candidates[0].get("content")
                if not content:
                    finish_reason = candidates[0].get("finishReason", "æœªçŸ¥")
                    safety_ratings = candidates[0].get("safetyRatings", [])
                    raise ValueError(f"APIå“åº”å†…å®¹ä¸ºç©º(å¯èƒ½è¢«å®‰å…¨ç­–ç•¥æ‹¦æˆª)ã€‚åŸå› : {finish_reason}, å®‰å…¨è¯„çº§: {safety_ratings}")

                json_text = content.get("parts", [{}])[0].get("text")
                if json_text is None:
                    raise ValueError(f"APIå“åº”çš„ 'parts' ä¸­ç¼ºå°‘ 'text' å­—æ®µã€‚å“åº”: {response.text}")

                # ä½¿ç”¨ Pydantic éªŒè¯ JSON ç»“æ„
                parsed_result = BatchTranscriptionResult.model_validate_json(json_text)
                subtitles_count = len(parsed_result.subtitles)
                log_system_event("info", f"âœ… [å­—å¹•ä»»åŠ¡ {group_index+1}] æˆåŠŸï¼è·å¾— {subtitles_count} æ¡å­—å¹•ã€‚", in_worker=True)
                
                thread_local_srt_list = []
                for i, subtitle in enumerate(parsed_result.subtitles):
                    if subtitle.end_ms is None:
                        if i + 1 < subtitles_count:
                            subtitle.end_ms = parsed_result.subtitles[i+1].start_ms
                        else:
                            subtitle.end_ms = chunk_group[-1]['end_ms']
                    if subtitle.start_ms > subtitle.end_ms:
                        subtitle.end_ms = subtitle.start_ms + 250
                    
                    line = f"{ms_to_srt_time(subtitle.start_ms)} --> {ms_to_srt_time(subtitle.end_ms)}\n{subtitle.text.strip()}\n"
                    thread_local_srt_list.append({"start_ms": subtitle.start_ms, "srt_line": line})
                
                # æˆåŠŸå¤„ç†ï¼Œè¿”å›ç»“æœï¼Œå‡½æ•°ç»“æŸ
                return thread_local_srt_list

            except (requests.exceptions.RequestException, pydantic.ValidationError, ValueError) as e:
                # æ•è·æ‰€æœ‰é¢„æœŸçš„ã€å¯é‡è¯•çš„æˆ–ä¸å¯é‡è¯•çš„é”™è¯¯
                log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å°è¯• {attempt + 1}/{max_retries} å¤±è´¥ã€‚é”™è¯¯: {type(e).__name__}: {e}", in_worker=True)
                last_exception = e
                if attempt < max_retries - 1:
                    wait_time = 5 * (2 ** attempt)
                    log_system_event("info", f"{wait_time}ç§’åå°†è‡ªåŠ¨é‡è¯•...", in_worker=True)
                    time.sleep(wait_time)
                else:
                    # æ‰€æœ‰é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œè·³å‡ºå¾ªç¯
                    break
        
        # å¦‚æœå¾ªç¯ç»“æŸï¼ˆæ„å‘³ç€æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼‰ï¼Œåˆ™æŠ›å‡ºæœ€åçš„å¼‚å¸¸
        log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥ã€‚", in_worker=True)
        raise RuntimeError(f"æ‰¹æ¬¡ {group_index+1} å¤±è´¥") from last_exception

    except Exception as e:
        # è¿™æ˜¯ä¸€ä¸ªæœ€ç»ˆçš„æ•è·å™¨ï¼Œç¡®ä¿ä»»ä½•æœªé¢„æ–™åˆ°çš„é”™è¯¯éƒ½ä¼šè¢«è®°å½•å¹¶é‡æ–°æŠ›å‡º
        log_system_event("critical", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å‘ç”Ÿæœªé¢„æ–™çš„ä¸¥é‡é”™è¯¯: {type(e).__name__}: {e}", in_worker=True)
        # ã€å…³é”®ã€‘é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ä¾¿ ProcessPoolExecutor èƒ½å¤Ÿæ•è·åˆ°è¿™ä¸ªä»»åŠ¡çš„å¤±è´¥çŠ¶æ€
        raise e

def run_subtitle_extraction_pipeline(subtitle_config: dict, chunk_files: list[dict], update_status_callback: callable) -> str:
    """
    ä¸»è°ƒåº¦å™¨ï¼Œè´Ÿè´£å¹¶å‘å¤„ç†æ‰€æœ‰éŸ³é¢‘æ‰¹æ¬¡å¹¶ç”Ÿæˆæœ€ç»ˆçš„ SRT å†…å®¹ã€‚
    (ä¼˜åŒ–ç‰ˆ v2: ä½¿ç”¨ ProcessPoolExecutor å®ç°è¿›ç¨‹éš”ç¦»ï¼Œé¿å…çº¿ç¨‹æ­»é”)
    """
    
    api_keys = subtitle_config.get('GEMINI_API_KEYS', [])
    prompt_api_url = subtitle_config.get('PROMPT_API_URL', '')
    gemini_endpoint_prefix = subtitle_config.get('GEMINI_API_ENDPOINT_PREFIX', '')
    
    if not all([api_keys, prompt_api_url, gemini_endpoint_prefix]):
        raise ValueError("å­—å¹•é…ç½®ä¸­ç¼ºå°‘ 'GEMINI_API_KEYS', 'PROMPT_API_URL', æˆ– 'GEMINI_API_ENDPOINT_PREFIX'ã€‚")
    
    total_chunks = len(chunk_files)
    if total_chunks == 0:
        log_system_event("warning", "æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆçš„è¯­éŸ³ç‰‡æ®µï¼Œæ— æ³•ç”Ÿæˆå­—å¹•ã€‚", in_worker=True)
        return ""

    chunk_groups = [chunk_files[i:i + SUBTITLE_BATCH_SIZE] for i in range(0, total_chunks, SUBTITLE_BATCH_SIZE)]
    num_groups = len(chunk_groups)
    log_system_event("info", f"å·²å°†è¯­éŸ³ç‰‡æ®µåˆ†ä¸º {num_groups} ä¸ªæ‰¹æ¬¡ï¼Œå‡†å¤‡é€šè¿‡å¤šè¿›ç¨‹å¹¶å‘å¤„ç†ã€‚", in_worker=True)
    update_status_callback(stage="subtitle_transcribing", details=f"å‡†å¤‡å¤„ç† {num_groups} ä¸ªå­—å¹•æ‰¹æ¬¡...")
    
    all_srt_blocks = []
    
    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘: ä½¿ç”¨ ProcessPoolExecutor æ›¿ä»£ ThreadPoolExecutor
    # max_workers å»ºè®®ä¸è¦è®¾ç½®å¾—è¿‡é«˜ï¼Œå› ä¸ºå®ƒä¼šæ¶ˆè€—æ›´å¤šå†…å­˜ã€‚4-8ä¸ªè¿›ç¨‹é€šå¸¸æ˜¯æ¯”è¾ƒå¥½çš„èµ·ç‚¹ã€‚
    # SUBTITLE_CONCURRENT_REQUESTS è¿™ä¸ªå…¨å±€å˜é‡çš„å€¼å¯ä»¥æ ¹æ®æƒ…å†µè°ƒæ•´ã€‚
    with ProcessPoolExecutor(max_workers=SUBTITLE_CONCURRENT_REQUESTS) as executor:
        # ä½¿ç”¨å­—å…¸å°† future æ˜ å°„å›å…¶ä»»åŠ¡ç´¢å¼•ï¼Œä¾¿äºæ—¥å¿—è®°å½•
        future_to_index = {}
        delay_between_submissions = 60.0 / SUBTITLE_REQUESTS_PER_MINUTE
        
        log_system_event("info", "å¼€å§‹å‘è¿›ç¨‹æ± æäº¤æ‰€æœ‰å­—å¹•ä»»åŠ¡...", in_worker=True)
        for i, group in enumerate(chunk_groups):
            api_key_for_process = api_keys[i % len(api_keys)]
            future = executor.submit(
                _process_subtitle_batch_with_ai,
                group,
                i, # ä»»åŠ¡ç´¢å¼•
                api_key_for_process,
                gemini_endpoint_prefix,
                prompt_api_url
            )
            future_to_index[future] = i + 1  # ä»»åŠ¡ç´¢å¼•ä»1å¼€å§‹ï¼Œæ›´ç¬¦åˆæ—¥å¿—ä¹ æƒ¯
            
            if i < num_groups - 1:
                time.sleep(delay_between_submissions)
        
        log_system_event("info", f"æ‰€æœ‰ {num_groups} ä¸ªä»»åŠ¡å‡å·²æäº¤ã€‚ç°åœ¨å¼€å§‹ç­‰å¾…å¹¶å¤„ç†è¿”å›ç»“æœ...", in_worker=True)
        
        completed_count = 0
        for future in as_completed(future_to_index):
            task_index = future_to_index[future]
            completed_count += 1
            
            try:
                # ã€æ–°å¢ã€‘ä¸ºè·å–ç»“æœè®¾ç½®ä¸€ä¸ªåˆç†çš„è¶…æ—¶æ—¶é—´ï¼ˆä¾‹å¦‚15åˆ†é’Ÿï¼‰
                # è¿™ä¸ªæ—¶é—´åº”è¯¥å¤§äºå•ä¸ªæ‰¹æ¬¡å¤„ç†çš„æœ€å¤§å¯èƒ½æ—¶é—´ï¼ˆåŒ…æ‹¬é‡è¯•ï¼‰
                # timeout = (å•ä¸ªè¯·æ±‚è¶…æ—¶ + é‡è¯•ç­‰å¾…) * é‡è¯•æ¬¡æ•°ï¼Œå†åŠ ä¸€äº›ä½™é‡
                # timeout = (360s + 5s*1 + 5s*2) * 3 = (375s) * 3 ~= 20åˆ†é’Ÿ
                result_timeout = 10 * 60 # 20åˆ†é’Ÿ
                
                # è·å–å·²å®Œæˆä»»åŠ¡çš„ç»“æœã€‚å¦‚æœå­è¿›ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼Œ.result()ä¼šé‡æ–°æŠ›å‡ºå®ƒ
                result = future.result(timeout=result_timeout)
                
                if result:
                    all_srt_blocks.extend(result)
                    log_system_event("info", f"âœ… å·²æˆåŠŸå¤„ç†å®Œå­—å¹•ä»»åŠ¡ {task_index} çš„ç»“æœã€‚", in_worker=True)
                else:
                    # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œé™¤é_process_subtitle_batch_with_aiåœ¨æ²¡æœ‰ç»“æœæ—¶è¿”å›äº†Noneæˆ–[]
                    log_system_event("warning", f"å­—å¹•ä»»åŠ¡ {task_index} è¿”å›äº†ç©ºç»“æœï¼Œå¯èƒ½å¤„ç†å¤±è´¥ä½†æœªæŠ›å‡ºå¼‚å¸¸ã€‚", in_worker=True)

            except TimeoutError:
                # ã€æ–°å¢ã€‘æ•è·è¶…æ—¶é”™è¯¯
                log_system_event("error", f"âŒ è·å–å­—å¹•ä»»åŠ¡ {task_index} çš„ç»“æœè¶…æ—¶ï¼è¯¥å­è¿›ç¨‹å¯èƒ½å·²åƒµæ­»ã€‚", in_worker=True)
                # å³ä½¿ä¸€ä¸ªä»»åŠ¡è¶…æ—¶ï¼Œæˆ‘ä»¬ä¾ç„¶è¦ç»§ç»­å¤„ç†å…¶ä»–å·²å®Œæˆçš„ä»»åŠ¡
                
            except Exception as e:
                # ã€å…³é”®ã€‘ç°åœ¨å¯ä»¥æ­£ç¡®æ•è·å­è¿›ç¨‹ä¸­çš„æ‰€æœ‰å¼‚å¸¸
                log_system_event("error", f"âŒ è·å–å­—å¹•ä»»åŠ¡ {task_index} çš„ç»“æœæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
            
            finally:
                # æ— è®ºæˆåŠŸå¤±è´¥ï¼Œéƒ½æ›´æ–°è¿›åº¦
                update_status_callback(stage="subtitle_transcribing", details=f"å·²å¤„ç† {completed_count}/{num_groups} ä¸ªå­—å¹•æ‰¹æ¬¡...")

    log_system_event("info", "æ‰€æœ‰å¹¶å‘ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ­£åœ¨æ•´åˆå­—å¹•...", in_worker=True)
    
    if not all_srt_blocks:
        log_system_event("warning", "æ‰€æœ‰å­—å¹•æ‰¹æ¬¡å¤„ç†å‡å¤±è´¥æˆ–æœªè¿”å›ä»»ä½•å†…å®¹ï¼Œæœ€ç»ˆç”Ÿæˆçš„å­—å¹•ä¸ºç©ºã€‚", in_worker=True)
        # æ ¹æ®ä¸šåŠ¡éœ€æ±‚ï¼Œå¯ä»¥é€‰æ‹©è¿”å›ç©ºå­—ç¬¦ä¸²æˆ–æŠ›å‡ºå¼‚å¸¸
        # è¿™é‡Œé€‰æ‹©è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œè®©ä¸»æµç¨‹åˆ¤æ–­
        return ""

    all_srt_blocks.sort(key=lambda x: x["start_ms"])
    
    final_srt_lines = [f"{i + 1}\n{block['srt_line']}" for i, block in enumerate(all_srt_blocks)]
    final_srt_content = "\n".join(final_srt_lines)
    
    log_system_event("info", f"å­—å¹•æ•´åˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_srt_blocks)} æ¡å­—å¹•ã€‚", in_worker=True)
    
    return final_srt_content

# =============================================================================
# --- ç¬¬ 5 æ­¥: MixFileCLI å®¢æˆ·ç«¯ ---
# =============================================================================

class MixFileCLIClient:
    # ä¸€ä¸ªç®€å•çš„ç”¨äºä¸ MixFileCLI åç«¯ API äº¤äº’çš„å®¢æˆ·ç«¯ã€‚
    def __init__(self, base_url: str, proxies: dict = None):
        if not base_url.startswith("http"):
            raise ValueError("Base URL å¿…é¡»ä»¥ http æˆ– https å¼€å¤´")
        self.base_url = base_url
        self.session = requests.Session()
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘è®¾ç½®ä»£ç†
        if proxies:
            self.session.proxies = proxies

    def _make_request(self, method: str, url: str, **kwargs):
        # ç»Ÿä¸€çš„è¯·æ±‚å‘é€æ–¹æ³•ï¼ŒåŒ…å«é”™è¯¯å¤„ç†ã€‚
        try:
            # ç¡®ä¿è¯·æ±‚ä¸ä¼šè¢«ç¼“å­˜
            headers = kwargs.get('headers', {})
            headers.update({'Cache-Control': 'no-cache', 'Pragma': 'no-cache'})
            kwargs['headers'] = headers
            
            # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´ä»¥é€‚åº”æ…¢é€Ÿç½‘ç»œ
            response = self.session.request(method, url, timeout=600, **kwargs)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            error_text = str(e)
            status_code = 500 # é»˜è®¤ä¸ºé€šç”¨æœåŠ¡å™¨é”™è¯¯
            if e.response is not None:
                error_text = e.response.text
                status_code = e.response.status_code
            return (status_code, error_text)

    def upload_file(self, local_file_path: str, progress_callback: callable = None):
        #
        # ã€ä¿®æ”¹åã€‘ä¸Šä¼ å•ä¸ªæ–‡ä»¶å¹¶è·å–åˆ†äº«ç ï¼Œå¢åŠ äº†è¿›åº¦å›è°ƒåŠŸèƒ½ã€‚
        #
        # Args:
        #     local_file_path (str): æœ¬åœ°æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
        #     progress_callback (callable, optional): è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (bytes_uploaded, total_bytes)ã€‚
        #
        # Returns:
        #     requests.Response or tuple: æˆåŠŸæ—¶è¿”å› Response å¯¹è±¡ï¼Œå¤±è´¥æ—¶è¿”å› (status_code, error_text)ã€‚
        #
        filename = os.path.basename(local_file_path)
        upload_url = urljoin(self.base_url, f"/api/upload/{quote(filename)}")
        
        try:
            file_size = os.path.getsize(local_file_path)
        except OSError as e:
             # å¦‚æœæ–‡ä»¶åœ¨è¿™é‡Œå°±æ‰¾ä¸åˆ°äº†ï¼Œç›´æ¥è¿”å›é”™è¯¯
            return (404, f"File not found: {e}")

        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘åˆ›å»ºä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå®ƒåœ¨è¯»å–æ–‡ä»¶çš„åŒæ—¶è°ƒç”¨å›è°ƒå‡½æ•°
        def file_reader_generator(file_handle):
            chunk_size = 1 * 1024 * 1024 # 1MB chunk
            bytes_read = 0
            while True:
                chunk = file_handle.read(chunk_size)
                if not chunk:
                    if progress_callback: # ç¡®ä¿æœ€å100%è¢«è°ƒç”¨
                        progress_callback(file_size, file_size)
                    break
                bytes_read += len(chunk)
                if progress_callback:
                    progress_callback(bytes_read, file_size)
                yield chunk

        try:
            with open(local_file_path, 'rb') as f:
                # å°†ç”Ÿæˆå™¨ä½œä¸º data ä¼ é€’
                return self._make_request("PUT", upload_url, data=file_reader_generator(f))
        except FileNotFoundError as e:
            return (404, str(e))

# =============================================================================
# --- ç¬¬ 6 æ­¥: ç»Ÿä¸€çš„ Flask API æœåŠ¡ (ä¸»è¿›ç¨‹) ---
# =============================================================================

# --- A. åº”ç”¨åˆå§‹åŒ–ä¸ä»»åŠ¡ç®¡ç† ---
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
# å…¨å±€ä»£ç†é…ç½®ï¼Œå°†åœ¨mainå‡½æ•°ä¸­è¢«è®¾ç½®
GLOBAL_PROXY_SETTINGS = None
# ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡çš„çŠ¶æ€ï¼Œå¹¶ç”¨é”æ¥ä¿è¯çº¿ç¨‹å®‰å…¨
tasks = {}
tasks_lock = threading.Lock()

# å…¨å±€å˜é‡ï¼Œå°†åœ¨ main å‡½æ•°ä¸­è¢«åˆå§‹åŒ–
api_client = None 
subtitle_config_global = {}
FRP_SERVER_ADDR = None # å£°æ˜å…¨å±€å˜é‡
TASK_QUEUE = None # å¤šè¿›ç¨‹ä»»åŠ¡é˜Ÿåˆ—
RESULT_QUEUE = None # å¤šè¿›ç¨‹ç»“æœé˜Ÿåˆ—

# --- B. API è·¯ç”±å®šä¹‰ ---

# =============================================================================
# --- (æ–¹æ³•1/3) unified_upload_endpoint [ä¿®æ”¹å] ---
# =============================================================================
@app.route("/api/upload", methods=["POST"])
def unified_upload_endpoint():
    #
    # ç»Ÿä¸€çš„åª’ä½“å¤„ç†å…¥å£ APIã€‚
    # æ¥æ”¶ä¸€ä¸ª URLï¼Œå¹¶æ ¹æ®å‚æ•°å†³å®šæ˜¯ä¸Šä¼ æ–‡ä»¶ã€æå–å­—å¹•ï¼Œè¿˜æ˜¯ä¸¤è€…éƒ½åšã€‚
    # å§‹ç»ˆä»¥å¼‚æ­¥æ¨¡å¼è¿è¡Œã€‚
    #
    data = request.get_json()
    if not data or "url" not in data:
        return jsonify({"error": "è¯·æ±‚ä½“ä¸­å¿…é¡»åŒ…å« 'url' å­—æ®µ"}), 400

    task_id = str(uuid.uuid4())
    
    # ä»è¯·æ±‚ä¸­æå–å‚æ•°
    request_params = {
        "url": data["url"],
        "extract_subtitle": data.get("extract_subtitle", False),
        "upload_video": data.get("upload_video", True),
        "upload_subtitle": data.get("upload_subtitle", False),
    }

    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘åˆå§‹åŒ–æ–°çš„ã€é¢æ¿å‹å¥½çš„ä»»åŠ¡çŠ¶æ€ç»“æ„
    with tasks_lock:
        tasks[task_id] = {
            "taskId": task_id,
            "status": "QUEUED",
            "progress": 0,
            "error": None,
            "results": {
                "video": {
                    "status": "PENDING" if request_params["upload_video"] else "SKIPPED",
                    "details": "ç­‰å¾…å¤„ç†" if request_params["upload_video"] else "ç”¨æˆ·æœªè¯·æ±‚æ­¤æ“ä½œ",
                    "output": None,
                    "error": None
                },
                "subtitle": {
                    "status": "PENDING" if request_params["extract_subtitle"] else "SKIPPED",
                    "details": "ç­‰å¾…å¤„ç†" if request_params["extract_subtitle"] else "ç”¨æˆ·æœªè¯·æ±‚æ­¤æ“ä½œ",
                    "output": None,
                    "error": None
                }
            },
            "createdAt": time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime()),
            "updatedAt": time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())
        }

    # å°†ä»»åŠ¡æ•°æ®æ”¾å…¥é˜Ÿåˆ—ï¼Œç”±å­è¿›ç¨‹å¤„ç†
    task_data = {
        'task_id': task_id,
        'params': request_params,
        'subtitle_config': subtitle_config_global,
        'api_client_base_url': api_client.base_url,
        'frp_server_addr': FRP_SERVER_ADDR
    }
    TASK_QUEUE.put(task_data)
    
    log_system_event("info", f"å·²åˆ›å»ºæ–°ä»»åŠ¡ {task_id} å¹¶æ¨å…¥å¤„ç†é˜Ÿåˆ—ã€‚")

    # è¿”å›ä½“ä¿æŒä¸å˜ï¼Œä¾ç„¶ç®€æ´
    return jsonify({
        "task_id": task_id,
        "status_url": f"/api/tasks/{task_id}"
    }), 202

@app.route("/api/tasks/<task_id>", methods=["GET"])
def get_task_status_endpoint(task_id):
    # è·å–æŒ‡å®šä»»åŠ¡çš„å½“å‰çŠ¶æ€å’Œç»“æœã€‚
    with tasks_lock:
        task = tasks.get(task_id)
    
    if task:
        return jsonify(task)
    else:
        return jsonify({"error": "æœªæ‰¾åˆ°æŒ‡å®šçš„ task_id"}), 404


@app.route('/killer_status_frp', methods=['GET'])
def health_status_endpoint():
    # ç”¨äºå¤–éƒ¨å¥åº·æ£€æŸ¥çš„ç®€å•ç«¯ç‚¹ã€‚
    return jsonify({
        "status": "killer_api_is_running_via_frp",
        "message": "ç»Ÿä¸€åª’ä½“å¤„ç† API æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶é€šè¿‡ FRP æš´éœ²ã€‚",
        "timestamp": time.time()
    }), 200


@app.route('/force_shutdown_notebook', methods=['GET'])
def force_shutdown_endpoint():
    #
    # è¿œç¨‹å…³é—­ APIï¼Œæ¥æ”¶ä¸€ä¸ª token è¿›è¡ŒéªŒè¯ã€‚
    #
    log_system_event("info", "API /force_shutdown_notebook è¢«è°ƒç”¨...")
    token_from_request = request.args.get('token')

    if token_from_request != KILLER_API_SHUTDOWN_TOKEN:
        log_system_event("error", "API Auth å¤±è´¥: Token æ— æ•ˆã€‚")
        return jsonify({"status": "error", "message": "Unauthorized"}), 401
    
    log_system_event("info", "API Auth æˆåŠŸã€‚æ­£åœ¨å®‰æ’åå°å…³é—­ä»»åŠ¡...")

    def delayed_full_shutdown():
        # Step 1: å…ˆå¹³æ»‘åœ°æ€æ­»å­è¿›ç¨‹
        log_system_event("info", "åå°å…³é—­ä»»åŠ¡ï¼šå¼€å§‹ç»ˆæ­¢å­è¿›ç¨‹...")
        _find_and_kill_targeted_processes(signal.SIGTERM)
        time.sleep(2)
        
        # Step 2: å¼ºåˆ¶ç»ˆæ­¢ä»ç„¶å­˜åœ¨çš„å­è¿›ç¨‹
        log_system_event("info", "åå°å…³é—­ä»»åŠ¡ï¼šå¼ºåˆ¶ç»ˆæ­¢ä»»ä½•æ®‹ç•™å­è¿›ç¨‹...")
        _find_and_kill_targeted_processes(signal.SIGKILL)
        time.sleep(1)

        # Step 3: ç»ˆæ­¢ä¸»å†…æ ¸
        _shutdown_notebook_kernel_immediately()
    
    # ç«‹å³å¯åŠ¨åå°çº¿ç¨‹ï¼Œç„¶åç«‹åˆ»è¿”å›å“åº”ï¼Œç¡®ä¿è°ƒç”¨æ–¹æ”¶åˆ°ç¡®è®¤
    threading.Thread(target=delayed_full_shutdown, daemon=True).start()
    
    return jsonify({
        "status": "shutdown_initiated",
        "message": "å…³é—­ä¿¡å·å·²æ¥æ”¶ã€‚åå°æ­£åœ¨æ‰§è¡Œæ¸…ç†å’Œå†…æ ¸å…³é—­æ“ä½œã€‚",
    }), 200

# =============================================================================
# --- ç¬¬ 7 æ­¥: é«˜å®¹é”™çš„ç»Ÿä¸€ä»»åŠ¡å¤„ç†å™¨ (åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œ) ---
# =============================================================================

def process_unified_task(task_data: dict, result_queue: multiprocessing.Queue, upload_queue: multiprocessing.Queue, subtitle_config: dict, ai_models: dict):
    """
    ã€å¹¶è¡Œä¼˜åŒ–æœ€ç»ˆç‰ˆã€‘å°†ffmpegæå–éŸ³é¢‘ä¸åç»­å¤„ç†æ‹†åˆ†ï¼Œå®ç°è§†é¢‘ä¸Šä¼ ä¸éŸ³é¢‘å¤„ç†çš„æœ€å¤§åŒ–å¹¶è¡Œã€‚
    """
    task_id = task_data['task_id']
    params = task_data['params']
    temp_dir = Path(f"/kaggle/working/task_{task_id}")
    temp_dir.mkdir(exist_ok=True)
    
    # ... [å†…éƒ¨çŠ¶æ€ _internal_status å’Œ _update_status å‡½æ•°ä¿æŒä¸å˜] ...
    _internal_status = {
        "progress": 0,
        "results": {
            "video": {"status": "PENDING", "details": "å‡†å¤‡ä¸­", "output": None, "error": None},
            "subtitle": {"status": "PENDING", "details": "å‡†å¤‡ä¸­", "output": None, "error": None}
        }
    }
    def _update_status(component=None, status=None, details=None, progress_val=None, output=None, error_obj=None):
        payload = {'type': 'status_update', 'task_id': task_id, 'payload': {}}
        update_target = {}
        if component and component in _internal_status["results"]:
            update_target = _internal_status["results"][component]
            partial_update = {}
            if status: update_target["status"] = status; partial_update['status'] = status
            if details: update_target["details"] = details; partial_update['details'] = details
            if output:
                if update_target.get("output") is None: update_target["output"] = {}
                update_target["output"].update(output)
                partial_update['output'] = update_target['output']
            if error_obj: update_target["error"] = error_obj; partial_update['error'] = error_obj
            payload['payload']['results'] = {component: partial_update}
        if progress_val is not None:
            _internal_status["progress"] = progress_val
            payload['payload']['progress'] = _internal_status['progress']
        if payload['payload']:
             result_queue.put(payload)

    try:
        # --- æ­¥éª¤ 1: ä¸‹è½½æ–‡ä»¶ ---
        _update_status(component="video", status="RUNNING", details="å¼€å§‹ä¸‹è½½æ–‡ä»¶...", progress_val=5)
        _update_status(component="subtitle", status="RUNNING", details="ç­‰å¾…è§†é¢‘ä¸‹è½½...")
        file_url = params['url']
        filename = unquote(file_url.split("/")[-1].split("?")[0] or f"file_{task_id}")
        local_file_path = temp_dir / filename
        with requests.get(file_url, stream=True, timeout=60) as r:
            r.raise_for_status()
            total_size = int(r.headers.get('content-length', 0))
            bytes_downloaded = 0
            with open(local_file_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
                    bytes_downloaded += len(chunk)
                    if total_size > 0:
                        dl_progress = round((bytes_downloaded / total_size) * 100)
                        _update_status(component="video", details=f"ä¸‹è½½ä¸­ ({dl_progress}%)...", progress_val=5 + int(dl_progress * 0.2))
        _update_status(component="video", details="ä¸‹è½½å®Œæˆ")

        # --- æ­¥éª¤ 2: (å¦‚æœéœ€è¦) ä¸²è¡Œæ‰§è¡Œå†²çªçš„ ffmpeg éŸ³é¢‘æå– ---
        raw_audio_path_for_subtitle = None
        if params["extract_subtitle"]:
            mime_type, _ = mimetypes.guess_type(local_file_path)
            if not (mime_type and mime_type.startswith("video")):
                _update_status(component="subtitle", status="SKIPPED", details="æºæ–‡ä»¶ä¸æ˜¯è§†é¢‘æ ¼å¼")
            else:
                try:
                    def sub_progress_callback(stage, details): _update_status(component="subtitle", details=details)
                    # åªæ‰§è¡Œç¬¬ä¸€æ­¥æå–ï¼Œå®Œæˆåè§†é¢‘æ–‡ä»¶å³è¢«é‡Šæ”¾
                    raw_audio_path_for_subtitle = extract_audio_with_ffmpeg(local_file_path, temp_dir, sub_progress_callback)
                except Exception as e:
                    _update_status(component="subtitle", status="FAILED", details=f"éŸ³é¢‘æå–å¤±è´¥: {e}", error_obj={"code": "AUDIO_EXTRACTION_FAILED", "message": str(e)})
        else:
            _update_status(component="subtitle", status="SKIPPED", details="ç”¨æˆ·æœªè¯·æ±‚æå–")

        # --- æ­¥éª¤ 3: å¹¶è¡Œæ‰§è¡Œè§†é¢‘ä¸Šä¼ å’Œåç»­éŸ³é¢‘å¤„ç† ---
        # æ­¤æ—¶ ffmpeg å·²ç»“æŸï¼Œè§†é¢‘æ–‡ä»¶å·²é‡Šæ”¾ï¼Œå¯ä»¥å®‰å…¨åœ°æ´¾å‘ä¸Šä¼ ä»»åŠ¡
        if params["upload_video"]:
            _update_status(component="video", status="RUNNING", details="å·²æ´¾å‘ä¸Šä¼ ä»»åŠ¡", progress_val=35)
            upload_queue.put({
                'task_id': task_id, 'component': 'video', 'local_file_path': str(local_file_path),
                'filename_for_link': filename, 'api_client_base_url': task_data['api_client_base_url'],
                'frp_server_addr': task_data['frp_server_addr']
            })
        else:
            _update_status(component="video", status="SKIPPED", details="ç”¨æˆ·æœªè¯·æ±‚ä¸Šä¼ ")

        # åŒæ—¶ï¼Œå¦‚æœç¬¬ä¸€æ­¥éŸ³é¢‘æå–æˆåŠŸï¼Œåˆ™å¼€å§‹è¿›è¡Œåç»­çš„ã€ä¸å†²çªçš„éŸ³é¢‘å¤„ç†å’ŒAIæµç¨‹
        if raw_audio_path_for_subtitle:
            try:
                def sub_progress_callback(stage, details): _update_status(component="subtitle", details=details)
                # ç¬¬äºŒæ­¥ï¼šå¤„ç†å·²æå–çš„éŸ³é¢‘
                audio_chunks = preprocess_audio_for_subtitles(raw_audio_path_for_subtitle, temp_dir, sub_progress_callback, ai_models)
                
                # ç¬¬ä¸‰æ­¥ï¼šAIå­—å¹•ç”Ÿæˆ
                _update_status(component="subtitle", status="RUNNING", details="AIå¤„ç†ä¸­...", progress_val=40)
                srt_content = run_subtitle_extraction_pipeline(subtitle_config, audio_chunks, sub_progress_callback)
                
                if not srt_content: raise RuntimeError("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å­—å¹•å†…å®¹ã€‚")
                
                srt_base64 = base64.b64encode(srt_content.encode('utf-8')).decode('utf-8')
                _update_status(component="subtitle", output={"contentBase64": srt_base64})
                
                if params["upload_subtitle"]:
                    _update_status(component="subtitle", status="RUNNING", details="å·²æ´¾å‘å­—å¹•ä¸Šä¼ ä»»åŠ¡")
                    srt_filename = local_file_path.stem + ".srt"
                    srt_path = temp_dir / srt_filename
                    with open(srt_path, "w", encoding="utf-8") as f: f.write(srt_content)
                    upload_queue.put({
                       'task_id': task_id, 'component': 'subtitle', 'local_file_path': str(srt_path),
                       'filename_for_link': srt_filename, 'api_client_base_url': task_data['api_client_base_url'],
                       'frp_server_addr': task_data['frp_server_addr']
                    })
                else:
                    _update_status(component="subtitle", status="SUCCESS", details="æå–æˆåŠŸ")
            except Exception as e:
                _update_status(component="subtitle", status="FAILED", details=str(e), error_obj={"code": "SUBTITLE_PIPELINE_FAILED", "message": str(e)})

        log_system_event("info", f"åª’ä½“å¤„ç†è¿›ç¨‹ä¸ºä»»åŠ¡ {task_id} çš„ä¸»è¦å·¥ä½œå·²å®Œæˆã€‚", in_worker=True)

    except Exception as e:
        log_system_event("error", f"å¤„ç†ä»»åŠ¡ {task_id} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
        result_queue.put({'type': 'task_result', 'task_id': task_id, 'status': 'FAILED', 'result': {}, 'error': {"code": "FATAL_ERROR", "message": str(e)}})
    finally:
        log_system_event("info", f"åª’ä½“å¤„ç†è¿›ç¨‹ {task_id} å®Œæˆï¼Œä¸å†æ¸…ç†ä¸»ä»»åŠ¡ç›®å½•ã€‚", in_worker=True)

# =============================================================================
# --- ç¬¬ 8 æ­¥: å¤šè¿›ç¨‹ Worker ä¸ä¸»ç¨‹åº ---
# =============================================================================


# =============================================================================
# --- (æ–°å¢æ–¹æ³• 1/3) uploader_process_loop [å…¨æ–°] ---
# =============================================================================

# è¿™ä¸ªæ–°å‡½æ•°ä¸“é—¨è´Ÿè´£æ–‡ä»¶ä¸Šä¼ ï¼Œè¿è¡Œåœ¨ç‹¬ç«‹çš„è¿›ç¨‹ä¸­
def uploader_process_loop(upload_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue):
    """
    ã€WARP é›†æˆç‰ˆã€‘ä¸Šä¼ å·¥ä½œè¿›ç¨‹ï¼Œèƒ½å¤„ç†æ¥è‡ª ProxyManager çš„ WARP æˆ– Xray é…ç½®ã€‚
    """
    log_system_event("info", "ä¸Šä¼ ä¸“ç”¨å·¥ä½œè¿›ç¨‹å·²å¯åŠ¨ã€‚", in_worker=True)
    task_proxy_cache = {}
    proxy_manager = ProxyManager(subtitle_config_global.get("V2RAY_SUB_URL"))
    xray_process = None
    is_warp_active_for_upload = False

    def start_proxy_for_task(config):
        nonlocal xray_process, is_warp_active_for_upload
        # å…ˆå…³é—­æ‰€æœ‰ç°æœ‰ä»£ç†
        if xray_process:
            xray_process.terminate(); xray_process.wait(); xray_process = None
        if is_warp_active_for_upload:
            proxy_manager.warp_manager.disconnect(); is_warp_active_for_upload = False

        if config == "WARP":
            if proxy_manager.warp_manager.connect():
                is_warp_active_for_upload = True
                return True
            return False
        elif isinstance(config, dict): # Xray config
            config_path = Path("/kaggle/working/xray_final_config.json")
            with open(config_path, 'w') as f: json.dump(config, f)
            xray_process = run_command(f"{proxy_manager.xray_path} -c {config_path}", "xray_upload.log")
            if not wait_for_port(proxy_manager.local_xray_socks_port, host='127.0.0.1', timeout=10):
                log_system_event("error", "å¯åŠ¨æœ€ä¼˜XrayèŠ‚ç‚¹ç”¨äºä¸Šä¼ å¤±è´¥ï¼", in_worker=True)
                return False
            return True
        return False # No config (direct connection)
    
    def shutdown_all_proxies():
        nonlocal xray_process, is_warp_active_for_upload
        if xray_process:
            xray_process.terminate(); xray_process.wait(); xray_process = None
        if is_warp_active_for_upload:
            proxy_manager.warp_manager.disconnect(); is_warp_active_for_upload = False


    try:
        while True:
            try:
                upload_task_data = upload_queue.get()
                if upload_task_data is None: break

                task_id = upload_task_data['task_id']
                
                if task_id not in task_proxy_cache:
                    proxies_for_task, config_for_task = proxy_manager.get_best_proxy_for_upload(upload_task_data['api_client_base_url'])
                    task_proxy_cache[task_id] = {'proxies': proxies_for_task, 'config': config_for_task}
                    
                    if config_for_task:
                        if not start_proxy_for_task(config_for_task):
                            task_proxy_cache[task_id]['proxies'] = None # å¯åŠ¨å¤±è´¥ï¼Œå›é€€åˆ°ç›´è¿
                    else:
                        shutdown_all_proxies() # å¦‚æœæ˜¯ç›´è¿ï¼Œç¡®ä¿æ‰€æœ‰ä»£ç†éƒ½å…³é—­
                
                # ... (åç»­çš„ä¸Šä¼ é€»è¾‘ï¼Œä» component = ... å¼€å§‹ï¼Œå®Œå…¨ä¸å˜) ...
                
            finally:
                # ä»»åŠ¡æœ€åä¸€ä¸ªç»„ä»¶å®Œæˆåçš„æ¸…ç†é€»è¾‘
                component = upload_task_data.get('component')
                if component == 'subtitle': # å‡è®¾å­—å¹•æ˜¯æœ€åä¸€ä¸ª
                    log_system_event("info", f"ä»»åŠ¡ {task_id} ä¸Šä¼ å®Œæˆï¼Œå…³é—­ä»£ç†å¹¶æ¸…ç†ç¼“å­˜ã€‚", in_worker=True)
                    if task_id in task_proxy_cache: del task_proxy_cache[task_id]
                    shutdown_all_proxies()

    finally:
        shutdown_all_proxies()
        log_system_event("info", "ä¸Šä¼ ä¸“ç”¨å·¥ä½œè¿›ç¨‹å·²å…³é—­ã€‚", in_worker=True)

def worker_process_loop(task_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue, upload_queue: multiprocessing.Queue):
    """
    åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å¾ªç¯ï¼Œè´Ÿè´£åª’ä½“å¤„ç†å¹¶å°†ä¸Šä¼ ä»»åŠ¡å¤–åŒ…ã€‚
    """
    log_system_event("info", "åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å·²å¯åŠ¨ã€‚", in_worker=True)
    ai_models = load_ai_models()
    # subtitle_config_global æ˜¯åœ¨mainå‡½æ•°ä¸­è§£å¯†çš„å…¨å±€å˜é‡ï¼Œå­è¿›ç¨‹å¯ä»¥ç›´æ¥è®¿é—®
    
    while True:
        try:
            task_data = task_queue.get()
            if task_data is None:
                break
            log_system_event("info", f"åª’ä½“å¤„ç†è¿›ç¨‹æ¥æ”¶åˆ°æ–°ä»»åŠ¡: {task_data['task_id']}", in_worker=True)
            process_unified_task(task_data, result_queue, upload_queue, subtitle_config_global, ai_models)
        except KeyboardInterrupt:
            break
        except Exception as e:
            log_system_event("critical", f"åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å¾ªç¯å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
    log_system_event("info", "åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å·²å…³é—­ã€‚", in_worker=True)

def result_processor_thread_loop(result_queue: multiprocessing.Queue):
    """
    ã€ä¿®æ”¹åã€‘ç»“æœå¤„ç†å™¨ï¼Œå¢åŠ äº†åœ¨ä»»åŠ¡è¾¾åˆ°æœ€ç»ˆçŠ¶æ€åï¼Œè´Ÿè´£æ¸…ç†ä»»åŠ¡ä¸´æ—¶ç›®å½•çš„é€»è¾‘ã€‚
    """
    log_system_event("info", "ç»“æœå¤„ç†çº¿ç¨‹å·²å¯åŠ¨ã€‚")
    while True:
        try:
            # ä½¿ç”¨é•¿è¶…æ—¶ä»¥é˜²é˜Ÿåˆ—é•¿æ—¶é—´ç©ºé—²
            result_data = result_queue.get(timeout=3600)
            task_id = result_data['task_id']

            with tasks_lock:
                if task_id not in tasks:
                    continue

                task = tasks[task_id]
                task['updatedAt'] = time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())
                
                data_type = result_data.get('type')
                
                # --- çŠ¶æ€æ›´æ–°é€»è¾‘ ---
                if data_type == 'status_update':
                    payload = result_data.get('payload', {})
                    if 'progress' in payload: task['progress'] = payload['progress']
                    if 'results' in payload:
                        for component, data in payload['results'].items():
                            if component in task['results']:
                                task['results'][component].update(data)
                    # åªè¦æœ‰æ›´æ–°ï¼Œå°±è®¤ä¸ºæ˜¯RUNNINGï¼ˆé™¤éå·²ç»æ˜¯æœ€ç»ˆçŠ¶æ€ï¼‰
                    if task['status'] not in ["SUCCESS", "FAILED", "PARTIAL_SUCCESS"]:
                         task['status'] = "RUNNING"

                # --- ä»»åŠ¡çº§ç»“æœï¼ˆé€šå¸¸æ˜¯ä¸¥é‡é”™è¯¯ï¼‰ ---
                elif data_type == 'task_result':
                    task['status'] = result_data.get('status', 'FAILED')
                    task['error'] = result_data.get('error')
                    task['progress'] = 100
                
                # --- æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¾¾åˆ°æœ€ç»ˆçŠ¶æ€ ---
                video_status = task['results']['video']['status']
                subtitle_status = task['results']['subtitle']['status']
                
                # æ£€æŸ¥æ‰€æœ‰å­ç»„ä»¶æ˜¯å¦éƒ½å·²è„±ç¦»â€œå¤„ç†ä¸­â€çš„çŠ¶æ€
                is_finished = "PENDING" not in (video_status, subtitle_status) and \
                              "RUNNING" not in (video_status, subtitle_status)

                # å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œå¹¶ä¸”å°šæœªè®¾ç½®æœ€ç»ˆçŠ¶æ€ï¼Œåˆ™è¿›è¡Œè¯„ä¼°å’Œæ¸…ç†
                if is_finished and task['status'] not in ["SUCCESS", "FAILED", "PARTIAL_SUCCESS"]:
                    log_system_event("info", f"ä»»åŠ¡ {task_id} æ‰€æœ‰ç»„ä»¶å·²å®Œæˆï¼Œæ­£åœ¨è¿›è¡Œæœ€ç»ˆçŠ¶æ€è¯„ä¼°ã€‚")
                    task['progress'] = 100
                    
                    # çŠ¶æ€æ¸…ç†ï¼šå¤„ç†å› è¿›ç¨‹å´©æºƒå¯¼è‡´çš„ "RUNNING" æ®‹ç•™çŠ¶æ€
                    for component in task['results']:
                        comp_data = task['results'][component]
                        if comp_data['status'] == 'RUNNING':
                            comp_data['status'] = 'FAILED'
                            comp_data['details'] = 'ç»„ä»¶å› æœªçŸ¥åŸå› æœªèƒ½å®Œæˆ'
                            comp_data['error'] = { 'code': 'WORKER_CRASHED', 'message': 'å¤„ç†æ­¤ç»„ä»¶çš„å·¥ä½œè¿›ç¨‹å¯èƒ½å·²æ„å¤–ç»ˆæ­¢ã€‚'}

                    # é‡æ–°è·å–æ¸…ç†åçš„çŠ¶æ€
                    final_video_status = task['results']['video']['status']
                    final_subtitle_status = task['results']['subtitle']['status']
                    
                    # å®šä¹‰å‚ä¸æœ€ç»ˆçŠ¶æ€è¯„ä¼°çš„çŠ¶æ€åˆ—è¡¨ (æ’é™¤ SKIPPED)
                    active_statuses = [s for s in (final_video_status, final_subtitle_status) if s != "SKIPPED"]
                    
                    if not active_statuses: # å¦‚æœæ‰€æœ‰ç»„ä»¶éƒ½è¢«è·³è¿‡
                        task['status'] = "SUCCESS"
                    elif all(s == "SUCCESS" for s in active_statuses):
                        task['status'] = "SUCCESS"
                    elif "FAILED" in active_statuses:
                        if "SUCCESS" in active_statuses:
                            task['status'] = "PARTIAL_SUCCESS"
                        else:
                            task['status'] = "FAILED"
                    else: # å…¶ä»–æƒ…å†µï¼Œä¾‹å¦‚å…¨æ˜¯SKIPPEDå’ŒSUCCESS
                        task['status'] = "SUCCESS"

                    log_system_event("info", f"ä»»åŠ¡ {task_id} æœ€ç»ˆçŠ¶æ€è¢«è®¾ç½®ä¸º: {task['status']}")

                    # ã€æ ¸å¿ƒæ–°å¢ã€‘åœ¨æ­¤å¤„æ‰§è¡Œæ¸…ç†æ“ä½œ
                    temp_dir_to_clean = Path(f"/kaggle/working/task_{task_id}")
                    if temp_dir_to_clean.exists():
                        log_system_event("info", f"ä»»åŠ¡ {task_id} å·²ç»“æŸï¼Œå‡†å¤‡æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir_to_clean}")
                        try:
                            shutil.rmtree(temp_dir_to_clean)
                            log_system_event("info", f"âœ… æˆåŠŸæ¸…ç†ä»»åŠ¡ {task_id} çš„ä¸´æ—¶ç›®å½•ã€‚")
                        except Exception as e:
                            log_system_event("error", f"âŒ æ¸…ç†ä»»åŠ¡ {task_id} çš„ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

        except QueueEmpty:
            # é˜Ÿåˆ—ä¸ºç©ºæ˜¯æ­£å¸¸æƒ…å†µï¼Œç»§ç»­å¾ªç¯
            continue
        except Exception as e:
            log_system_event("error", f"ç»“æœå¤„ç†çº¿ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

def main():
    #
    # ä¸»æ‰§è¡Œå‡½æ•°ï¼Œè´Ÿè´£åˆå§‹åŒ–å’Œå¯åŠ¨æ‰€æœ‰æœåŠ¡ã€‚
    #
    global api_client, subtitle_config_global, FRP_SERVER_ADDR
    global TASK_QUEUE, RESULT_QUEUE, UPLOAD_QUEUE # <--- æ–°å¢ UPLOAD_QUEUE

    try:
        # --- 1. å¯åŠ¨å‰å‡†å¤‡ ---
        log_system_event("info", "æœåŠ¡æ­£åœ¨å¯åŠ¨...")
        
        install_torch_cmd = (
            "pip uninstall -y torch torchvision torchaudio && "
            "pip install torch==2.3.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
        )
        log_system_event("info", "æ­£åœ¨å®‰è£…å…¼å®¹çš„ PyTorch ç‰ˆæœ¬...")
        install_proc = subprocess.run(install_torch_cmd, shell=True, capture_output=True, text=True)
        if install_proc.returncode != 0:
            log_system_event("error", f"PyTorch å®‰è£…å¤±è´¥ï¼\nStdout: {install_proc.stdout}\nStderr: {install_proc.stderr}")
            raise RuntimeError("æœªèƒ½å®‰è£…å…¼å®¹çš„ PyTorch ç‰ˆæœ¬ã€‚")
        log_system_event("info", "âœ… å…¼å®¹çš„ PyTorch å®‰è£…å®Œæˆã€‚")
        
        install_other_cmd = "pip install -q pydantic pydub faster-whisper@https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz denoiser google-generativeai requests psutil"
        log_system_event("info", "æ­£åœ¨å®‰è£…å…¶ä½™ä¾èµ–åº“...")
        subprocess.run(install_other_cmd, shell=True, check=True)
        log_system_event("info", "âœ… å…¶ä½™ä¾èµ–åº“å®‰è£…å®Œæˆã€‚")
        
        check_environment()
        
        # --- è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³• ---
        multiprocessing.set_start_method('fork', force=True)

        # --- 2. è§£å¯†é…ç½® ---
        frp_config = get_decrypted_config(ENCRYPTED_FRP_CONFIG, "FRP")
        subtitle_config_global = get_decrypted_config(ENCRYPTED_SUBTITLE_CONFIG, "Subtitle")
        FRP_SERVER_ADDR = frp_config['FRP_SERVER_ADDR']
        FRP_SERVER_PORT = frp_config['FRP_SERVER_PORT']
        FRP_TOKEN = frp_config['FRP_TOKEN']
        
        # --- 3. åˆå§‹åŒ– MixFile å®¢æˆ·ç«¯ ---
        api_client_base_url = f"http://127.0.0.1:{MIXFILE_LOCAL_PORT}"
        api_client = MixFileCLIClient(base_url=api_client_base_url)

        # --- 4. å¯åŠ¨ MixFileCLI æœåŠ¡ ---
        log_system_event("info", "æ­£åœ¨åˆ›å»º MixFileCLI config.yml...")
        with open("config.yml", "w") as f: f.write(mixfile_config_yaml)
        log_system_event("info", "æ­£åœ¨ä¸‹è½½å¹¶å¯åŠ¨ MixFileCLI...")
        if not os.path.exists("mixfile-cli.jar"):
            run_command("wget -q --show-progress https://raw.githubusercontent.com/jornhand/kagglewithmixfile/refs/heads/main/mixfile-cli-2.0.1.jar -O mixfile-cli.jar").wait()
        run_command("java -jar mixfile-cli.jar", "mixfile.log")
        if not wait_for_port(MIXFILE_LOCAL_PORT):
            raise RuntimeError("MixFileCLI æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ mixfile.logã€‚")

        # --- 5. åˆå§‹åŒ–å¤šè¿›ç¨‹é˜Ÿåˆ—å’Œå·¥ä½œè¿›ç¨‹ ---
        TASK_QUEUE = multiprocessing.Queue()
        RESULT_QUEUE = multiprocessing.Queue()
        UPLOAD_QUEUE = multiprocessing.Queue() # æ–°å¢ä¸Šä¼ é˜Ÿåˆ—

        # å¯åŠ¨åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹
        media_worker = multiprocessing.Process(
            target=worker_process_loop,
            args=(TASK_QUEUE, RESULT_QUEUE, UPLOAD_QUEUE), 
            daemon=False
        )
        media_worker.start()
        
        # å¯åŠ¨ä¸“ç”¨çš„ä¸Šä¼ å·¥ä½œè¿›ç¨‹
        uploader_worker = multiprocessing.Process(
            target=uploader_process_loop,
            args=(UPLOAD_QUEUE, RESULT_QUEUE),
            daemon=False
        )
        uploader_worker.start()

        # å¯åŠ¨ç»“æœå¤„ç†çº¿ç¨‹
        result_thread = threading.Thread(
            target=result_processor_thread_loop,
            args=(RESULT_QUEUE,),
            daemon=True
        )
        result_thread.start()

        # --- 6. å¯åŠ¨ Flask API æœåŠ¡ ---
        def run_flask_app():
            app.run(host='0.0.0.0', port=FLASK_API_LOCAL_PORT, debug=False, use_reloader=False)
        log_system_event("info", "æ­£åœ¨åå°å¯åŠ¨ Flask API æœåŠ¡...")
        threading.Thread(target=run_flask_app, daemon=True).start()
        if not wait_for_port(FLASK_API_LOCAL_PORT):
             raise RuntimeError("Flask æœåŠ¡å¯åŠ¨å¤±è´¥ã€‚")

        # --- 7. å¯åŠ¨ frpc å®¢æˆ·ç«¯ ---
        log_system_event("info", "æ­£åœ¨å‡†å¤‡ frpc å®¢æˆ·ç«¯...")
        if not os.path.exists("/kaggle/working/frpc"):
            run_command("wget -q https://github.com/fatedier/frp/releases/download/v0.54.0/frp_0.54.0_linux_amd64.tar.gz && tar -zxvf frp_0.54.0_linux_amd64.tar.gz && mv frp_0.54.0_linux_amd64/frpc /kaggle/working/frpc && chmod +x /kaggle/working/frpc").wait()
        
        frpc_ini = f"""
[common]
server_addr = {FRP_SERVER_ADDR}
server_port = {FRP_SERVER_PORT}
token = {FRP_TOKEN}
log_file = /kaggle/working/frpc.log
[mixfile_webdav_{MIXFILE_REMOTE_PORT}]
type = tcp
local_ip = 127.0.0.1
local_port = {MIXFILE_LOCAL_PORT}
remote_port = {MIXFILE_REMOTE_PORT}
[flask_api_{FLASK_API_REMOTE_PORT}]
type = tcp
local_ip = 127.0.0.1
local_port = {FLASK_API_LOCAL_PORT}
remote_port = {FLASK_API_REMOTE_PORT}
"""
        with open('frpc.ini', 'w') as f: f.write(frpc_ini)
        run_command('/kaggle/working/frpc -c ./frpc.ini')
        log_system_event("info", "frpc å®¢æˆ·ç«¯å·²åœ¨åå°å¯åŠ¨ã€‚")
        time.sleep(3)
        
        # --- 8. æœ€ç»ˆçŠ¶æ€æŠ¥å‘Šä¸ä¿æ´» ---
        public_api_base_url = f"http://{FRP_SERVER_ADDR}:{FLASK_API_REMOTE_PORT}"
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æœåŠ¡å‡å·²æˆåŠŸå¯åŠ¨ï¼æ‚¨çš„ç»Ÿä¸€ API å·²ä¸Šçº¿ã€‚")
        print(f"  -> API å…¥å£ (POST) : {public_api_base_url}/api/upload")
        print(f"  -> ä»»åŠ¡çŠ¶æ€ (GET)  : {public_api_base_url}/api/tasks/<task_id>")
        print(f"  -> å¥åº·æ£€æŸ¥ (GET)  : {public_api_base_url}/killer_status_frp")
        print(f"  -> è¿œç¨‹å…³é—­ (GET)  : {public_api_base_url}/force_shutdown_notebook?token={KILLER_API_SHUTDOWN_TOKEN}")
        print("="*60)
        
        while True:
            time.sleep(300)
            all_workers_alive = media_worker.is_alive() and uploader_worker.is_alive()
            worker_status_msg = 'å…¨éƒ¨å­˜æ´»' if all_workers_alive else 'å­˜åœ¨å·²é€€å‡ºçš„å·¥ä½œè¿›ç¨‹'
            log_system_event("info", f"æœåŠ¡æŒç»­è¿è¡Œä¸­... ({time.ctime()}) å·¥ä½œè¿›ç¨‹çŠ¶æ€: {worker_status_msg}")
            
    except (DecryptionError, RuntimeError, ValueError) as e:
        log_system_event("critical", f"ç¨‹åºå¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
        log_system_event("critical", "æœåŠ¡æ— æ³•å¯åŠ¨ï¼Œç¨‹åºå°†ç»ˆæ­¢ã€‚")
    except KeyboardInterrupt:
        log_system_event("info", "æœåŠ¡å·²æ‰‹åŠ¨åœæ­¢ã€‚")
        if 'TASK_QUEUE' in globals() and TASK_QUEUE is not None:
            TASK_QUEUE.put(None)
        if 'UPLOAD_QUEUE' in globals() and UPLOAD_QUEUE is not None:
            UPLOAD_QUEUE.put(None)
    except Exception as e:
        log_system_event("critical", f"å‘ç”ŸæœªçŸ¥çš„è‡´å‘½é”™è¯¯: {e}")

if __name__ == '__main__':
    main()