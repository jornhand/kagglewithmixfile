# =============================================================================
#         Kaggle æŒä¹…åŒ–åª’ä½“å¤„ç†æœåŠ¡ - ä¸»åº”ç”¨ v2.1 (æ–¹æ¡ˆ2 - å¤šè¿›ç¨‹éš”ç¦»ç‰ˆ)
# =============================================================================
#
# åŠŸèƒ½:
#   - å¯åŠ¨ MixFileCLI ä½œä¸ºæ–‡ä»¶å­˜å‚¨åç«¯ã€‚
#   - æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ Flask APIï¼Œç”¨äº:
#     1. ä» URL ä¸‹è½½æ–‡ä»¶å¹¶ä¸Šä¼ åˆ° MixFileã€‚
#     2. ä»è§†é¢‘ URL ä¸­æå–é«˜è´¨é‡å­—å¹• (å¯é€‰)ã€‚
#     3. å°†åŸå§‹è§†é¢‘å’Œç”Ÿæˆçš„å­—å¹•çµæ´»åœ°ä¸Šä¼ åˆ° MixFileã€‚
#   - é€šè¿‡ FRP å°†å†…éƒ¨æœåŠ¡å®‰å…¨åœ°æš´éœ²åˆ°å…¬ç½‘ã€‚
#   - æ‰€æœ‰æ•æ„Ÿé…ç½® (FRP, APIå¯†é’¥) å‡é€šè¿‡åŠ å¯†æ–¹å¼ç®¡ç†ã€‚
#   - å®ç°é«˜å®¹é”™çš„ä»»åŠ¡å¤„ç†é€»è¾‘ï¼Œå­ä»»åŠ¡å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹ã€‚
#   - **V2.1 å˜æ›´: é‡‡ç”¨å¤šè¿›ç¨‹æ¶æ„ï¼Œå°†é‡é‡çº§çš„åª’ä½“å¤„ç†ä»»åŠ¡æ”¾åˆ°ç‹¬ç«‹çš„å­è¿›ç¨‹
#     ä¸­æ‰§è¡Œï¼Œä»¥è§£å†³ä¸»è¿›ç¨‹ä¸­å› æœåŠ¡ç¯å¢ƒå†²çªå¯¼è‡´çš„AIæ¨¡å‹åŠ è½½å¤±è´¥é—®é¢˜ã€‚**
#
# =============================================================================

# --- æ ¸å¿ƒ Python åº“ ---
import os
import subprocess
import threading
import time
import uuid
import socket
import json
import base64
import hashlib
import signal
import sys
import shutil
import mimetypes
from pathlib import Path
from datetime import timedelta
from urllib.parse import urljoin, quote, unquote
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed, TimeoutError


# --- å¤šè¿›ç¨‹ä¸é˜Ÿåˆ— ---
import multiprocessing
from queue import Empty as QueueEmpty

# --- Web æ¡†æ¶ä¸ HTTP å®¢æˆ·ç«¯ ---
from flask import Flask, request, jsonify, Response
import requests





# =============================================================================
# --- (æ–°å¢æ¨¡å—) ç¬¬ 3.5 æ­¥: V2Ray ä»£ç†ç®¡ç†å™¨ ---
# =============================================================================
import base64
import json
import random
import string
from urllib.parse import urlparse, parse_qs




# --- åŠ å¯†ä¸å¯†é’¥ç®¡ç† ---
# å°è¯•å¯¼å…¥å…³é”®åº“ï¼Œå¦‚æœå¤±è´¥åˆ™åœ¨åç»­æ£€æŸ¥ä¸­å¤„ç†
try:
    from kaggle_secrets import UserSecretsClient
    from cryptography.fernet import Fernet
except ImportError:
    UserSecretsClient = None
    Fernet = None

# --- å­—å¹•æå–ç›¸å…³åº“ (å°†åœ¨ check_environment ä¸­éªŒè¯) ---
try:
    import torch
    import torchaudio
    from pydub import AudioSegment
    import pydantic
except ImportError as e:
    # å…è®¸å¯åŠ¨æ—¶å¯¼å…¥å¤±è´¥ï¼Œåç»­ç¯å¢ƒæ£€æŸ¥ä¼šæ•è·
    print(f"[è­¦å‘Š] é¢„åŠ è½½éƒ¨åˆ†åº“å¤±è´¥: {e}ã€‚å°†åœ¨ç¯å¢ƒæ£€æŸ¥ä¸­ç¡®è®¤ã€‚")
    torch = None
    torchaudio = None
    AudioSegment = None
    pydantic = None

# =============================================================================
# --- ç¬¬ 1 æ­¥: å…¨å±€é…ç½® ---
# =============================================================================

# -- A. MixFileCLI é…ç½® --
mixfile_config_yaml = """
uploader: "çº¿è·¯A2"
upload_task: 10
upload_retry: 10
download_task: 5
chunk_size: 1024
port: 4719
host: "0.0.0.0"
password: ""
webdav_path: "data.mix_dav"
history: "history.mix_list"
"""

# -- B. åŠ å¯†çš„æœåŠ¡é…ç½® --
# !!! é‡è¦ !!!
# åœ¨è¿™é‡Œç²˜è´´ä½ ä» encrypt_util.py å·¥å…·ä¸­è·å¾—çš„åŠ å¯†é…ç½®å­—ç¬¦ä¸²
# ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²ç”¨äº FRP (åå‘ä»£ç†)
ENCRYPTED_FRP_CONFIG = "gAAAAABovqeXe55F4-o0c7vo8hIdPtcdQGWGAZganedI9sUqYQeyzDCzCxLLIjFLpCtTJQGedUh17nzhWhjgNEooN9ybgSYPnoHCSgKsGo64m0ghmUxPhbxrsocys4zki5IAMpuah_NrOH5YX4r-rQnKCI_S2yfurJp2E-eB_ciXodI8-X002KsMIK1ZpbvmVbH5I88bxV5V"

# ç¬¬äºŒä¸ªå­—ç¬¦ä¸²ç”¨äºå­—å¹•æœåŠ¡ (Gemini API å¯†é’¥ç­‰)
ENCRYPTED_SUBTITLE_CONFIG = "gAAAAABovqfhTA9gwxUiOPivABQMk3PXitnzjVdCzOVTu9T1RcE7KSRQWHWqVn3geXJ--yA7IqD8Y3JIIG3X-zUA0j6xMwD9jOhysygys84C1pmm4LyqCg2FqEn-V6UitUxKzbOeXEMDPXVjSS_gP97-k6Dtmp38qSMp91v3BxNp94iJvcRNfDH4DbfZm4rrRwamVFeJzRiFGzPe35bwUqBXhUrIngQweiR1VTvtSnk0tnP53AfRdRPfuK9DSluhfASBrXQnt8bf9KtN0uzuc02SGMGtjFDt_nwxjfE2UEeC6nbQZ6peGDRbfQMFvEu8owsaWnaQdFe-YV7S4xjCjmmj9xw-2NnaFnplP3hYmPfLtE3EuvXXsQ4x30cnkwba0z_0-BEh0pXx-1kaR70T-uaDq5SHfAFbabIALekIRtaUX6EscuWxjU6MPP4S8Ld41kiub5mLVx5YzB3MAK7lMfC6gbtRQY9RWM-LZq739fXbf2U30W5M0R2ZDGsQNfAKcHoqfIQNPJBo" # ç¤ºä¾‹ï¼Œè¯·æ›¿æ¢

# -- C. æœ¬åœ°æœåŠ¡ä¸ç«¯å£é…ç½® --
MIXFILE_LOCAL_PORT = 4719
FLASK_API_LOCAL_PORT = 5000
MIXFILE_REMOTE_PORT = 20000  # æ˜ å°„åˆ°å…¬ç½‘çš„ MixFile ç«¯å£
FLASK_API_REMOTE_PORT = 20001  # æ˜ å°„åˆ°å…¬ç½‘çš„ Flask API ç«¯å£

# -- D. Killer API & è¿›ç¨‹ç®¡ç†é…ç½® --
KILLER_API_SHUTDOWN_TOKEN = "123456" # !!! å¼ºçƒˆå»ºè®®ä¿®æ”¹ !!!
PROCESS_KEYWORDS_TO_KILL = ["java", "frpc", "python -c"] # æ–°å¢pythonå­è¿›ç¨‹å…³é”®è¯
EXCLUDE_KEYWORDS_FROM_KILL = ["jupyter", "kernel", "ipykernel", "conda", "grep"]

# -- E. å­—å¹•æå–æµç¨‹é…ç½® --
# è¿™äº›å€¼æœªæ¥ä¹Ÿå¯ä»¥åŠ å…¥åˆ°åŠ å¯†é…ç½®ä¸­
SUBTITLE_CHUNK_DURATION_MS = 10 * 60 * 1000  # 10åˆ†é’Ÿ
SUBTITLE_BATCH_SIZE = 40
SUBTITLE_CONCURRENT_REQUESTS = 8
SUBTITLE_REQUESTS_PER_MINUTE = 8

# =============================================================================
# --- ç¬¬ 2 æ­¥: è§£å¯†ä¸é…ç½®åŠ è½½æ¨¡å— ---
# =============================================================================

class DecryptionError(Exception):
    # è‡ªå®šä¹‰å¼‚å¸¸ï¼Œç”¨äºè¡¨ç¤ºè§£å¯†è¿‡ç¨‹ä¸­çš„ç‰¹å®šå¤±è´¥ã€‚
    pass

def _get_decryption_cipher():
    #
    # è¾…åŠ©å‡½æ•°ï¼šä» Kaggle Secrets è·å–å¯†ç å¹¶ç”Ÿæˆ Fernet è§£å¯†å™¨ã€‚
    # è¿™æ˜¯ä¸€ä¸ªå…±äº«é€»è¾‘ï¼Œè¢«å…¶ä»–è§£å¯†å‡½æ•°è°ƒç”¨ã€‚
    #
    if not UserSecretsClient or not Fernet:
        raise DecryptionError("å…³é”®åº“ (kaggle_secrets, cryptography) æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥ã€‚")
    
    print("ğŸ” æ­£åœ¨ä» Kaggle Secrets è·å–è§£å¯†å¯†é’¥ 'FRP_DECRYPTION_KEY'...")
    try:
        secrets = UserSecretsClient()
        decryption_key_password = secrets.get_secret("FRP_DECRYPTION_KEY")
        if not decryption_key_password:
             raise ValueError("Kaggle Secret 'FRP_DECRYPTION_KEY' çš„å€¼ä¸ºç©ºã€‚")
    except Exception as e:
        print(f"âŒ æ— æ³•ä» Kaggle Secrets è·å–å¯†é’¥ï¼è¯·ç¡®ä¿ä½ å·²æ­£ç¡®è®¾ç½®äº†åä¸º 'FRP_DECRYPTION_KEY' çš„ Secretã€‚")
        raise DecryptionError(f"è·å– Kaggle Secret å¤±è´¥: {e}") from e

    # ä½¿ç”¨ SHA256 ä»ç”¨æˆ·å¯†ç æ´¾ç”Ÿä¸€ä¸ªç¡®å®šæ€§çš„ã€å®‰å…¨çš„32å­—èŠ‚å¯†é’¥
    key = base64.urlsafe_b64encode(hashlib.sha256(decryption_key_password.encode()).digest())
    return Fernet(key)

def get_decrypted_config(encrypted_string: str, config_name: str) -> dict:
    #
    # é€šç”¨çš„è§£å¯†å‡½æ•°ï¼Œç”¨äºè§£å¯†ä»»ä½•ç»™å®šçš„åŠ å¯†å­—ç¬¦ä¸²ã€‚
    #
    # Args:
    #     encrypted_string (str): ä» encrypt_util.py è·å–çš„ Base64 ç¼–ç çš„åŠ å¯†å­—ç¬¦ä¸²ã€‚
    #     config_name (str): é…ç½®çš„åç§°ï¼Œç”¨äºæ—¥å¿—è¾“å‡º (ä¾‹å¦‚ "FRP", "Subtitle")ã€‚
    #
    # Returns:
    #     dict: è§£å¯†å¹¶è§£æåçš„é…ç½®å­—å…¸ã€‚
    # 
    # Raises:
    #     DecryptionError: å¦‚æœè§£å¯†è¿‡ç¨‹çš„ä»»ä½•æ­¥éª¤å¤±è´¥ã€‚
    #
    print(f"ğŸ”‘ æ­£åœ¨å‡†å¤‡è§£å¯† {config_name} é…ç½®...")
    try:
        if "PASTE_YOUR_ENCRYPTED" in encrypted_string:
             raise ValueError(f"æ£€æµ‹åˆ°å ä½ç¬¦åŠ å¯†å­—ç¬¦ä¸²ï¼Œè¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®é…ç½®ã€‚")
        
        cipher = _get_decryption_cipher()
        decrypted_bytes = cipher.decrypt(encrypted_string.encode('utf-8'))
        config = json.loads(decrypted_bytes.decode('utf-8'))
        print(f"âœ… {config_name} é…ç½®è§£å¯†æˆåŠŸï¼")
        return config
    except ValueError as e:
        print(f"âŒ è§£å¯† {config_name} é…ç½®å¤±è´¥: {e}")
        raise DecryptionError(f"é…ç½®å­—ç¬¦ä¸²æ— æ•ˆ: {e}")
    except Exception as e:
        print(f"âŒ è§£å¯† {config_name} é…ç½®å¤±è´¥ï¼")
        print("   è¿™é€šå¸¸æ„å‘³ç€ä½ çš„ Kaggle Secret (å¯†ç ) ä¸åŠ å¯†æ—¶ä½¿ç”¨çš„å¯†ç ä¸åŒ¹é…ï¼Œ")
        print("   æˆ–è€…åŠ å¯†å­—ç¬¦ä¸²å·²æŸåã€‚")
        raise DecryptionError(f"è§£å¯†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}") from e
        
# =============================================================================
# --- ç¬¬ 3 æ­¥: ç¯å¢ƒæ£€æŸ¥ã€è¾…åŠ©å‡½æ•°ä¸ Killer API ---
# =============================================================================

def check_environment():
    #
    # æ£€æŸ¥è¿è¡Œæ‰€éœ€çš„æ‰€æœ‰å…³é”®ä¾èµ–å’Œåº“æ˜¯å¦éƒ½å·²æ­£ç¡®å®‰è£…ã€‚
    # å¦‚æœç¼ºå°‘å…³é”®ç»„ä»¶ï¼Œåˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œä½¿ç¨‹åºæå‰å¤±è´¥ã€‚
    #
    print("\n" + "="*60)
    print("ğŸ”¬ æ­£åœ¨æ‰§è¡Œç¯å¢ƒä¾èµ–æ£€æŸ¥...")
    print("="*60)
    
    # æ£€æŸ¥åŸºç¡€åº“
    if not UserSecretsClient or not Fernet:
        raise RuntimeError("å…³é”®å®‰å…¨åº“ 'kaggle_secrets' æˆ– 'cryptography' æœªæ‰¾åˆ°ã€‚æ— æ³•ç»§ç»­ã€‚")
    print("âœ… å®‰å…¨åº“ (kaggle_secrets, cryptography) - æ­£å¸¸")
    
    # æ£€æŸ¥å­—å¹•æå–æ ¸å¿ƒåº“
    critical_subtitle_libs = {
        "torch": torch,
        "torchaudio": torchaudio,
        "pydub": AudioSegment,
        "pydantic": pydantic
    }
    for name, lib in critical_subtitle_libs.items():
        if not lib:
            raise RuntimeError(f"å­—å¹•æå–æ ¸å¿ƒåº“ '{name}' æœªæ‰¾åˆ°æˆ–å¯¼å…¥å¤±è´¥ã€‚è¯·æ£€æŸ¥ Kaggle ç¯å¢ƒã€‚")
        print(f"âœ… å­—å¹•åº“ ({name}) - æ­£å¸¸")

    # æ£€æŸ¥å‘½ä»¤è¡Œå·¥å…·
    if not shutil.which("ffmpeg"):
        raise RuntimeError("'ffmpeg' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¿™æ˜¯æå–éŸ³é¢‘æ‰€å¿…éœ€çš„ã€‚")
    print("âœ… å‘½ä»¤è¡Œå·¥å…· (ffmpeg) - æ­£å¸¸")
    
    if not shutil.which("java"):
        raise RuntimeError("'java' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¿™æ˜¯è¿è¡Œ MixFileCLI æ‰€å¿…éœ€çš„ã€‚")
    print("âœ… å‘½ä»¤è¡Œå·¥å…· (java) - æ­£å¸¸")

    print("\nâœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼æ‰€æœ‰å…³é”®ä¾èµ–å‡å·²å°±ç»ªã€‚\n")


def log_system_event(level: str, message: str, in_worker=False):
    # ä¸€ä¸ªç®€å•çš„å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—è®°å½•å™¨ã€‚
    # æ–°å¢ in_worker å‚æ•°ä»¥åŒºåˆ†æ—¥å¿—æ¥æº
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
    worker_tag = "[WORKER]" if in_worker else "[SYSTEM]"
    prefix = f"[{timestamp} {worker_tag} {level.upper()}]"
    print(f"{prefix} {message}", flush=True)


def ms_to_srt_time(ms: int) -> str:
    # å°†æ¯«ç§’è½¬æ¢ä¸º SRT å­—å¹•æ–‡ä»¶çš„æ—¶é—´æ ¼å¼ (HH:MM:SS,ms)ã€‚
    try:
        ms = int(ms)
    except (ValueError, TypeError):
        ms = 0
    td = timedelta(milliseconds=ms)
    hours, remainder = divmod(td.seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    milliseconds = td.microseconds // 1000
    return f"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}"


def run_command(command: str, log_file: str = None) -> subprocess.Popen:
    #
    # åœ¨åå°ä»¥éé˜»å¡æ–¹å¼æ‰§è¡Œä¸€ä¸ª shell å‘½ä»¤ã€‚
    #
    # Args:
    #     command (str): è¦æ‰§è¡Œçš„å‘½ä»¤ã€‚
    #     log_file (str, optional): å°† stdout å’Œ stderr é‡å®šå‘åˆ°çš„æ—¥å¿—æ–‡ä»¶åã€‚
    #
    # Returns:
    #     subprocess.Popen: æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹å¯¹è±¡ã€‚
    #
    log_system_event("info", f"æ‰§è¡Œå‘½ä»¤: {command}")
    stdout_dest = None
    stderr_dest = None
    if log_file:
        log_handle = open(log_file, 'w', encoding='utf-8')
        stdout_dest = log_handle
        stderr_dest = log_handle
    
    # ä½¿ç”¨ Popen ä»¥éé˜»å¡æ–¹å¼å¯åŠ¨è¿›ç¨‹
    process = subprocess.Popen(
        command,
        shell=True,
        stdout=stdout_dest,
        stderr=stderr_dest,
        encoding='utf-8',
        errors='ignore'
    )
    return process


def wait_for_port(port: int, host: str = '127.0.0.1', timeout: float = 60.0) -> bool:
    #
    # ç­‰å¾…æŒ‡å®šçš„æœ¬åœ°ç«¯å£å˜ä¸ºå¯ç”¨çŠ¶æ€ã€‚
    #
    # Args:
    #     port (int): è¦æ£€æŸ¥çš„ç«¯å£å·ã€‚
    #     host (str, optional): ç›‘å¬çš„ä¸»æœºåœ°å€ã€‚é»˜è®¤ä¸º '127.0.0.1'ã€‚
    #     timeout (float, optional): æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ã€‚é»˜è®¤ä¸º 60.0ã€‚
    #
    # Returns:
    #     bool: å¦‚æœç«¯å£åœ¨è¶…æ—¶å‰å˜ä¸ºå¯ç”¨ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    #
    log_system_event("info", f"æ­£åœ¨ç­‰å¾…ç«¯å£ {host}:{port} å¯åŠ¨...")
    start_time = time.perf_counter()
    while True:
        try:
            with socket.create_connection((host, port), timeout=1):
                log_system_event("info", f"âœ… ç«¯å£ {port} å·²æˆåŠŸå¯åŠ¨ï¼")
                return True
        except (socket.timeout, ConnectionRefusedError):
            time.sleep(1)
            if time.perf_counter() - start_time >= timeout:
                log_system_event("error", f"âŒ ç­‰å¾…ç«¯å£ {port} è¶…æ—¶ ({timeout}ç§’)ã€‚")
                return False

# --- Killer API é€»è¾‘ (ç”¨äºè¿œç¨‹å…³é—­) ---

def _find_and_kill_targeted_processes(signal_to_send=signal.SIGTERM):
    # æŸ¥æ‰¾å¹¶ç»ˆæ­¢æ­¤è„šæœ¬å¯åŠ¨çš„æ‰€æœ‰å…³é”®å­è¿›ç¨‹ (java, frpc, python -c ...)ã€‚
    killed_pids_info = []
    log_system_event("info", f"æŸ¥æ‰¾å¹¶å°è¯•ç»ˆæ­¢ä¸ '{PROCESS_KEYWORDS_TO_KILL}' ç›¸å…³çš„è¿›ç¨‹...")
    
    current_kernel_pid = os.getpid()
    pids_to_target = set()

    try:
        # ä½¿ç”¨ ps å‘½ä»¤è·å–æ‰€æœ‰è¿›ç¨‹ä¿¡æ¯
        ps_output = subprocess.check_output(['ps', '-eo', 'pid,ppid,args'], text=True)
        
        for line in ps_output.splitlines()[1:]:
            parts = line.strip().split(None, 2)
            if len(parts) < 3: continue
            
            try:
                pid = int(parts[0])
                command_line = parts[2]
                command_lower = command_line.lower()
            except (ValueError, IndexError):
                continue
            
            # æ’é™¤å½“å‰å†…æ ¸è¿›ç¨‹å’Œç³»ç»Ÿå…³é”®è¿›ç¨‹
            if pid == current_kernel_pid: continue
            is_excluded = any(ex_kw.lower() in command_lower for ex_kw in EXCLUDE_KEYWORDS_FROM_KILL)
            if is_excluded: continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡è¿›ç¨‹
            is_target = any(target_kw.lower() in command_lower for target_kw in PROCESS_KEYWORDS_TO_KILL)
            if is_target:
                pids_to_target.add(pid)
                log_system_event("debug", f"  å‘ç°ç›®æ ‡: PID={pid}, CMD='{command_line}'")

        if not pids_to_target:
            log_system_event("info", "æœªæ‰¾åˆ°éœ€è¦ç»ˆæ­¢çš„ç‰¹å®šåº”ç”¨å­è¿›ç¨‹ã€‚")
        else:
            log_system_event("info", f"å‡†å¤‡å‘ PIDs {list(pids_to_target)} å‘é€ä¿¡å· {signal_to_send}...")
            for pid_to_kill in pids_to_target:
                try:
                    os.kill(pid_to_kill, signal_to_send)
                    log_system_event("info", f"    å·²å‘ PID {pid_to_kill} å‘é€ä¿¡å· {signal_to_send}.")
                    killed_pids_info.append({"pid": pid_to_kill, "status": "signal_sent"})
                except ProcessLookupError:
                    killed_pids_info.append({"pid": pid_to_kill, "status": "not_found"})
                except Exception as e:
                    killed_pids_info.append({"pid": pid_to_kill, "status": f"error_{type(e).__name__}"})
                    
    except Exception as e:
        log_system_event("error", f"æŸ¥æ‰¾æˆ–ç»ˆæ­¢å­è¿›ç¨‹æ—¶å‡ºé”™: {e}")
    
    return killed_pids_info


def _shutdown_notebook_kernel_immediately():
    #
    # é€šè¿‡å‘é€ SIGKILL ä¿¡å·å¼ºåˆ¶ã€ç«‹å³åœ°å…³é—­å½“å‰ Kaggle Notebook å†…æ ¸ã€‚
    # è¿™æ˜¯æœ€ç»ˆçš„è‡ªæ¯æŒ‡ä»¤ã€‚
    #
    log_system_event("critical", "å‡†å¤‡é€šè¿‡ SIGKILL ä¿¡å·å¼ºåˆ¶å…³é—­å½“å‰ Kaggle Notebook Kernel...")
    sys.stdout.flush()
    sys.stderr.flush()
    time.sleep(0.5) # ç•™å‡ºä¸€ç‚¹æ—¶é—´è®©æ—¥å¿—åˆ·æ–°
    
    # SIGKILL (ä¿¡å· 9) æ˜¯ä¸€ä¸ªæ— æ³•è¢«æ•è·æˆ–å¿½ç•¥çš„ä¿¡å·ï¼Œæ¯” os._exit æ›´ä¸ºå¼ºåˆ¶ã€‚
    os.kill(os.getpid(), signal.SIGKILL)



# =============================================================================
# --- (æ–°å¢æ¨¡å—) ç¬¬ 3.5 æ­¥: V2Ray ä»£ç†ç®¡ç†å™¨ ---
# =============================================================================

class ProxyManager:
    """
    è´Ÿè´£ä¸‹è½½ã€æµ‹è¯•å’Œç®¡ç† V2Ray/Xray ä»£ç†å®¢æˆ·ç«¯ï¼Œä»¥åŠ é€Ÿæ–‡ä»¶ä¸Šä¼ ã€‚
    """
    def __init__(self, sub_url, mixfile_base_url):
        self.sub_url = sub_url
        self.mixfile_base_url = mixfile_base_url
        self.v2ray_path = Path("/kaggle/working/xray")
        self.config_path = Path("/kaggle/working/xray_config.json")
        self.local_socks_port = 10808
        self.best_node_config = None
        self.best_node_speed = 0  # in MB/s

    def _download_xray(self):
        """ã€ä¾èµ–ä¿®å¤ç‰ˆã€‘ä¸‹è½½ Xray æ ¸å¿ƒçš„åŒæ—¶ï¼Œä¹Ÿä¸‹è½½å…¶è·¯ç”±æ‰€éœ€çš„ geoip.dat å’Œ geosite.dat æ–‡ä»¶ã€‚"""
        geoip_path = Path("/kaggle/working/geoip.dat")
        geosite_path = Path("/kaggle/working/geosite.dat")

        # æ£€æŸ¥æ ¸å¿ƒå’Œæ•°æ®åº“æ–‡ä»¶æ˜¯å¦éƒ½å­˜åœ¨
        if self.v2ray_path.exists() and geoip_path.exists() and geosite_path.exists():
            log_system_event("info", "Xray æ ¸å¿ƒåŠæ•°æ®åº“æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
            return
        
        log_system_event("info", "æ­£åœ¨ä¸‹è½½ Xray æ ¸å¿ƒåŠæ•°æ®åº“æ–‡ä»¶...")
        
        # å®šä¹‰ä¸‹è½½URL
        xray_url = "https://github.com/XTLS/Xray-core/releases/download/v1.8.10/Xray-linux-64.zip"
        geoip_url = "https://github.com/Loyalsoldier/v2ray-rules-dat/releases/latest/download/geoip.dat"
        geosite_url = "https://github.com/Loyalsoldier/v2ray-rules-dat/releases/latest/download/geosite.dat"
        
        zip_path = Path("/kaggle/working/xray.zip")
        
        try:
            # ä¸‹è½½ Xray æ ¸å¿ƒ
            if not self.v2ray_path.exists():
                log_system_event("info", "  -> ä¸‹è½½ Xray core...")
                run_command(f"wget -q -O {zip_path} {xray_url}").wait()
                run_command(f"unzip -o {zip_path} xray -d /kaggle/working/").wait()
                self.v2ray_path.chmod(0o755)
                zip_path.unlink()

            # ä¸‹è½½ geoip.dat
            if not geoip_path.exists():
                log_system_event("info", "  -> ä¸‹è½½ geoip.dat...")
                run_command(f"wget -q -O {geoip_path} {geoip_url}").wait()
            
            # ä¸‹è½½ geosite.dat
            if not geosite_path.exists():
                log_system_event("info", "  -> ä¸‹è½½ geosite.dat...")
                run_command(f"wget -q -O {geosite_path} {geosite_url}").wait()

            log_system_event("info", "âœ… Xray æ ¸å¿ƒåŠæ•°æ®åº“æ–‡ä»¶ä¸‹è½½å®Œæˆã€‚")
        except Exception as e:
            raise RuntimeError(f"ä¸‹è½½ Xray ç»„ä»¶å¤±è´¥: {e}")

    def _fetch_and_parse_subscription(self):
        """è·å–å¹¶è§£æè®¢é˜…é“¾æ¥ï¼Œè¿”å›èŠ‚ç‚¹é“¾æ¥åˆ—è¡¨ã€‚"""
        log_system_event("info", f"æ­£åœ¨ä» {self.sub_url[:30]}... è·å–è®¢é˜…...")
        try:
            response = requests.get(self.sub_url, timeout=20)
            response.raise_for_status()
            decoded_content = base64.b64decode(response.content).decode('utf-8')
            return decoded_content.strip().split('\n')
        except Exception as e:
            log_system_event("error", f"è·å–æˆ–è§£æè®¢é˜…å¤±è´¥: {e}")
            return []

    def _generate_node_config(self, node_url):
        """ã€è·¯ç”±ä¿®å¤ç‰ˆã€‘åœ¨é…ç½®ä¸­åŠ å…¥æ™ºèƒ½è·¯ç”±ï¼ŒåŒºåˆ†ä»£ç†æµé‡å’Œç›´è¿æµé‡ã€‚"""
        try:
            parsed_url = urlparse(node_url)
            node_name_raw = parsed_url.fragment
            node_name = unquote(node_name_raw) if node_name_raw else "Unnamed Node"
            
            # --- æ­¥éª¤ 1: æ„å»º Outbounds ---
            #
            # Proxy Outbound (tag: "proxy")
            protocol = parsed_url.scheme
            proxy_outbound = {"protocol": protocol, "settings": {}, "tag": "proxy"}
            
            if protocol == "vmess":
                # ... vmess é€»è¾‘ (ä¿æŒä¸å˜) ...
                try:
                    decoded_vmess_str = base64.b64decode(parsed_url.netloc).decode('utf-8')
                    decoded_vmess = json.loads(decoded_vmess_str)
                except Exception:
                     return None, f"Invalid VMess format for node {node_name}"

                proxy_outbound["settings"]["vnext"] = [{
                    "address": decoded_vmess["add"],
                    "port": int(decoded_vmess["port"]),
                    "users": [{"id": decoded_vmess["id"], "alterId": int(decoded_vmess["aid"]), "security": decoded_vmess.get("scy", "auto")}]
                }]
                stream_settings = {"network": decoded_vmess.get("net", "tcp")}
                if stream_settings["network"] == "ws":
                    ws_settings = {"path": decoded_vmess.get("path", "/")}
                    host = decoded_vmess.get("host")
                    if host:
                        ws_settings["headers"] = {"Host": host}
                    stream_settings["wsSettings"] = ws_settings
                if decoded_vmess.get("tls", "") == "tls":
                     stream_settings["security"] = "tls"
                     stream_settings["tlsSettings"] = {"serverName": decoded_vmess.get("sni", decoded_vmess.get("host", decoded_vmess["add"]))}
                proxy_outbound["streamSettings"] = stream_settings

            elif protocol == "vless":
                # ... vless é€»è¾‘ (ä¿æŒä¸å˜) ...
                qs = parse_qs(parsed_url.query)
                user_obj = { "id": parsed_url.username, "encryption": "none", "flow": qs.get("flow", [None])[0], "alterId": 0, "security": "auto" }
                if user_obj["flow"] is None: del user_obj["flow"]
                proxy_outbound["settings"]["vnext"] = [{"address": parsed_url.hostname, "port": int(parsed_url.port), "users": [user_obj]}]
                stream_settings = {"network": qs.get("type", ["tcp"])[0]}
                if stream_settings["network"] == "ws":
                    ws_path = unquote(qs.get("path", ["/"])[0])
                    ws_host = unquote(qs.get("host", [""])[0])
                    ws_settings = {"path": ws_path}
                    if ws_host: ws_settings["headers"] = {"Host": ws_host}
                    stream_settings["wsSettings"] = ws_settings
                if qs.get("security", ["none"])[0] == "tls":
                    stream_settings["security"] = "tls"
                    tls_sni = unquote(qs.get("sni", [""])[0]) or unquote(qs.get("host", [""])[0]) or parsed_url.hostname
                    fp = qs.get("fp", ["random"])[0]
                    tls_settings = {"serverName": tls_sni, "fingerprint": fp, "allowInsecure": False, "show": False}
                    alpn = qs.get("alpn")
                    if alpn: tls_settings["alpn"] = [val for val in alpn[0].split(',') if val]
                    stream_settings["tlsSettings"] = tls_settings
                proxy_outbound["streamSettings"] = stream_settings

            else:
                return None, f"Unsupported protocol: {protocol}"

            # Direct Outbound (tag: "direct")
            direct_outbound = {"protocol": "freedom", "settings": {}, "tag": "direct"}
            
            # Block Outbound (tag: "block") - for ads, etc.
            block_outbound = {"protocol": "blackhole", "settings": {}, "tag": "block"}

            # --- æ­¥éª¤ 2: æ„å»ºæœ€ç»ˆé…ç½® ---
            config = {
                "log": {"loglevel": "warning"},
                "dns": { "servers": ["8.8.8.8", "1.1.1.1", "localhost"] },
                "inbounds": [{
                    "port": self.local_socks_port,
                    "protocol": "socks",
                    "listen": "127.0.0.1",
                    "settings": {"auth": "noauth", "udp": True},
                    "sniffing": { "enabled": True, "destOverride": ["http", "tls"] }
                }],
                "outbounds": [
                    proxy_outbound,
                    direct_outbound,
                    block_outbound
                ],
                "routing": {
                    "domainStrategy": "AsIs",
                    "rules": [
                        { # è§„åˆ™1: ç›´è¿ç§æœ‰åœ°å€å’Œæœ¬åœ°åœ°å€
                            "type": "field",
                            "ip": ["geoip:private"],
                            "outboundTag": "direct"
                        },
                        { # è§„åˆ™2: (å¯é€‰) æ‹¦æˆªå¹¿å‘Š
                            "type": "field",
                            "domain": ["geosite:category-ads-all"],
                            "outboundTag": "block"
                        },
                        # é»˜è®¤è§„åˆ™ï¼šå…¶ä»–æ‰€æœ‰æµé‡éƒ½èµ°ä»£ç†
                        # ï¼ˆXrayé»˜è®¤ä¼šå°†ä¸åŒ¹é…ä»»ä½•è§„åˆ™çš„æµé‡å‘å¾€ç¬¬ä¸€ä¸ªoutboundï¼Œå³proxy_outboundï¼‰
                    ]
                }
            }
            
            return config, node_name
        except Exception as e:
            import traceback
            traceback.print_exc()
            return None, f"Error parsing node '{node_name}': {e}"

    def _test_node_upload_speed(self, node_config, node_name):
        """å¯åŠ¨èŠ‚ç‚¹å¹¶æµ‹è¯•å…¶ä¸Šä¼ é€Ÿåº¦ï¼Œè¿”å›MB/sã€‚"""
        log_system_event("info", f"  -> æ­£åœ¨æµ‹è¯•èŠ‚ç‚¹: {node_name}...")
        with open(self.config_path, 'w') as f:
            json.dump(node_config, f)
        
        process = run_command(f"{self.v2ray_path} -c {self.config_path}")
        if not wait_for_port(self.local_socks_port, timeout=10):
            log_system_event("warning", f"     èŠ‚ç‚¹ {node_name} å¯åŠ¨å¤±è´¥ã€‚")
            process.terminate()
            process.wait()
            return 0

        speed = 0
        try:
            proxies = {
                'http': f'socks5h://127.0.0.1:{self.local_socks_port}',
                'https': f'socks5h://127.0.0.1:{self.local_socks_port}',
            }
            # åˆ›å»ºä¸€ä¸ª 2MB çš„éšæœºæ–‡ä»¶ç”¨äºæµ‹è¯•
            test_data_size = 5 * 1024 * 1024
            test_data = ''.join(random.choices(string.ascii_letters + string.digits, k=test_data_size)).encode()
            
            test_upload_url = urljoin(self.mixfile_base_url, "/api/upload/proxy_test.tmp")
            
            start_time = time.time()
            # æ³¨æ„ï¼šè¿™é‡Œçš„æµ‹è¯•ç›®æ ‡æ˜¯MixFileæœåŠ¡ï¼Œç¡®ä¿æµ‹è¯•çš„æ˜¯çœŸå®ä¸Šä¼ é“¾è·¯
            response = requests.put(test_upload_url, data=test_data, proxies=proxies, timeout=60)
            end_time = time.time()
            
            response.raise_for_status()
            
            duration = end_time - start_time
            speed = (test_data_size / duration) / (1024 * 1024) # MB/s
            log_system_event("info", f"     âœ… èŠ‚ç‚¹ {node_name} å¯ç”¨ï¼Œä¸Šä¼ é€Ÿåº¦: {speed:.2f} MB/s")
        except Exception as e:
            log_system_event("warning", f"     âŒ èŠ‚ç‚¹ {node_name} æµ‹è¯•å¤±è´¥: {e}")
        finally:
            process.terminate()
            process.wait()
            time.sleep(1) # ç¡®ä¿ç«¯å£å·²é‡Šæ”¾
        return speed

    def setup_best_proxy(self):
        """ä¸»æµç¨‹ï¼šå¯»æ‰¾å¹¶å¯åŠ¨æœ€å¿«çš„ä»£ç†èŠ‚ç‚¹ã€‚"""
        global GLOBAL_PROXY_SETTINGS
        try:
            self._download_xray()
            node_urls = self._fetch_and_parse_subscription()
            if not node_urls:
                log_system_event("warning", "æœªè·å–åˆ°ä»»ä½•ä»£ç†èŠ‚ç‚¹ï¼Œå°†ä¸ä½¿ç”¨ä»£ç†ã€‚")
                return

            log_system_event("info", f"è·å–åˆ° {len(node_urls)} ä¸ªèŠ‚ç‚¹ï¼Œå¼€å§‹æµ‹é€Ÿ...")
            
            for node_url in node_urls:
                node_config, node_name = self._generate_node_config(node_url.strip())
                if not node_config:
                    log_system_event("debug", f"è·³è¿‡ä¸æ”¯æŒçš„èŠ‚ç‚¹æˆ–è§£æå¤±è´¥: {node_name}")
                    continue
                
                speed = self._test_node_upload_speed(node_config, node_name)
                if speed > self.best_node_speed:
                    self.best_node_speed = speed
                    self.best_node_config = node_config
            
            if self.best_node_config:
                log_system_event("info", "="*60)
                log_system_event("info", f"ğŸš€ æœ€å¿«èŠ‚ç‚¹é€‰æ‹©å®Œæˆï¼é€Ÿåº¦: {self.best_node_speed:.2f} MB/s")
                log_system_event("info", "æ­£åœ¨åå°å¯åŠ¨æ­¤èŠ‚ç‚¹ç”¨äºåç»­æ‰€æœ‰ä¸Šä¼ ä»»åŠ¡...")
                log_system_event("info", "="*60)

                with open(self.config_path, 'w') as f:
                    json.dump(self.best_node_config, f)
                
                run_command(f"{self.v2ray_path} -c {self.config_path}", "xray.log")
                if not wait_for_port(self.local_socks_port, timeout=10):
                    raise RuntimeError("å¯åŠ¨æœ€ä¼˜ä»£ç†èŠ‚ç‚¹å¤±è´¥ï¼")
                
                GLOBAL_PROXY_SETTINGS = {
                    'http': f'socks5h://127.0.0.1:{self.local_socks_port}',
                    'https': f'socks5h://127.0.0.1:{self.local_socks_port}',
                }
            else:
                log_system_event("warning", "æ‰€æœ‰èŠ‚ç‚¹å‡æµ‹è¯•å¤±è´¥ï¼Œæœ¬æ¬¡è¿è¡Œå°†ä¸ä½¿ç”¨ä»£ç†ã€‚")

        except Exception as e:
            log_system_event("error", f"è®¾ç½®ä»£ç†æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}ã€‚å°†ä¸ä½¿ç”¨ä»£ç†ã€‚")

# =============================================================================
# --- ç¬¬ 4 æ­¥: å­—å¹•æå–æ ¸å¿ƒæ¨¡å— (åœ¨å­è¿›ç¨‹ä¸­è°ƒç”¨) ---
# =============================================================================

# --- A. Pydantic æ•°æ®éªŒè¯æ¨¡å‹ ---
# ç”¨äºä¸¥æ ¼éªŒè¯ Gemini API è¿”å›çš„ JSON ç»“æ„ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚

class SubtitleLine(pydantic.BaseModel):
    start_ms: int
    end_ms: int | None = None
    text: str

class BatchTranscriptionResult(pydantic.BaseModel):
    subtitles: list[SubtitleLine]

def load_ai_models():
    """
    åœ¨å·¥ä½œè¿›ç¨‹å¯åŠ¨æ—¶é¢„åŠ è½½æ‰€æœ‰éœ€è¦çš„ AI æ¨¡å‹ã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«å·²åŠ è½½æ¨¡å‹çš„å­—å…¸ã€‚
    """
    log_system_event("info", "å·¥ä½œè¿›ç¨‹æ­£åœ¨é¢„åŠ è½½ AI æ¨¡å‹...", in_worker=True)
    loaded_models = {
        "denoiser": None
    }
    
    # 1. å°è¯•åŠ è½½ AI é™å™ªæ¨¡å‹
    try:
        from denoiser import pretrained
        log_system_event("info", "æ­£åœ¨åŠ è½½ AI é™å™ªæ¨¡å‹ (denoiser)...", in_worker=True)
        denoiser_model = pretrained.dns64().cuda()
        loaded_models["denoiser"] = denoiser_model
        log_system_event("info", "âœ… AI é™å™ªæ¨¡å‹åŠ è½½æˆåŠŸã€‚", in_worker=True)
    except Exception as e:
        log_system_event("warning", f"é¢„åŠ è½½ AI é™å™ªæ¨¡å‹å¤±è´¥ï¼Œé™å™ªåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚é”™è¯¯: {e}", in_worker=True)

    # 2. VAD æ¨¡å‹è¯´æ˜
    # faster-whisper çš„ VAD åŠŸèƒ½ (get_speech_timestamps) æ˜¯ä¸€ä¸ªè½»é‡çº§å‡½æ•°ï¼Œ
    # ä¸éœ€è¦åƒé™å™ªæ¨¡å‹é‚£æ ·è¿›è¡Œé‡é‡çº§çš„é¢„åŠ è½½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡ŒåªåŠ è½½é™å™ªæ¨¡å‹ã€‚
    
    return loaded_models

# --- B. åŠ¨æ€æç¤ºè¯ä¸æ–‡ä»¶å¤„ç† ---

def get_dynamic_prompts(api_url: str) -> tuple[str, str]:
    #
    # ä»æŒ‡å®šçš„ API è·å–åŠ¨æ€æç¤ºè¯ã€‚å¦‚æœå¤±è´¥ï¼Œåˆ™è¿”å›ç¡¬ç¼–ç çš„å¤‡ç”¨æç¤ºè¯ã€‚
    #
    log_system_event("info", "æ­£åœ¨å°è¯•ä» API è·å–åŠ¨æ€æç¤ºè¯...", in_worker=True)
    try:
        response = requests.get(api_url, timeout=10)
        response.raise_for_status()
        prompts = response.json()
        if "system_instruction" in prompts and "prompt_for_task" in prompts:
            log_system_event("info", "âœ… æˆåŠŸä» API è·å–åŠ¨æ€æç¤ºè¯ã€‚", in_worker=True)
            return prompts['system_instruction'], prompts['prompt_for_task']
        else:
            raise ValueError("API å“åº”ä¸­ç¼ºå°‘å¿…è¦çš„é”®ã€‚")
    except Exception as e:
        log_system_event("warning", f"è·å–åŠ¨æ€æç¤ºè¯å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨å¤‡ç”¨æç¤ºè¯ã€‚", in_worker=True)
        # --- å¤‡ç”¨æç¤ºè¯ (Fallback Prompts) ---
        fallback_system = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­—å¹•ç¿»è¯‘æ¨¡å‹ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æä¾›çš„ä»»ä½•è¯­è¨€çš„éŸ³é¢‘å†…å®¹ç¿»è¯‘æˆ**ç®€ä½“ä¸­æ–‡**ã€‚"
            "åœ¨ä½ çš„æ‰€æœ‰è¾“å‡ºä¸­ï¼Œ`text` å­—æ®µçš„å€¼**å¿…é¡»æ˜¯ç®€ä½“ä¸­æ–‡**ã€‚"
            "**ç»å¯¹ç¦æ­¢**åœ¨ `text` å­—æ®µä¸­è¿”å›ä»»ä½•åŸå§‹è¯­è¨€ï¼ˆå¦‚æ—¥è¯­ï¼‰çš„æ–‡æœ¬ã€‚"
        )
        fallback_task = (
            "æˆ‘å°†ä¸ºä½ æä¾›ä¸€ç³»åˆ—éŸ³é¢‘ç‰‡æ®µå’Œå®ƒä»¬åœ¨è§†é¢‘ä¸­çš„ç»å¯¹æ—¶é—´ `[AUDIO_INFO] start_ms --> end_ms`ã€‚\n"
            "è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n"
            "1.  **å¬å–éŸ³é¢‘**å¹¶ç†è§£å…¶å†…å®¹ã€‚\n"
            "2.  **åˆ›å»ºå†…éƒ¨æ—¶é—´è½´**: å°†**ç¿»è¯‘æˆä¸­æ–‡å**çš„æ–‡æœ¬ï¼Œæ ¹æ®è¯­éŸ³åœé¡¿åˆ†å‰²æˆæ›´çŸ­çš„å­—å¹•è¡Œï¼Œå¹¶ä¸ºæ¯ä¸€è¡Œä¼°ç®—ä¸€ä¸ªç²¾ç¡®çš„ `start_ms` å’Œ `end_ms`ã€‚\n"
            "3.  **æ ¼å¼åŒ–è¾“å‡º**: ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªæ ¼å¼å®Œå…¨æ­£ç¡®çš„ JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½• markdown æ ‡è®°ã€‚ç»“æ„å¦‚ä¸‹ï¼Œå…¶ä¸­ `text` å­—æ®µçš„å€¼å¿…é¡»æ˜¯ä½ ç¿»è¯‘åçš„**ç®€ä½“ä¸­æ–‡**ï¼š\n"
            '{\n'
            '  "subtitles": [\n'
            '    { "start_ms": 12345, "end_ms": 13456, "text": "è¿™æ˜¯ç¿»è¯‘åçš„ç¬¬ä¸€å¥å­—å¹•" },\n'
            '    { "start_ms": 13600, "text": "è¿™æ˜¯ç¬¬äºŒå¥" }\n'
            '  ]\n'
            '}\n'
        )
        return fallback_system, fallback_task


def read_and_encode_file_base64(filepath: str) -> str | None:
    # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è¿”å› Base64 ç¼–ç çš„å­—ç¬¦ä¸²ã€‚
    try:
        with open(filepath, "rb") as f:
            binary_data = f.read()
        return base64.b64encode(binary_data).decode('utf-8')
    except Exception as e:
        log_system_event("error", f"æ— æ³•è¯»å–æˆ–ç¼–ç æ–‡ä»¶: {filepath}. é”™è¯¯: {e}", in_worker=True)
        return None

# --- C. éŸ³é¢‘é¢„å¤„ç†ç®¡é“ ---

def preprocess_audio_for_subtitles(
    video_path: Path,
    temp_dir: Path,
    update_status_callback: callable,
    ai_models: dict  # <--- ä¿®æ”¹ï¼šå¢åŠ  ai_models å‚æ•°
) -> list[dict]:
    #
    # å®Œæ•´çš„éŸ³é¢‘é¢„å¤„ç†æµç¨‹ï¼šæå– -> é™å™ª -> VAD åˆ‡åˆ†ã€‚
    #
    # Args:
    #     video_path (Path): è¾“å…¥çš„è§†é¢‘æ–‡ä»¶è·¯å¾„ã€‚
    #     temp_dir (Path): ç”¨äºå­˜æ”¾æ‰€æœ‰ä¸­é—´æ–‡ä»¶çš„ä¸´æ—¶ç›®å½•ã€‚
    #     update_status_callback (callable): ç”¨äºæ›´æ–°ä»»åŠ¡çŠ¶æ€çš„å›è°ƒå‡½æ•°ã€‚
    #
    # Returns:
    #     list[dict]: ä¸€ä¸ªåŒ…å«æ‰€æœ‰æœ‰æ•ˆè¯­éŸ³ç‰‡æ®µä¿¡æ¯çš„åˆ—è¡¨ï¼Œ
    #                 æ¯ä¸ªå…ƒç´ æ˜¯ {"path": str, "start_ms": int, "end_ms": int}ã€‚
    # 
    # Raises:
    #     Exception: å¦‚æœåœ¨ä»»ä½•å…³é”®æ­¥éª¤ï¼ˆå¦‚ ffmpegï¼‰ä¸­å‘ç”Ÿå¤±è´¥ã€‚
    #
    
    # 1. ä½¿ç”¨ ffmpeg æå–åŸå§‹éŸ³é¢‘
    update_status_callback(stage="subtitle_extract_audio", details="æ­£åœ¨ä»è§†é¢‘ä¸­æå–éŸ³é¢‘...")
    raw_audio_path = temp_dir / "raw_audio.wav"
    try:
        command = [
            "ffmpeg", "-i", str(video_path),
            "-ac", "1", "-ar", "16000", # å•å£°é“, 16kHz é‡‡æ ·ç‡ (è¯­éŸ³è¯†åˆ«æ ‡å‡†)
            "-vn", "-y", "-loglevel", "error", str(raw_audio_path)
        ]
        # ä½¿ç”¨ subprocess.run ç­‰å¾…å‘½ä»¤å®Œæˆ
        process = subprocess.run(command, check=True, capture_output=True, text=True)
    except subprocess.CalledProcessError as e:
        log_system_event("error", f"FFmpeg æå–éŸ³é¢‘å¤±è´¥ã€‚Stderr: {e.stderr}", in_worker=True)
        raise RuntimeError(f"FFmpeg æå–éŸ³é¢‘å¤±è´¥: {e.stderr}")

    # 2. å°è¯•åŠ è½½ AI é™å™ªæ¨¡å‹
    # 2. è·å–é¢„åŠ è½½çš„ AI é™å™ªæ¨¡å‹
    denoiser_model = ai_models.get("denoiser")
    if denoiser_model:
        update_status_callback(stage="subtitle_denoise", details="AI é™å™ªæ¨¡å‹å·²åŠ è½½ï¼Œå‡†å¤‡å¤„ç†...")
        log_system_event("info", "å°†ä½¿ç”¨é¢„åŠ è½½çš„ AI é™å™ªæ¨¡å‹ã€‚", in_worker=True)
    else:
        log_system_event("warning", "é™å™ªæ¨¡å‹ä¸å¯ç”¨ï¼Œå°†è·³è¿‡é™å™ªæ­¥éª¤ã€‚", in_worker=True)
    # <--- ä¿®æ”¹ç»“æŸ --->


    # 3. åˆ†å—å¤„ç†éŸ³é¢‘ï¼šé™å™ª -> VAD
    update_status_callback(stage="subtitle_vad", details="æ­£åœ¨è¿›è¡ŒéŸ³é¢‘åˆ†å—ä¸è¯­éŸ³æ£€æµ‹...")
    original_audio = AudioSegment.from_wav(raw_audio_path)
    total_duration_ms = len(original_audio)
    chunk_files = []
    chunks_dir = temp_dir / "audio_chunks"
    chunks_dir.mkdir(exist_ok=True)
    num_chunks = -(-total_duration_ms // SUBTITLE_CHUNK_DURATION_MS)

    for i in range(num_chunks):
        start_time_ms = i * SUBTITLE_CHUNK_DURATION_MS
        end_time_ms = min((i + 1) * SUBTITLE_CHUNK_DURATION_MS, total_duration_ms)
        
        log_system_event("info", f"æ­£åœ¨å¤„ç†éŸ³é¢‘æ€»å— {i+1}/{num_chunks}...", in_worker=True)
        
        audio_chunk = original_audio[start_time_ms:end_time_ms]
        temp_chunk_path = temp_dir / f"temp_chunk_{i}.wav"
        audio_chunk.export(temp_chunk_path, format="wav")
        
        processing_path = temp_chunk_path
        
        # 3.1 AI é™å™ª (å¦‚æœæ¨¡å‹åŠ è½½æˆåŠŸ)
        if denoiser_model:
            try:
                wav, sr = torchaudio.load(temp_chunk_path)
                wav = wav.cuda()
                with torch.no_grad():
                    denoised_wav = denoiser_model(wav[None])[0]
                denoised_chunk_path = temp_dir / f"denoised_chunk_{i}.wav"
                torchaudio.save(denoised_chunk_path, denoised_wav.cpu(), 16000)
                processing_path = denoised_chunk_path
            except Exception as e:
                log_system_event("warning", f"å½“å‰å—é™å™ªå¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹éŸ³é¢‘ã€‚é”™è¯¯: {e}", in_worker=True)
        
        # 3.2 VAD è¯­éŸ³æ£€æµ‹
        try:
            from faster_whisper.audio import decode_audio
            from faster_whisper.vad import VadOptions, get_speech_timestamps
            
            vad_parameters = {
                "threshold": 0.38, 
                "min_speech_duration_ms": 150, 
                "max_speech_duration_s": 15.0, # ç¨å¾®æ”¾å®½ä»¥å®¹çº³é•¿å¥
                "min_silence_duration_ms": 1500, 
                "speech_pad_ms": 500
            }
            sampling_rate = 16000
            audio_data = decode_audio(str(processing_path), sampling_rate=sampling_rate)
            speech_timestamps = get_speech_timestamps(audio_data, vad_options=VadOptions(**vad_parameters))
            
            # 3.3 æ ¹æ® VAD ç»“æœä»åŸå§‹éŸ³é¢‘ä¸­ç²¾ç¡®åˆ‡ç‰‡
            for speech in speech_timestamps:
                relative_start_ms = int(speech["start"] / sampling_rate * 1000)
                relative_end_ms = int(speech["end"] / sampling_rate * 1000)
                absolute_start_ms = start_time_ms + relative_start_ms
                absolute_end_ms = start_time_ms + relative_end_ms
                
                final_chunk = original_audio[absolute_start_ms:absolute_end_ms]
                final_chunk_path = chunks_dir / f"chunk_{absolute_start_ms}.wav"
                final_chunk.export(str(final_chunk_path), format="wav")
                chunk_files.append({
                    "path": str(final_chunk_path),
                    "start_ms": absolute_start_ms,
                    "end_ms": absolute_end_ms
                })
        except Exception as e:
            log_system_event("error", f"å½“å‰å— VAD å¤„ç†å¤±è´¥: {e}", in_worker=True)
    
    log_system_event("info", f"éŸ³é¢‘åˆ†å—å¤„ç†å®Œæˆï¼Œæ€»å…±åˆ‡åˆ†ä¸º {len(chunk_files)} ä¸ªæœ‰æ•ˆè¯­éŸ³ç‰‡æ®µã€‚", in_worker=True)
    return chunk_files

# --- D. AI äº¤äº’ä¸å¹¶å‘è°ƒåº¦ ---

def _process_subtitle_batch_with_ai(
    chunk_group: list[dict],
    group_index: int,
    api_key: str,
    gemini_endpoint_prefix: str,
    prompt_api_url: str
) -> list[dict]:
    """
    å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„éŸ³é¢‘å—ï¼Œè°ƒç”¨ Gemini API è·å–å­—å¹•ã€‚
    (ä¼˜åŒ–ç‰ˆ v2: æ˜ç¡®æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œä¸æ˜¯è¿”å›ç©ºåˆ—è¡¨ï¼Œä»¥ä¾¿ä¸Šå±‚æ•è·)
    """
    log_system_event("info", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å·²åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­å¯åŠ¨...", in_worker=True)
    
    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘: å°†æ•´ä¸ªå‡½æ•°é€»è¾‘åŒ…è£¹åœ¨ä¸€ä¸ªå¤§çš„ try...except å—ä¸­ã€‚
    # è¿™æ ·åšçš„ç›®çš„æ˜¯ï¼Œæ— è®ºå‘ç”Ÿä½•ç§ç±»å‹çš„é”™è¯¯ï¼ˆç½‘ç»œã€APIã€æ•°æ®è§£æç­‰ï¼‰ï¼Œ
    # éƒ½èƒ½ç¡®ä¿å®ƒè¢«é‡æ–°æŠ›å‡ºï¼Œè€Œä¸æ˜¯è¢«å‡½æ•°å†…éƒ¨æ¶ˆåŒ–æ‰ã€‚
    try:
        # 1. è·å–åŠ¨æ€æç¤ºè¯
        system_instruction, prompt_for_task = get_dynamic_prompts(prompt_api_url)

        # 2. æ„å»º REST API è¯·æ±‚ä½“ (payload)
        model_name = "gemini-2.5-flash"
        generate_url = f"{gemini_endpoint_prefix.rstrip('/')}/v1beta/models/{model_name}:generateContent"
        headers = {"x-goog-api-key": api_key, "Content-Type": "application/json"}
        
        parts = [{"text": prompt_for_task}]
        for chunk in chunk_group:
            encoded_data = read_and_encode_file_base64(chunk["path"])
            if not encoded_data:
                # å¦‚æœå•ä¸ªæ–‡ä»¶ç¼–ç å¤±è´¥ï¼Œè®°å½•è­¦å‘Šå¹¶è·³è¿‡ï¼Œä¸å½±å“æ•´ä¸ªæ‰¹æ¬¡
                log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ–‡ä»¶ {chunk['path']} ç¼–ç å¤±è´¥ï¼Œå°†è·³è¿‡æ­¤éŸ³é¢‘ç‰‡æ®µã€‚", in_worker=True)
                continue
            parts.append({"text": f"[AUDIO_INFO] {chunk['start_ms']} --> {chunk['end_ms']}"})
            parts.append({"inlineData": {"mime_type": "audio/wav", "data": encoded_data}})
        
        if len(parts) <= 1:
            # å¦‚æœæ•´ä¸ªæ‰¹æ¬¡çš„æ‰€æœ‰æ–‡ä»¶éƒ½æ— æ³•ç¼–ç ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡´å‘½é”™è¯¯ï¼Œåº”æŠ›å‡ºå¼‚å¸¸
            raise ValueError(f"æ‰¹æ¬¡ {group_index+1} ä¸­çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶å‡æ— æ³•ç¼–ç ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚")

        payload = {
            "contents": [{"parts": parts}],
            "systemInstruction": {"parts": [{"text": system_instruction}]},
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
            ],
            "generationConfig": {"responseMimeType": "application/json"}
        }

        # 3. å‘é€è¯·æ±‚å¹¶å®ç°å…¨é¢çš„é‡è¯•é€»è¾‘
        log_system_event("info", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ•°æ®å‡†å¤‡å®Œæ¯•ï¼Œæ­£åœ¨è°ƒç”¨ Gemini API...", in_worker=True)
        max_retries = 3
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                # ä½¿ç”¨ç¨é•¿çš„è¶…æ—¶æ—¶é—´ï¼Œå› ä¸ºå¯èƒ½åŒ…å«å¤§é‡éŸ³é¢‘æ•°æ®
                response = requests.post(generate_url, headers=headers, json=payload, timeout=300) 
                
                # å¯é‡è¯•çš„æœåŠ¡å™¨ç«¯é”™è¯¯
                if response.status_code in [429, 500, 503, 504]:
                    log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] é‡åˆ°å¯é‡è¯•çš„ API é”™è¯¯ (HTTP {response.status_code}, å°è¯• {attempt + 1}/{max_retries})ã€‚", in_worker=True)
                    # è§¦å‘ä¸‹ä¸€æ¬¡å¾ªç¯çš„é‡è¯•
                    raise requests.exceptions.HTTPError(f"Server error: {response.status_code}")

                # ä»»ä½•å…¶ä»–é2xxçš„çŠ¶æ€ç ï¼Œéƒ½è§†ä¸ºä¸å¯é‡è¯•çš„å¤±è´¥
                response.raise_for_status()
                
                # --- å¦‚æœè¯·æ±‚æˆåŠŸï¼Œåˆ™å¤„ç†å“åº” ---
                response_data = response.json()
                
                candidates = response_data.get("candidates")
                if not candidates:
                    raise ValueError(f"APIå“åº”ä¸­ç¼ºå°‘ 'candidates' å­—æ®µã€‚å“åº”: {response.text}")

                content = candidates[0].get("content")
                if not content:
                    finish_reason = candidates[0].get("finishReason", "æœªçŸ¥")
                    safety_ratings = candidates[0].get("safetyRatings", [])
                    raise ValueError(f"APIå“åº”å†…å®¹ä¸ºç©º(å¯èƒ½è¢«å®‰å…¨ç­–ç•¥æ‹¦æˆª)ã€‚åŸå› : {finish_reason}, å®‰å…¨è¯„çº§: {safety_ratings}")

                json_text = content.get("parts", [{}])[0].get("text")
                if json_text is None:
                    raise ValueError(f"APIå“åº”çš„ 'parts' ä¸­ç¼ºå°‘ 'text' å­—æ®µã€‚å“åº”: {response.text}")

                # ä½¿ç”¨ Pydantic éªŒè¯ JSON ç»“æ„
                parsed_result = BatchTranscriptionResult.model_validate_json(json_text)
                subtitles_count = len(parsed_result.subtitles)
                log_system_event("info", f"âœ… [å­—å¹•ä»»åŠ¡ {group_index+1}] æˆåŠŸï¼è·å¾— {subtitles_count} æ¡å­—å¹•ã€‚", in_worker=True)
                
                thread_local_srt_list = []
                for i, subtitle in enumerate(parsed_result.subtitles):
                    if subtitle.end_ms is None:
                        if i + 1 < subtitles_count:
                            subtitle.end_ms = parsed_result.subtitles[i+1].start_ms
                        else:
                            subtitle.end_ms = chunk_group[-1]['end_ms']
                    if subtitle.start_ms > subtitle.end_ms:
                        subtitle.end_ms = subtitle.start_ms + 250
                    
                    line = f"{ms_to_srt_time(subtitle.start_ms)} --> {ms_to_srt_time(subtitle.end_ms)}\n{subtitle.text.strip()}\n"
                    thread_local_srt_list.append({"start_ms": subtitle.start_ms, "srt_line": line})
                
                # æˆåŠŸå¤„ç†ï¼Œè¿”å›ç»“æœï¼Œå‡½æ•°ç»“æŸ
                return thread_local_srt_list

            except (requests.exceptions.RequestException, pydantic.ValidationError, ValueError) as e:
                # æ•è·æ‰€æœ‰é¢„æœŸçš„ã€å¯é‡è¯•çš„æˆ–ä¸å¯é‡è¯•çš„é”™è¯¯
                log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å°è¯• {attempt + 1}/{max_retries} å¤±è´¥ã€‚é”™è¯¯: {type(e).__name__}: {e}", in_worker=True)
                last_exception = e
                if attempt < max_retries - 1:
                    wait_time = 5 * (2 ** attempt)
                    log_system_event("info", f"{wait_time}ç§’åå°†è‡ªåŠ¨é‡è¯•...", in_worker=True)
                    time.sleep(wait_time)
                else:
                    # æ‰€æœ‰é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œè·³å‡ºå¾ªç¯
                    break
        
        # å¦‚æœå¾ªç¯ç»“æŸï¼ˆæ„å‘³ç€æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼‰ï¼Œåˆ™æŠ›å‡ºæœ€åçš„å¼‚å¸¸
        log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥ã€‚", in_worker=True)
        raise RuntimeError(f"æ‰¹æ¬¡ {group_index+1} å¤±è´¥") from last_exception

    except Exception as e:
        # è¿™æ˜¯ä¸€ä¸ªæœ€ç»ˆçš„æ•è·å™¨ï¼Œç¡®ä¿ä»»ä½•æœªé¢„æ–™åˆ°çš„é”™è¯¯éƒ½ä¼šè¢«è®°å½•å¹¶é‡æ–°æŠ›å‡º
        log_system_event("critical", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å‘ç”Ÿæœªé¢„æ–™çš„ä¸¥é‡é”™è¯¯: {type(e).__name__}: {e}", in_worker=True)
        # ã€å…³é”®ã€‘é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ä¾¿ ProcessPoolExecutor èƒ½å¤Ÿæ•è·åˆ°è¿™ä¸ªä»»åŠ¡çš„å¤±è´¥çŠ¶æ€
        raise e

def run_subtitle_extraction_pipeline(subtitle_config: dict, chunk_files: list[dict], update_status_callback: callable) -> str:
    """
    ä¸»è°ƒåº¦å™¨ï¼Œè´Ÿè´£å¹¶å‘å¤„ç†æ‰€æœ‰éŸ³é¢‘æ‰¹æ¬¡å¹¶ç”Ÿæˆæœ€ç»ˆçš„ SRT å†…å®¹ã€‚
    (ä¼˜åŒ–ç‰ˆ v2: ä½¿ç”¨ ProcessPoolExecutor å®ç°è¿›ç¨‹éš”ç¦»ï¼Œé¿å…çº¿ç¨‹æ­»é”)
    """
    
    api_keys = subtitle_config.get('GEMINI_API_KEYS', [])
    prompt_api_url = subtitle_config.get('PROMPT_API_URL', '')
    gemini_endpoint_prefix = subtitle_config.get('GEMINI_API_ENDPOINT_PREFIX', '')
    
    if not all([api_keys, prompt_api_url, gemini_endpoint_prefix]):
        raise ValueError("å­—å¹•é…ç½®ä¸­ç¼ºå°‘ 'GEMINI_API_KEYS', 'PROMPT_API_URL', æˆ– 'GEMINI_API_ENDPOINT_PREFIX'ã€‚")
    
    total_chunks = len(chunk_files)
    if total_chunks == 0:
        log_system_event("warning", "æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆçš„è¯­éŸ³ç‰‡æ®µï¼Œæ— æ³•ç”Ÿæˆå­—å¹•ã€‚", in_worker=True)
        return ""

    chunk_groups = [chunk_files[i:i + SUBTITLE_BATCH_SIZE] for i in range(0, total_chunks, SUBTITLE_BATCH_SIZE)]
    num_groups = len(chunk_groups)
    log_system_event("info", f"å·²å°†è¯­éŸ³ç‰‡æ®µåˆ†ä¸º {num_groups} ä¸ªæ‰¹æ¬¡ï¼Œå‡†å¤‡é€šè¿‡å¤šè¿›ç¨‹å¹¶å‘å¤„ç†ã€‚", in_worker=True)
    update_status_callback(stage="subtitle_transcribing", details=f"å‡†å¤‡å¤„ç† {num_groups} ä¸ªå­—å¹•æ‰¹æ¬¡...")
    
    all_srt_blocks = []
    
    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘: ä½¿ç”¨ ProcessPoolExecutor æ›¿ä»£ ThreadPoolExecutor
    # max_workers å»ºè®®ä¸è¦è®¾ç½®å¾—è¿‡é«˜ï¼Œå› ä¸ºå®ƒä¼šæ¶ˆè€—æ›´å¤šå†…å­˜ã€‚4-8ä¸ªè¿›ç¨‹é€šå¸¸æ˜¯æ¯”è¾ƒå¥½çš„èµ·ç‚¹ã€‚
    # SUBTITLE_CONCURRENT_REQUESTS è¿™ä¸ªå…¨å±€å˜é‡çš„å€¼å¯ä»¥æ ¹æ®æƒ…å†µè°ƒæ•´ã€‚
    with ProcessPoolExecutor(max_workers=SUBTITLE_CONCURRENT_REQUESTS) as executor:
        # ä½¿ç”¨å­—å…¸å°† future æ˜ å°„å›å…¶ä»»åŠ¡ç´¢å¼•ï¼Œä¾¿äºæ—¥å¿—è®°å½•
        future_to_index = {}
        delay_between_submissions = 60.0 / SUBTITLE_REQUESTS_PER_MINUTE
        
        log_system_event("info", "å¼€å§‹å‘è¿›ç¨‹æ± æäº¤æ‰€æœ‰å­—å¹•ä»»åŠ¡...", in_worker=True)
        for i, group in enumerate(chunk_groups):
            api_key_for_process = api_keys[i % len(api_keys)]
            future = executor.submit(
                _process_subtitle_batch_with_ai,
                group,
                i, # ä»»åŠ¡ç´¢å¼•
                api_key_for_process,
                gemini_endpoint_prefix,
                prompt_api_url
            )
            future_to_index[future] = i + 1  # ä»»åŠ¡ç´¢å¼•ä»1å¼€å§‹ï¼Œæ›´ç¬¦åˆæ—¥å¿—ä¹ æƒ¯
            
            if i < num_groups - 1:
                time.sleep(delay_between_submissions)
        
        log_system_event("info", f"æ‰€æœ‰ {num_groups} ä¸ªä»»åŠ¡å‡å·²æäº¤ã€‚ç°åœ¨å¼€å§‹ç­‰å¾…å¹¶å¤„ç†è¿”å›ç»“æœ...", in_worker=True)
        
        completed_count = 0
        for future in as_completed(future_to_index):
            task_index = future_to_index[future]
            completed_count += 1
            
            try:
                # ã€æ–°å¢ã€‘ä¸ºè·å–ç»“æœè®¾ç½®ä¸€ä¸ªåˆç†çš„è¶…æ—¶æ—¶é—´ï¼ˆä¾‹å¦‚15åˆ†é’Ÿï¼‰
                # è¿™ä¸ªæ—¶é—´åº”è¯¥å¤§äºå•ä¸ªæ‰¹æ¬¡å¤„ç†çš„æœ€å¤§å¯èƒ½æ—¶é—´ï¼ˆåŒ…æ‹¬é‡è¯•ï¼‰
                # timeout = (å•ä¸ªè¯·æ±‚è¶…æ—¶ + é‡è¯•ç­‰å¾…) * é‡è¯•æ¬¡æ•°ï¼Œå†åŠ ä¸€äº›ä½™é‡
                # timeout = (360s + 5s*1 + 5s*2) * 3 = (375s) * 3 ~= 20åˆ†é’Ÿ
                result_timeout = 10 * 60 # 20åˆ†é’Ÿ
                
                # è·å–å·²å®Œæˆä»»åŠ¡çš„ç»“æœã€‚å¦‚æœå­è¿›ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼Œ.result()ä¼šé‡æ–°æŠ›å‡ºå®ƒ
                result = future.result(timeout=result_timeout)
                
                if result:
                    all_srt_blocks.extend(result)
                    log_system_event("info", f"âœ… å·²æˆåŠŸå¤„ç†å®Œå­—å¹•ä»»åŠ¡ {task_index} çš„ç»“æœã€‚", in_worker=True)
                else:
                    # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œé™¤é_process_subtitle_batch_with_aiåœ¨æ²¡æœ‰ç»“æœæ—¶è¿”å›äº†Noneæˆ–[]
                    log_system_event("warning", f"å­—å¹•ä»»åŠ¡ {task_index} è¿”å›äº†ç©ºç»“æœï¼Œå¯èƒ½å¤„ç†å¤±è´¥ä½†æœªæŠ›å‡ºå¼‚å¸¸ã€‚", in_worker=True)

            except TimeoutError:
                # ã€æ–°å¢ã€‘æ•è·è¶…æ—¶é”™è¯¯
                log_system_event("error", f"âŒ è·å–å­—å¹•ä»»åŠ¡ {task_index} çš„ç»“æœè¶…æ—¶ï¼è¯¥å­è¿›ç¨‹å¯èƒ½å·²åƒµæ­»ã€‚", in_worker=True)
                # å³ä½¿ä¸€ä¸ªä»»åŠ¡è¶…æ—¶ï¼Œæˆ‘ä»¬ä¾ç„¶è¦ç»§ç»­å¤„ç†å…¶ä»–å·²å®Œæˆçš„ä»»åŠ¡
                
            except Exception as e:
                # ã€å…³é”®ã€‘ç°åœ¨å¯ä»¥æ­£ç¡®æ•è·å­è¿›ç¨‹ä¸­çš„æ‰€æœ‰å¼‚å¸¸
                log_system_event("error", f"âŒ è·å–å­—å¹•ä»»åŠ¡ {task_index} çš„ç»“æœæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
            
            finally:
                # æ— è®ºæˆåŠŸå¤±è´¥ï¼Œéƒ½æ›´æ–°è¿›åº¦
                update_status_callback(stage="subtitle_transcribing", details=f"å·²å¤„ç† {completed_count}/{num_groups} ä¸ªå­—å¹•æ‰¹æ¬¡...")

    log_system_event("info", "æ‰€æœ‰å¹¶å‘ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ­£åœ¨æ•´åˆå­—å¹•...", in_worker=True)
    
    if not all_srt_blocks:
        log_system_event("warning", "æ‰€æœ‰å­—å¹•æ‰¹æ¬¡å¤„ç†å‡å¤±è´¥æˆ–æœªè¿”å›ä»»ä½•å†…å®¹ï¼Œæœ€ç»ˆç”Ÿæˆçš„å­—å¹•ä¸ºç©ºã€‚", in_worker=True)
        # æ ¹æ®ä¸šåŠ¡éœ€æ±‚ï¼Œå¯ä»¥é€‰æ‹©è¿”å›ç©ºå­—ç¬¦ä¸²æˆ–æŠ›å‡ºå¼‚å¸¸
        # è¿™é‡Œé€‰æ‹©è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œè®©ä¸»æµç¨‹åˆ¤æ–­
        return ""

    all_srt_blocks.sort(key=lambda x: x["start_ms"])
    
    final_srt_lines = [f"{i + 1}\n{block['srt_line']}" for i, block in enumerate(all_srt_blocks)]
    final_srt_content = "\n".join(final_srt_lines)
    
    log_system_event("info", f"å­—å¹•æ•´åˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_srt_blocks)} æ¡å­—å¹•ã€‚", in_worker=True)
    
    return final_srt_content

# =============================================================================
# --- ç¬¬ 5 æ­¥: MixFileCLI å®¢æˆ·ç«¯ ---
# =============================================================================

class MixFileCLIClient:
    # ä¸€ä¸ªç®€å•çš„ç”¨äºä¸ MixFileCLI åç«¯ API äº¤äº’çš„å®¢æˆ·ç«¯ã€‚
    def __init__(self, base_url: str, proxies: dict = None):
        if not base_url.startswith("http"):
            raise ValueError("Base URL å¿…é¡»ä»¥ http æˆ– https å¼€å¤´")
        self.base_url = base_url
        self.session = requests.Session()
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘è®¾ç½®ä»£ç†
        if proxies:
            self.session.proxies = proxies

    def _make_request(self, method: str, url: str, **kwargs):
        # ç»Ÿä¸€çš„è¯·æ±‚å‘é€æ–¹æ³•ï¼ŒåŒ…å«é”™è¯¯å¤„ç†ã€‚
        try:
            # ç¡®ä¿è¯·æ±‚ä¸ä¼šè¢«ç¼“å­˜
            headers = kwargs.get('headers', {})
            headers.update({'Cache-Control': 'no-cache', 'Pragma': 'no-cache'})
            kwargs['headers'] = headers
            
            # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´ä»¥é€‚åº”æ…¢é€Ÿç½‘ç»œ
            response = self.session.request(method, url, timeout=600, **kwargs)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            error_text = str(e)
            status_code = 500 # é»˜è®¤ä¸ºé€šç”¨æœåŠ¡å™¨é”™è¯¯
            if e.response is not None:
                error_text = e.response.text
                status_code = e.response.status_code
            return (status_code, error_text)

    def upload_file(self, local_file_path: str, progress_callback: callable = None):
        #
        # ã€ä¿®æ”¹åã€‘ä¸Šä¼ å•ä¸ªæ–‡ä»¶å¹¶è·å–åˆ†äº«ç ï¼Œå¢åŠ äº†è¿›åº¦å›è°ƒåŠŸèƒ½ã€‚
        #
        # Args:
        #     local_file_path (str): æœ¬åœ°æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
        #     progress_callback (callable, optional): è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (bytes_uploaded, total_bytes)ã€‚
        #
        # Returns:
        #     requests.Response or tuple: æˆåŠŸæ—¶è¿”å› Response å¯¹è±¡ï¼Œå¤±è´¥æ—¶è¿”å› (status_code, error_text)ã€‚
        #
        filename = os.path.basename(local_file_path)
        upload_url = urljoin(self.base_url, f"/api/upload/{quote(filename)}")
        
        try:
            file_size = os.path.getsize(local_file_path)
        except OSError as e:
             # å¦‚æœæ–‡ä»¶åœ¨è¿™é‡Œå°±æ‰¾ä¸åˆ°äº†ï¼Œç›´æ¥è¿”å›é”™è¯¯
            return (404, f"File not found: {e}")

        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘åˆ›å»ºä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå®ƒåœ¨è¯»å–æ–‡ä»¶çš„åŒæ—¶è°ƒç”¨å›è°ƒå‡½æ•°
        def file_reader_generator(file_handle):
            chunk_size = 1 * 1024 * 1024 # 1MB chunk
            bytes_read = 0
            while True:
                chunk = file_handle.read(chunk_size)
                if not chunk:
                    if progress_callback: # ç¡®ä¿æœ€å100%è¢«è°ƒç”¨
                        progress_callback(file_size, file_size)
                    break
                bytes_read += len(chunk)
                if progress_callback:
                    progress_callback(bytes_read, file_size)
                yield chunk

        try:
            with open(local_file_path, 'rb') as f:
                # å°†ç”Ÿæˆå™¨ä½œä¸º data ä¼ é€’
                return self._make_request("PUT", upload_url, data=file_reader_generator(f))
        except FileNotFoundError as e:
            return (404, str(e))

# =============================================================================
# --- ç¬¬ 6 æ­¥: ç»Ÿä¸€çš„ Flask API æœåŠ¡ (ä¸»è¿›ç¨‹) ---
# =============================================================================

# --- A. åº”ç”¨åˆå§‹åŒ–ä¸ä»»åŠ¡ç®¡ç† ---
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
# å…¨å±€ä»£ç†é…ç½®ï¼Œå°†åœ¨mainå‡½æ•°ä¸­è¢«è®¾ç½®
GLOBAL_PROXY_SETTINGS = None
# ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡çš„çŠ¶æ€ï¼Œå¹¶ç”¨é”æ¥ä¿è¯çº¿ç¨‹å®‰å…¨
tasks = {}
tasks_lock = threading.Lock()

# å…¨å±€å˜é‡ï¼Œå°†åœ¨ main å‡½æ•°ä¸­è¢«åˆå§‹åŒ–
api_client = None 
subtitle_config_global = {}
FRP_SERVER_ADDR = None # å£°æ˜å…¨å±€å˜é‡
TASK_QUEUE = None # å¤šè¿›ç¨‹ä»»åŠ¡é˜Ÿåˆ—
RESULT_QUEUE = None # å¤šè¿›ç¨‹ç»“æœé˜Ÿåˆ—

# --- B. API è·¯ç”±å®šä¹‰ ---

# =============================================================================
# --- (æ–¹æ³•1/3) unified_upload_endpoint [ä¿®æ”¹å] ---
# =============================================================================
@app.route("/api/upload", methods=["POST"])
def unified_upload_endpoint():
    #
    # ç»Ÿä¸€çš„åª’ä½“å¤„ç†å…¥å£ APIã€‚
    # æ¥æ”¶ä¸€ä¸ª URLï¼Œå¹¶æ ¹æ®å‚æ•°å†³å®šæ˜¯ä¸Šä¼ æ–‡ä»¶ã€æå–å­—å¹•ï¼Œè¿˜æ˜¯ä¸¤è€…éƒ½åšã€‚
    # å§‹ç»ˆä»¥å¼‚æ­¥æ¨¡å¼è¿è¡Œã€‚
    #
    data = request.get_json()
    if not data or "url" not in data:
        return jsonify({"error": "è¯·æ±‚ä½“ä¸­å¿…é¡»åŒ…å« 'url' å­—æ®µ"}), 400

    task_id = str(uuid.uuid4())
    
    # ä»è¯·æ±‚ä¸­æå–å‚æ•°
    request_params = {
        "url": data["url"],
        "extract_subtitle": data.get("extract_subtitle", False),
        "upload_video": data.get("upload_video", True),
        "upload_subtitle": data.get("upload_subtitle", False),
    }

    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘åˆå§‹åŒ–æ–°çš„ã€é¢æ¿å‹å¥½çš„ä»»åŠ¡çŠ¶æ€ç»“æ„
    with tasks_lock:
        tasks[task_id] = {
            "taskId": task_id,
            "status": "QUEUED",
            "progress": 0,
            "error": None,
            "results": {
                "video": {
                    "status": "PENDING" if request_params["upload_video"] else "SKIPPED",
                    "details": "ç­‰å¾…å¤„ç†" if request_params["upload_video"] else "ç”¨æˆ·æœªè¯·æ±‚æ­¤æ“ä½œ",
                    "output": None,
                    "error": None
                },
                "subtitle": {
                    "status": "PENDING" if request_params["extract_subtitle"] else "SKIPPED",
                    "details": "ç­‰å¾…å¤„ç†" if request_params["extract_subtitle"] else "ç”¨æˆ·æœªè¯·æ±‚æ­¤æ“ä½œ",
                    "output": None,
                    "error": None
                }
            },
            "createdAt": time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime()),
            "updatedAt": time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())
        }

    # å°†ä»»åŠ¡æ•°æ®æ”¾å…¥é˜Ÿåˆ—ï¼Œç”±å­è¿›ç¨‹å¤„ç†
    task_data = {
        'task_id': task_id,
        'params': request_params,
        'subtitle_config': subtitle_config_global,
        'api_client_base_url': api_client.base_url,
        'frp_server_addr': FRP_SERVER_ADDR
    }
    TASK_QUEUE.put(task_data)
    
    log_system_event("info", f"å·²åˆ›å»ºæ–°ä»»åŠ¡ {task_id} å¹¶æ¨å…¥å¤„ç†é˜Ÿåˆ—ã€‚")

    # è¿”å›ä½“ä¿æŒä¸å˜ï¼Œä¾ç„¶ç®€æ´
    return jsonify({
        "task_id": task_id,
        "status_url": f"/api/tasks/{task_id}"
    }), 202

@app.route("/api/tasks/<task_id>", methods=["GET"])
def get_task_status_endpoint(task_id):
    # è·å–æŒ‡å®šä»»åŠ¡çš„å½“å‰çŠ¶æ€å’Œç»“æœã€‚
    with tasks_lock:
        task = tasks.get(task_id)
    
    if task:
        return jsonify(task)
    else:
        return jsonify({"error": "æœªæ‰¾åˆ°æŒ‡å®šçš„ task_id"}), 404


@app.route('/killer_status_frp', methods=['GET'])
def health_status_endpoint():
    # ç”¨äºå¤–éƒ¨å¥åº·æ£€æŸ¥çš„ç®€å•ç«¯ç‚¹ã€‚
    return jsonify({
        "status": "killer_api_is_running_via_frp",
        "message": "ç»Ÿä¸€åª’ä½“å¤„ç† API æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶é€šè¿‡ FRP æš´éœ²ã€‚",
        "timestamp": time.time()
    }), 200


@app.route('/force_shutdown_notebook', methods=['GET'])
def force_shutdown_endpoint():
    #
    # è¿œç¨‹å…³é—­ APIï¼Œæ¥æ”¶ä¸€ä¸ª token è¿›è¡ŒéªŒè¯ã€‚
    #
    log_system_event("info", "API /force_shutdown_notebook è¢«è°ƒç”¨...")
    token_from_request = request.args.get('token')

    if token_from_request != KILLER_API_SHUTDOWN_TOKEN:
        log_system_event("error", "API Auth å¤±è´¥: Token æ— æ•ˆã€‚")
        return jsonify({"status": "error", "message": "Unauthorized"}), 401
    
    log_system_event("info", "API Auth æˆåŠŸã€‚æ­£åœ¨å®‰æ’åå°å…³é—­ä»»åŠ¡...")

    def delayed_full_shutdown():
        # Step 1: å…ˆå¹³æ»‘åœ°æ€æ­»å­è¿›ç¨‹
        log_system_event("info", "åå°å…³é—­ä»»åŠ¡ï¼šå¼€å§‹ç»ˆæ­¢å­è¿›ç¨‹...")
        _find_and_kill_targeted_processes(signal.SIGTERM)
        time.sleep(2)
        
        # Step 2: å¼ºåˆ¶ç»ˆæ­¢ä»ç„¶å­˜åœ¨çš„å­è¿›ç¨‹
        log_system_event("info", "åå°å…³é—­ä»»åŠ¡ï¼šå¼ºåˆ¶ç»ˆæ­¢ä»»ä½•æ®‹ç•™å­è¿›ç¨‹...")
        _find_and_kill_targeted_processes(signal.SIGKILL)
        time.sleep(1)

        # Step 3: ç»ˆæ­¢ä¸»å†…æ ¸
        _shutdown_notebook_kernel_immediately()
    
    # ç«‹å³å¯åŠ¨åå°çº¿ç¨‹ï¼Œç„¶åç«‹åˆ»è¿”å›å“åº”ï¼Œç¡®ä¿è°ƒç”¨æ–¹æ”¶åˆ°ç¡®è®¤
    threading.Thread(target=delayed_full_shutdown, daemon=True).start()
    
    return jsonify({
        "status": "shutdown_initiated",
        "message": "å…³é—­ä¿¡å·å·²æ¥æ”¶ã€‚åå°æ­£åœ¨æ‰§è¡Œæ¸…ç†å’Œå†…æ ¸å…³é—­æ“ä½œã€‚",
    }), 200

# =============================================================================
# --- ç¬¬ 7 æ­¥: é«˜å®¹é”™çš„ç»Ÿä¸€ä»»åŠ¡å¤„ç†å™¨ (åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œ) ---
# =============================================================================

def process_unified_task(task_data: dict, result_queue: multiprocessing.Queue, upload_queue: multiprocessing.Queue, subtitle_config: dict, ai_models: dict):
    """
    ã€æœ€ç»ˆä¿®å¤ç‰ˆã€‘ä¿®å¤äº†çŠ¶æ€æŠ¥å‘Šå»¶è¿Ÿé—®é¢˜ï¼Œç¡®ä¿APIçŠ¶æ€ä¸åå°æ“ä½œä¸¥æ ¼åŒæ­¥ã€‚
    """
    task_id = task_data['task_id']
    params = task_data['params']
    temp_dir = Path(f"/kaggle/working/task_{task_id}")
    temp_dir.mkdir(exist_ok=True)
    
    # ... [å†…éƒ¨çŠ¶æ€ _internal_status å’Œ _update_status å‡½æ•°ä¿æŒä¸å˜] ...
    _internal_status = {
        "progress": 0,
        "results": {
            "video": {"status": "PENDING", "details": "å‡†å¤‡ä¸­", "output": None, "error": None},
            "subtitle": {"status": "PENDING", "details": "å‡†å¤‡ä¸­", "output": None, "error": None}
        }
    }

    def _update_status(component=None, status=None, details=None, progress_val=None, output=None, error_obj=None):
        payload = {'type': 'status_update', 'task_id': task_id, 'payload': {}}
        update_target = {}
        if component and component in _internal_status["results"]:
            update_target = _internal_status["results"][component]
            partial_update = {}
            if status: update_target["status"] = status; partial_update['status'] = status
            if details: update_target["details"] = details; partial_update['details'] = details
            if output:
                if update_target.get("output") is None: update_target["output"] = {}
                update_target["output"].update(output)
                partial_update['output'] = update_target['output']
            if error_obj: update_target["error"] = error_obj; partial_update['error'] = error_obj
            payload['payload']['results'] = {component: partial_update}
        if progress_val is not None:
            _internal_status["progress"] = progress_val
            payload['payload']['progress'] = _internal_status['progress']
        if payload['payload']:
             result_queue.put(payload)
             
    try:
        # --- æ­¥éª¤ 1: ä¸‹è½½æ–‡ä»¶ ---
        _update_status(component="video", status="RUNNING", details="å¼€å§‹ä¸‹è½½æ–‡ä»¶...", progress_val=5)
        _update_status(component="subtitle", status="RUNNING", details="ç­‰å¾…è§†é¢‘ä¸‹è½½...")
        file_url = params['url']
        filename = unquote(file_url.split("/")[-1].split("?")[0] or f"file_{task_id}")
        local_file_path = temp_dir / filename

        with requests.get(file_url, stream=True, timeout=60) as r:
            r.raise_for_status()
            total_size = int(r.headers.get('content-length', 0))
            bytes_downloaded = 0
            with open(local_file_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
                    bytes_downloaded += len(chunk)
                    if total_size > 0:
                        dl_progress = round((bytes_downloaded / total_size) * 100)
                        _update_status(component="video", details=f"ä¸‹è½½ä¸­ ({dl_progress}%)...", progress_val=5 + int(dl_progress * 0.2))
        
        # ã€æ ¸å¿ƒä¿®æ”¹ç‚¹ 1ã€‘ä¸‹è½½å®Œæˆåï¼Œæ˜ç¡®æ›´æ–°videoçŠ¶æ€
        _update_status(component="video", details="ä¸‹è½½å®Œæˆï¼Œå‡†å¤‡éŸ³é¢‘æå–...")


        # --- æ­¥éª¤ 2: (å¦‚æœéœ€è¦) ä¸²è¡Œæ‰§è¡Œæœ‰å†²çªçš„éŸ³é¢‘é¢„å¤„ç† ---
        audio_chunks = None
        if params["extract_subtitle"]:
            mime_type, _ = mimetypes.guess_type(local_file_path)
            if not (mime_type and mime_type.startswith("video")):
                _update_status(component="subtitle", status="SKIPPED", details="æºæ–‡ä»¶ä¸æ˜¯è§†é¢‘æ ¼å¼")
            else:
                try:
                    _update_status(component="subtitle", status="RUNNING", details="æ­£åœ¨é¢„å¤„ç†éŸ³é¢‘...", progress_val=30)
                    def sub_progress_callback(stage, details): _update_status(component="subtitle", details=details)
                    # è¿™ä¸ªå‡½æ•°åŒ…å«äº†ffmpegè°ƒç”¨å’Œåç»­çš„VADç­‰æ­¥éª¤ï¼Œå®ƒæ˜¯é˜»å¡çš„
                    audio_chunks = preprocess_audio_for_subtitles(local_file_path, temp_dir, sub_progress_callback, ai_models)
                except Exception as e:
                    log_system_event("error", f"éŸ³é¢‘é¢„å¤„ç†å¤±è´¥: {e}", in_worker=True)
                    _update_status(component="subtitle", status="FAILED", details=f"éŸ³é¢‘é¢„å¤„ç†å¤±è´¥: {e}", error_obj={"code": "AUDIO_EXTRACTION_FAILED", "message": str(e)})
                    audio_chunks = None
        else:
            _update_status(component="subtitle", status="SKIPPED", details="ç”¨æˆ·æœªè¯·æ±‚æå–")

        # --- æ­¥éª¤ 3: å¹¶è¡Œæ‰§è¡Œè§†é¢‘ä¸Šä¼ å’Œå­—å¹•AIå¤„ç† ---
        # æ­¤æ—¶éŸ³é¢‘é¢„å¤„ç†å·²ç»“æŸï¼Œè§†é¢‘æ–‡ä»¶å·²é‡Šæ”¾ï¼Œå¯ä»¥å®‰å…¨åœ°å¼€å§‹ä¸Šä¼ 
        if params["upload_video"]:
            # ã€æ ¸å¿ƒä¿®æ”¹ç‚¹ 2ã€‘åœ¨æ´¾å‘ä»»åŠ¡å‰ï¼Œç«‹å³æ›´æ–°çŠ¶æ€
            _update_status(component="video", status="RUNNING", details="å·²æ´¾å‘ä¸Šä¼ ä»»åŠ¡ (0%)...", progress_val=35)
            upload_queue.put({
                'task_id': task_id, 'component': 'video', 'local_file_path': str(local_file_path),
                'filename_for_link': filename, 'api_client_base_url': task_data['api_client_base_url'],
                'frp_server_addr': task_data['frp_server_addr']
            })
        else:
            _update_status(component="video", status="SKIPPED", details="ç”¨æˆ·æœªè¯·æ±‚ä¸Šä¼ ")
        
        # åŒæ—¶ï¼Œå¦‚æœéŸ³é¢‘å—å·²æˆåŠŸæå–ï¼Œåˆ™å¼€å§‹è¿›è¡Œè€—æ—¶çš„AIå¤„ç†
        if audio_chunks is not None:
            try:
                _update_status(component="subtitle", status="RUNNING", details="AIå¤„ç†ä¸­...", progress_val=40)
                def sub_progress_callback(stage, details): _update_status(component="subtitle", details=details)
                srt_content = run_subtitle_extraction_pipeline(subtitle_config, audio_chunks, sub_progress_callback)
                
                if not srt_content: raise RuntimeError("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å­—å¹•å†…å®¹ã€‚")
                
                srt_base64 = base64.b64encode(srt_content.encode('utf-8')).decode('utf-8')
                _update_status(component="subtitle", output={"contentBase64": srt_base64})
                
                if params["upload_subtitle"]:
                    _update_status(component="subtitle", status="RUNNING", details="å·²æ´¾å‘å­—å¹•ä¸Šä¼ ä»»åŠ¡")
                    srt_filename = local_file_path.stem + ".srt"
                    srt_path = temp_dir / srt_filename
                    with open(srt_path, "w", encoding="utf-8") as f: f.write(srt_content)
                    upload_queue.put({
                       'task_id': task_id, 'component': 'subtitle', 'local_file_path': str(srt_path),
                       'filename_for_link': srt_filename, 'api_client_base_url': task_data['api_client_base_url'],
                       'frp_server_addr': task_data['frp_server_addr']
                    })
                else:
                    _update_status(component="subtitle", status="SUCCESS", details="æå–æˆåŠŸ")
            except Exception as e:
                _update_status(component="subtitle", status="FAILED", details=str(e), error_obj={"code": "TRANSCRIPTION_FAILED", "message": str(e)})

        log_system_event("info", f"åª’ä½“å¤„ç†è¿›ç¨‹ä¸ºä»»åŠ¡ {task_id} çš„ä¸»è¦å·¥ä½œå·²å®Œæˆã€‚", in_worker=True)

    except Exception as e:
        log_system_event("error", f"å¤„ç†ä»»åŠ¡ {task_id} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
        result_queue.put({'type': 'task_result', 'task_id': task_id, 'status': 'FAILED', 'result': {}, 'error': {"code": "FATAL_ERROR", "message": str(e)}})
    finally:
        log_system_event("info", f"åª’ä½“å¤„ç†è¿›ç¨‹ {task_id} å®Œæˆï¼Œä¸å†æ¸…ç†ä¸»ä»»åŠ¡ç›®å½•ã€‚", in_worker=True)

# =============================================================================
# --- ç¬¬ 8 æ­¥: å¤šè¿›ç¨‹ Worker ä¸ä¸»ç¨‹åº ---
# =============================================================================


# =============================================================================
# --- (æ–°å¢æ–¹æ³• 1/3) uploader_process_loop [å…¨æ–°] ---
# =============================================================================

# è¿™ä¸ªæ–°å‡½æ•°ä¸“é—¨è´Ÿè´£æ–‡ä»¶ä¸Šä¼ ï¼Œè¿è¡Œåœ¨ç‹¬ç«‹çš„è¿›ç¨‹ä¸­
def uploader_process_loop(upload_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue):
    """
    ã€ä¿®æ”¹åã€‘ä¸€ä¸ªä¸“ç”¨çš„ä¸Šä¼ å·¥ä½œè¿›ç¨‹ï¼Œå¢åŠ äº†è¯¦ç»†çš„è¿›åº¦æŠ¥å‘ŠåŠŸèƒ½ï¼Œä»¥è§£å†³é™é»˜æŒ‚èµ·é—®é¢˜ã€‚
    """
    log_system_event("info", "ä¸Šä¼ ä¸“ç”¨å·¥ä½œè¿›ç¨‹å·²å¯åŠ¨ã€‚", in_worker=True)
    
    while True:
        try:
            upload_task_data = upload_queue.get()
            if upload_task_data is None:
                break

            task_id = upload_task_data['task_id']
            component = upload_task_data['component']
            local_file_path_str = upload_task_data['local_file_path']

            # --- å®šä¹‰ä¸€ä¸ªçŠ¶æ€æ›´æ–°çš„å¿«æ·æ–¹å¼ ---
            def _update_uploader_status(status, details=None, output=None, error_obj=None):
                payload_results = {component: {}}
                if status: payload_results[component]['status'] = status
                if details: payload_results[component]['details'] = details
                if output: payload_results[component]['output'] = output
                if error_obj: payload_results[component]['error'] = error_obj
                result_queue.put({'type': 'status_update', 'task_id': task_id, 'payload': {'results': payload_results}})

            try:
                filename_for_link = upload_task_data['filename_for_link']
                api_client_base_url = upload_task_data['api_client_base_url']
                frp_server_addr = upload_task_data['frp_server_addr']
                
                # api_client = MixFileCLIClient(base_url=api_client_base_url)
                api_client = MixFileCLIClient(base_url=api_client_base_url, proxies=GLOBAL_PROXY_SETTINGS)
                log_system_event("info", f"[ä¸Šä¼ è¿›ç¨‹] [{component}] å¼€å§‹å¤„ç†ä¸Šä¼ ä»»åŠ¡ï¼Œæ–‡ä»¶: {local_file_path_str}", in_worker=True)
                
                _update_uploader_status("RUNNING", details="æ­£åœ¨ä¸Šä¼  (0%)...")

                # ã€æ ¸å¿ƒä¿®æ”¹ã€‘å®šä¹‰å¹¶ä½¿ç”¨è¿›åº¦å›è°ƒ
                # ä½¿ç”¨ nonlocal å…³é”®å­—æ¥ä¿®æ”¹å¤–éƒ¨ä½œç”¨åŸŸçš„å˜é‡
                last_reported_percent = -1
                def progress_callback(bytes_uploaded, total_bytes):
                    nonlocal last_reported_percent
                    if total_bytes > 0:
                        percent = int((bytes_uploaded / total_bytes) * 100)
                        # æ¯ 5% æ›´æ–°ä¸€æ¬¡çŠ¶æ€ï¼Œé¿å…æ¶ˆæ¯è¿‡å¤šï¼ŒåŒæ—¶ç¡®ä¿ 100% ä¼šè¢«æŠ¥å‘Š
                        if percent > last_reported_percent and (percent % 5 == 0 or percent == 100):
                            _update_uploader_status("RUNNING", details=f"æ­£åœ¨ä¸Šä¼  ({percent}%)")
                            last_reported_percent = percent
                
                log_system_event("info", f"[ä¸Šä¼ è¿›ç¨‹] [{component}] å‡†å¤‡è°ƒç”¨ api_client.upload_file (å¸¦è¿›åº¦å›è°ƒ)", in_worker=True)
                # å°†å›è°ƒå‡½æ•°ä¼ é€’ç»™ upload_file æ–¹æ³•
                response = api_client.upload_file(local_file_path_str, progress_callback=progress_callback)
                log_system_event("info", f"[ä¸Šä¼ è¿›ç¨‹] [{component}] api_client.upload_file è°ƒç”¨å·²è¿”å›", in_worker=True)
                
                if isinstance(response, requests.Response):
                    if response.ok:
                        share_code = response.text.strip()
                        share_url = f"http://{frp_server_addr}:{MIXFILE_REMOTE_PORT}/api/download/{quote(filename_for_link)}?s={share_code}"
                        _update_uploader_status("SUCCESS", details="ä¸Šä¼ æˆåŠŸ", output={"shareUrl": share_url})
                        log_system_event("info", f"âœ… [ä¸Šä¼ è¿›ç¨‹] [{component}] ä¸Šä¼ æˆåŠŸã€‚", in_worker=True)
                    else:
                        error_msg = f"HTTP {response.status_code}: {response.text}"
                        raise RuntimeError(error_msg)
                elif isinstance(response, tuple):
                    error_msg = f"è¯·æ±‚å¤±è´¥ (çŠ¶æ€ç  {response[0]}): {response[1]}"
                    raise RuntimeError(error_msg)
                else:
                    error_msg = f"MixFileå®¢æˆ·ç«¯è¿”å›äº†æœªçŸ¥ç±»å‹: {type(response)}"
                    raise RuntimeError(error_msg)

            except Exception as e:
                log_system_event("error", f"âŒ [ä¸Šä¼ è¿›ç¨‹] [{component}] ä¸Šä¼ ä»»åŠ¡å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
                _update_uploader_status("FAILED", details=f"ä¸Šä¼ å¤±è´¥: {e}", error_obj={"code": "UPLOAD_FAILED", "message": str(e)})

            finally:
                # æ–‡ä»¶æ¸…ç†é€»è¾‘ä¿æŒä¸å˜
                try:
                    local_file_path_obj = Path(local_file_path_str)
                    if local_file_path_obj.exists():
                        local_file_path_obj.unlink()
                        log_system_event("info", f"[ä¸Šä¼ è¿›ç¨‹] [{component}] å·²æ¸…ç†æ–‡ä»¶: {local_file_path_str}", in_worker=True)
                except OSError as e:
                    log_system_event("warning", f"[ä¸Šä¼ è¿›ç¨‹] [{component}] æ¸…ç†æ–‡ä»¶ {local_file_path_str} å¤±è´¥: {e}", in_worker=True)

        except KeyboardInterrupt:
            break
        except Exception as e:
            log_system_event("critical", f"ä¸Šä¼ ä¸“ç”¨å·¥ä½œè¿›ç¨‹å‘ç”Ÿè‡´å‘½é”™è¯¯ï¼Œå¾ªç¯å¯èƒ½ç»ˆæ­¢: {e}", in_worker=True)
            
    log_system_event("info", "ä¸Šä¼ ä¸“ç”¨å·¥ä½œè¿›ç¨‹å·²å…³é—­ã€‚", in_worker=True)

def worker_process_loop(task_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue, upload_queue: multiprocessing.Queue):
    """
    åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å¾ªç¯ï¼Œè´Ÿè´£åª’ä½“å¤„ç†å¹¶å°†ä¸Šä¼ ä»»åŠ¡å¤–åŒ…ã€‚
    """
    log_system_event("info", "åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å·²å¯åŠ¨ã€‚", in_worker=True)
    ai_models = load_ai_models()
    # subtitle_config_global æ˜¯åœ¨mainå‡½æ•°ä¸­è§£å¯†çš„å…¨å±€å˜é‡ï¼Œå­è¿›ç¨‹å¯ä»¥ç›´æ¥è®¿é—®
    
    while True:
        try:
            task_data = task_queue.get()
            if task_data is None:
                break
            log_system_event("info", f"åª’ä½“å¤„ç†è¿›ç¨‹æ¥æ”¶åˆ°æ–°ä»»åŠ¡: {task_data['task_id']}", in_worker=True)
            process_unified_task(task_data, result_queue, upload_queue, subtitle_config_global, ai_models)
        except KeyboardInterrupt:
            break
        except Exception as e:
            log_system_event("critical", f"åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å¾ªç¯å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
    log_system_event("info", "åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å·²å…³é—­ã€‚", in_worker=True)

def result_processor_thread_loop(result_queue: multiprocessing.Queue):
    """
    ã€ä¿®æ”¹åã€‘ç»“æœå¤„ç†å™¨ï¼Œå¢åŠ äº†åœ¨ä»»åŠ¡è¾¾åˆ°æœ€ç»ˆçŠ¶æ€åï¼Œè´Ÿè´£æ¸…ç†ä»»åŠ¡ä¸´æ—¶ç›®å½•çš„é€»è¾‘ã€‚
    """
    log_system_event("info", "ç»“æœå¤„ç†çº¿ç¨‹å·²å¯åŠ¨ã€‚")
    while True:
        try:
            # ä½¿ç”¨é•¿è¶…æ—¶ä»¥é˜²é˜Ÿåˆ—é•¿æ—¶é—´ç©ºé—²
            result_data = result_queue.get(timeout=3600)
            task_id = result_data['task_id']

            with tasks_lock:
                if task_id not in tasks:
                    continue

                task = tasks[task_id]
                task['updatedAt'] = time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())
                
                data_type = result_data.get('type')
                
                # --- çŠ¶æ€æ›´æ–°é€»è¾‘ ---
                if data_type == 'status_update':
                    payload = result_data.get('payload', {})
                    if 'progress' in payload: task['progress'] = payload['progress']
                    if 'results' in payload:
                        for component, data in payload['results'].items():
                            if component in task['results']:
                                task['results'][component].update(data)
                    # åªè¦æœ‰æ›´æ–°ï¼Œå°±è®¤ä¸ºæ˜¯RUNNINGï¼ˆé™¤éå·²ç»æ˜¯æœ€ç»ˆçŠ¶æ€ï¼‰
                    if task['status'] not in ["SUCCESS", "FAILED", "PARTIAL_SUCCESS"]:
                         task['status'] = "RUNNING"

                # --- ä»»åŠ¡çº§ç»“æœï¼ˆé€šå¸¸æ˜¯ä¸¥é‡é”™è¯¯ï¼‰ ---
                elif data_type == 'task_result':
                    task['status'] = result_data.get('status', 'FAILED')
                    task['error'] = result_data.get('error')
                    task['progress'] = 100
                
                # --- æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¾¾åˆ°æœ€ç»ˆçŠ¶æ€ ---
                video_status = task['results']['video']['status']
                subtitle_status = task['results']['subtitle']['status']
                
                # æ£€æŸ¥æ‰€æœ‰å­ç»„ä»¶æ˜¯å¦éƒ½å·²è„±ç¦»â€œå¤„ç†ä¸­â€çš„çŠ¶æ€
                is_finished = "PENDING" not in (video_status, subtitle_status) and \
                              "RUNNING" not in (video_status, subtitle_status)

                # å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œå¹¶ä¸”å°šæœªè®¾ç½®æœ€ç»ˆçŠ¶æ€ï¼Œåˆ™è¿›è¡Œè¯„ä¼°å’Œæ¸…ç†
                if is_finished and task['status'] not in ["SUCCESS", "FAILED", "PARTIAL_SUCCESS"]:
                    log_system_event("info", f"ä»»åŠ¡ {task_id} æ‰€æœ‰ç»„ä»¶å·²å®Œæˆï¼Œæ­£åœ¨è¿›è¡Œæœ€ç»ˆçŠ¶æ€è¯„ä¼°ã€‚")
                    task['progress'] = 100
                    
                    # çŠ¶æ€æ¸…ç†ï¼šå¤„ç†å› è¿›ç¨‹å´©æºƒå¯¼è‡´çš„ "RUNNING" æ®‹ç•™çŠ¶æ€
                    for component in task['results']:
                        comp_data = task['results'][component]
                        if comp_data['status'] == 'RUNNING':
                            comp_data['status'] = 'FAILED'
                            comp_data['details'] = 'ç»„ä»¶å› æœªçŸ¥åŸå› æœªèƒ½å®Œæˆ'
                            comp_data['error'] = { 'code': 'WORKER_CRASHED', 'message': 'å¤„ç†æ­¤ç»„ä»¶çš„å·¥ä½œè¿›ç¨‹å¯èƒ½å·²æ„å¤–ç»ˆæ­¢ã€‚'}

                    # é‡æ–°è·å–æ¸…ç†åçš„çŠ¶æ€
                    final_video_status = task['results']['video']['status']
                    final_subtitle_status = task['results']['subtitle']['status']
                    
                    # å®šä¹‰å‚ä¸æœ€ç»ˆçŠ¶æ€è¯„ä¼°çš„çŠ¶æ€åˆ—è¡¨ (æ’é™¤ SKIPPED)
                    active_statuses = [s for s in (final_video_status, final_subtitle_status) if s != "SKIPPED"]
                    
                    if not active_statuses: # å¦‚æœæ‰€æœ‰ç»„ä»¶éƒ½è¢«è·³è¿‡
                        task['status'] = "SUCCESS"
                    elif all(s == "SUCCESS" for s in active_statuses):
                        task['status'] = "SUCCESS"
                    elif "FAILED" in active_statuses:
                        if "SUCCESS" in active_statuses:
                            task['status'] = "PARTIAL_SUCCESS"
                        else:
                            task['status'] = "FAILED"
                    else: # å…¶ä»–æƒ…å†µï¼Œä¾‹å¦‚å…¨æ˜¯SKIPPEDå’ŒSUCCESS
                        task['status'] = "SUCCESS"

                    log_system_event("info", f"ä»»åŠ¡ {task_id} æœ€ç»ˆçŠ¶æ€è¢«è®¾ç½®ä¸º: {task['status']}")

                    # ã€æ ¸å¿ƒæ–°å¢ã€‘åœ¨æ­¤å¤„æ‰§è¡Œæ¸…ç†æ“ä½œ
                    temp_dir_to_clean = Path(f"/kaggle/working/task_{task_id}")
                    if temp_dir_to_clean.exists():
                        log_system_event("info", f"ä»»åŠ¡ {task_id} å·²ç»“æŸï¼Œå‡†å¤‡æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir_to_clean}")
                        try:
                            shutil.rmtree(temp_dir_to_clean)
                            log_system_event("info", f"âœ… æˆåŠŸæ¸…ç†ä»»åŠ¡ {task_id} çš„ä¸´æ—¶ç›®å½•ã€‚")
                        except Exception as e:
                            log_system_event("error", f"âŒ æ¸…ç†ä»»åŠ¡ {task_id} çš„ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

        except QueueEmpty:
            # é˜Ÿåˆ—ä¸ºç©ºæ˜¯æ­£å¸¸æƒ…å†µï¼Œç»§ç»­å¾ªç¯
            continue
        except Exception as e:
            log_system_event("error", f"ç»“æœå¤„ç†çº¿ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

def main():
    #
    # ä¸»æ‰§è¡Œå‡½æ•°ï¼Œè´Ÿè´£åˆå§‹åŒ–å’Œå¯åŠ¨æ‰€æœ‰æœåŠ¡ã€‚
    #
    global api_client, subtitle_config_global, FRP_SERVER_ADDR
    global TASK_QUEUE, RESULT_QUEUE, UPLOAD_QUEUE # <--- æ–°å¢ UPLOAD_QUEUE

    try:
        # --- 1. å¯åŠ¨å‰å‡†å¤‡ ---
        log_system_event("info", "æœåŠ¡æ­£åœ¨å¯åŠ¨...")
        
        install_torch_cmd = (
            "pip uninstall -y torch torchvision torchaudio && "
            "pip install torch==2.3.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
        )
        log_system_event("info", "æ­£åœ¨å®‰è£…å…¼å®¹çš„ PyTorch ç‰ˆæœ¬...")
        install_proc = subprocess.run(install_torch_cmd, shell=True, capture_output=True, text=True)
        if install_proc.returncode != 0:
            log_system_event("error", f"PyTorch å®‰è£…å¤±è´¥ï¼\nStdout: {install_proc.stdout}\nStderr: {install_proc.stderr}")
            raise RuntimeError("æœªèƒ½å®‰è£…å…¼å®¹çš„ PyTorch ç‰ˆæœ¬ã€‚")
        log_system_event("info", "âœ… å…¼å®¹çš„ PyTorch å®‰è£…å®Œæˆã€‚")
        
        install_other_cmd = "pip install -q pydantic pydub faster-whisper@https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz denoiser google-generativeai requests psutil"
        log_system_event("info", "æ­£åœ¨å®‰è£…å…¶ä½™ä¾èµ–åº“...")
        subprocess.run(install_other_cmd, shell=True, check=True)
        log_system_event("info", "âœ… å…¶ä½™ä¾èµ–åº“å®‰è£…å®Œæˆã€‚")
        
        check_environment()
        
        # --- è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³• ---
        multiprocessing.set_start_method('fork', force=True)

        # --- 2. è§£å¯†é…ç½® ---
        frp_config = get_decrypted_config(ENCRYPTED_FRP_CONFIG, "FRP")
        subtitle_config_global = get_decrypted_config(ENCRYPTED_SUBTITLE_CONFIG, "Subtitle")
        FRP_SERVER_ADDR = frp_config['FRP_SERVER_ADDR']
        FRP_SERVER_PORT = frp_config['FRP_SERVER_PORT']
        FRP_TOKEN = frp_config['FRP_TOKEN']
        
        # --- 3. åˆå§‹åŒ– MixFile å®¢æˆ·ç«¯ ---
        api_client_base_url = f"http://127.0.0.1:{MIXFILE_LOCAL_PORT}"
        api_client = MixFileCLIClient(base_url=api_client_base_url)

        # --- 4. å¯åŠ¨ MixFileCLI æœåŠ¡ ---
        log_system_event("info", "æ­£åœ¨åˆ›å»º MixFileCLI config.yml...")
        with open("config.yml", "w") as f: f.write(mixfile_config_yaml)
        log_system_event("info", "æ­£åœ¨ä¸‹è½½å¹¶å¯åŠ¨ MixFileCLI...")
        if not os.path.exists("mixfile-cli.jar"):
            run_command("wget -q --show-progress https://raw.githubusercontent.com/jornhand/kagglewithmixfile/refs/heads/main/mixfile-cli-2.0.1.jar -O mixfile-cli.jar").wait()
        run_command("java -jar mixfile-cli.jar", "mixfile.log")
        if not wait_for_port(MIXFILE_LOCAL_PORT):
            raise RuntimeError("MixFileCLI æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ mixfile.logã€‚")

        # ã€æ ¸å¿ƒæ–°å¢ã€‘--- 4.5 å¯åŠ¨å¹¶é…ç½®æœ€ä¼˜ä»£ç† ---
        v2ray_sub_url = subtitle_config_global.get("V2RAY_SUB_URL")
        if v2ray_sub_url:
            proxy_manager = ProxyManager(sub_url=v2ray_sub_url, mixfile_base_url=api_client_base_url)
            proxy_manager.setup_best_proxy()
        else:
            log_system_event("info", "æœªåœ¨é…ç½®ä¸­æ‰¾åˆ° V2RAY_SUB_URLï¼Œä¸å¯ç”¨ä»£ç†åŠŸèƒ½ã€‚")

        # --- 5. åˆå§‹åŒ–å¤šè¿›ç¨‹é˜Ÿåˆ—å’Œå·¥ä½œè¿›ç¨‹ ---
        TASK_QUEUE = multiprocessing.Queue()
        RESULT_QUEUE = multiprocessing.Queue()
        UPLOAD_QUEUE = multiprocessing.Queue() # æ–°å¢ä¸Šä¼ é˜Ÿåˆ—

        # å¯åŠ¨åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹
        media_worker = multiprocessing.Process(
            target=worker_process_loop,
            args=(TASK_QUEUE, RESULT_QUEUE, UPLOAD_QUEUE), 
            daemon=False
        )
        media_worker.start()
        
        # å¯åŠ¨ä¸“ç”¨çš„ä¸Šä¼ å·¥ä½œè¿›ç¨‹
        uploader_worker = multiprocessing.Process(
            target=uploader_process_loop,
            args=(UPLOAD_QUEUE, RESULT_QUEUE),
            daemon=False
        )
        uploader_worker.start()

        # å¯åŠ¨ç»“æœå¤„ç†çº¿ç¨‹
        result_thread = threading.Thread(
            target=result_processor_thread_loop,
            args=(RESULT_QUEUE,),
            daemon=True
        )
        result_thread.start()

        # --- 6. å¯åŠ¨ Flask API æœåŠ¡ ---
        def run_flask_app():
            app.run(host='0.0.0.0', port=FLASK_API_LOCAL_PORT, debug=False, use_reloader=False)
        log_system_event("info", "æ­£åœ¨åå°å¯åŠ¨ Flask API æœåŠ¡...")
        threading.Thread(target=run_flask_app, daemon=True).start()
        if not wait_for_port(FLASK_API_LOCAL_PORT):
             raise RuntimeError("Flask æœåŠ¡å¯åŠ¨å¤±è´¥ã€‚")

        # --- 7. å¯åŠ¨ frpc å®¢æˆ·ç«¯ ---
        log_system_event("info", "æ­£åœ¨å‡†å¤‡ frpc å®¢æˆ·ç«¯...")
        if not os.path.exists("/kaggle/working/frpc"):
            run_command("wget -q https://github.com/fatedier/frp/releases/download/v0.54.0/frp_0.54.0_linux_amd64.tar.gz && tar -zxvf frp_0.54.0_linux_amd64.tar.gz && mv frp_0.54.0_linux_amd64/frpc /kaggle/working/frpc && chmod +x /kaggle/working/frpc").wait()
        
        frpc_ini = f"""
[common]
server_addr = {FRP_SERVER_ADDR}
server_port = {FRP_SERVER_PORT}
token = {FRP_TOKEN}
log_file = /kaggle/working/frpc.log
[mixfile_webdav_{MIXFILE_REMOTE_PORT}]
type = tcp
local_ip = 127.0.0.1
local_port = {MIXFILE_LOCAL_PORT}
remote_port = {MIXFILE_REMOTE_PORT}
[flask_api_{FLASK_API_REMOTE_PORT}]
type = tcp
local_ip = 127.0.0.1
local_port = {FLASK_API_LOCAL_PORT}
remote_port = {FLASK_API_REMOTE_PORT}
"""
        with open('frpc.ini', 'w') as f: f.write(frpc_ini)
        run_command('/kaggle/working/frpc -c ./frpc.ini')
        log_system_event("info", "frpc å®¢æˆ·ç«¯å·²åœ¨åå°å¯åŠ¨ã€‚")
        time.sleep(3)
        
        # --- 8. æœ€ç»ˆçŠ¶æ€æŠ¥å‘Šä¸ä¿æ´» ---
        public_api_base_url = f"http://{FRP_SERVER_ADDR}:{FLASK_API_REMOTE_PORT}"
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æœåŠ¡å‡å·²æˆåŠŸå¯åŠ¨ï¼æ‚¨çš„ç»Ÿä¸€ API å·²ä¸Šçº¿ã€‚")
        print(f"  -> API å…¥å£ (POST) : {public_api_base_url}/api/upload")
        print(f"  -> ä»»åŠ¡çŠ¶æ€ (GET)  : {public_api_base_url}/api/tasks/<task_id>")
        print(f"  -> å¥åº·æ£€æŸ¥ (GET)  : {public_api_base_url}/killer_status_frp")
        print(f"  -> è¿œç¨‹å…³é—­ (GET)  : {public_api_base_url}/force_shutdown_notebook?token={KILLER_API_SHUTDOWN_TOKEN}")
        print("="*60)
        
        while True:
            time.sleep(300)
            all_workers_alive = media_worker.is_alive() and uploader_worker.is_alive()
            worker_status_msg = 'å…¨éƒ¨å­˜æ´»' if all_workers_alive else 'å­˜åœ¨å·²é€€å‡ºçš„å·¥ä½œè¿›ç¨‹'
            log_system_event("info", f"æœåŠ¡æŒç»­è¿è¡Œä¸­... ({time.ctime()}) å·¥ä½œè¿›ç¨‹çŠ¶æ€: {worker_status_msg}")
            
    except (DecryptionError, RuntimeError, ValueError) as e:
        log_system_event("critical", f"ç¨‹åºå¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
        log_system_event("critical", "æœåŠ¡æ— æ³•å¯åŠ¨ï¼Œç¨‹åºå°†ç»ˆæ­¢ã€‚")
    except KeyboardInterrupt:
        log_system_event("info", "æœåŠ¡å·²æ‰‹åŠ¨åœæ­¢ã€‚")
        if 'TASK_QUEUE' in globals() and TASK_QUEUE is not None:
            TASK_QUEUE.put(None)
        if 'UPLOAD_QUEUE' in globals() and UPLOAD_QUEUE is not None:
            UPLOAD_QUEUE.put(None)
    except Exception as e:
        log_system_event("critical", f"å‘ç”ŸæœªçŸ¥çš„è‡´å‘½é”™è¯¯: {e}")

if __name__ == '__main__':
    main()