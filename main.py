# =============================================================================
#         Kaggle æŒä¹…åŒ–åª’ä½“å¤„ç†æœåŠ¡ - ä¸»åº”ç”¨ v2.1 (æ–¹æ¡ˆ2 - å¤šè¿›ç¨‹éš”ç¦»ç‰ˆ)
# =============================================================================
#
# åŠŸèƒ½:
#   - å¯åŠ¨ MixFileCLI ä½œä¸ºæ–‡ä»¶å­˜å‚¨åç«¯ã€‚
#   - æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ Flask APIï¼Œç”¨äº:
#     1. ä» URL ä¸‹è½½æ–‡ä»¶å¹¶ä¸Šä¼ åˆ° MixFileã€‚
#     2. ä»è§†é¢‘ URL ä¸­æå–é«˜è´¨é‡å­—å¹• (å¯é€‰)ã€‚
#     3. å°†åŸå§‹è§†é¢‘å’Œç”Ÿæˆçš„å­—å¹•çµæ´»åœ°ä¸Šä¼ åˆ° MixFileã€‚
#   - é€šè¿‡ FRP å°†å†…éƒ¨æœåŠ¡å®‰å…¨åœ°æš´éœ²åˆ°å…¬ç½‘ã€‚
#   - æ‰€æœ‰æ•æ„Ÿé…ç½® (FRP, APIå¯†é’¥) å‡é€šè¿‡åŠ å¯†æ–¹å¼ç®¡ç†ã€‚
#   - å®ç°é«˜å®¹é”™çš„ä»»åŠ¡å¤„ç†é€»è¾‘ï¼Œå­ä»»åŠ¡å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹ã€‚
#   - **V2.1 å˜æ›´: é‡‡ç”¨å¤šè¿›ç¨‹æ¶æ„ï¼Œå°†é‡é‡çº§çš„åª’ä½“å¤„ç†ä»»åŠ¡æ”¾åˆ°ç‹¬ç«‹çš„å­è¿›ç¨‹
#     ä¸­æ‰§è¡Œï¼Œä»¥è§£å†³ä¸»è¿›ç¨‹ä¸­å› æœåŠ¡ç¯å¢ƒå†²çªå¯¼è‡´çš„AIæ¨¡å‹åŠ è½½å¤±è´¥é—®é¢˜ã€‚**
#
# =============================================================================

# --- æ ¸å¿ƒ Python åº“ ---
import os
import subprocess
import threading
import time
import uuid
import socket
import json
import base64
import hashlib
import signal
import sys
import shutil
import mimetypes
from pathlib import Path
from datetime import datetime, timedelta
from urllib.parse import urljoin, quote, unquote
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed, TimeoutError


# --- å¤šè¿›ç¨‹ä¸é˜Ÿåˆ— ---
import multiprocessing
from queue import Empty as QueueEmpty

# --- Web æ¡†æ¶ä¸ HTTP å®¢æˆ·ç«¯ ---
from flask import Flask, request, jsonify, Response
import requests





# =============================================================================
# --- (æ–°å¢æ¨¡å—) ç¬¬ 3.5 æ­¥: V2Ray ä»£ç†ç®¡ç†å™¨ ---
# =============================================================================
import base64
import json
import random
import string
from urllib.parse import urlparse, parse_qs




# --- åŠ å¯†ä¸å¯†é’¥ç®¡ç† ---
# å°è¯•å¯¼å…¥å…³é”®åº“ï¼Œå¦‚æœå¤±è´¥åˆ™åœ¨åç»­æ£€æŸ¥ä¸­å¤„ç†
try:
    from kaggle_secrets import UserSecretsClient
    from cryptography.fernet import Fernet
except ImportError:
    UserSecretsClient = None
    Fernet = None

# --- å­—å¹•æå–ç›¸å…³åº“ (å°†åœ¨ check_environment ä¸­éªŒè¯) ---
try:
    import torch
    import torchaudio
    from pydub import AudioSegment
    import pydantic
except ImportError as e:
    # å…è®¸å¯åŠ¨æ—¶å¯¼å…¥å¤±è´¥ï¼Œåç»­ç¯å¢ƒæ£€æŸ¥ä¼šæ•è·
    print(f"[è­¦å‘Š] é¢„åŠ è½½éƒ¨åˆ†åº“å¤±è´¥: {e}ã€‚å°†åœ¨ç¯å¢ƒæ£€æŸ¥ä¸­ç¡®è®¤ã€‚")
    torch = None
    torchaudio = None
    AudioSegment = None
    pydantic = None

# =============================================================================
# --- ç¬¬ 1 æ­¥: å…¨å±€é…ç½® ---
# =============================================================================

# -- A. MixFileCLI é…ç½® --
mixfile_config_yaml = """
uploader: "çº¿è·¯A2"
upload_task: 20
upload_retry: 10
download_task: 5
chunk_size: 1024
port: 4719
host: "0.0.0.0"
password: ""
webdav_path: "data.mix_dav"
history: "history.mix_list"
"""

# -- B. åŠ å¯†çš„æœåŠ¡é…ç½® --
# !!! é‡è¦ !!!
# åœ¨è¿™é‡Œç²˜è´´ä½ ä» encrypt_util.py å·¥å…·ä¸­è·å¾—çš„åŠ å¯†é…ç½®å­—ç¬¦ä¸²
# ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²ç”¨äº FRP (åå‘ä»£ç†)
ENCRYPTED_FRP_CONFIG = "gAAAAABovr0nLhcvqidm1uzqWvVToI9StZZAAa9vgtqoxlXSKuMosiX8yEzEIwPrDghud3lGK0zRB0pLsc14E8Bvbk8OxSQVL_NT31PHrXsz6ulKWJYo6fNU3ycOuBoeSWLkou4-HP33INu5SyWH5rnLqvGW8KuFcZtV9qtM-A3zX-dDzDMdhBo5NeS_u5yHpCKy478r029Y"

# ç¬¬äºŒä¸ªå­—ç¬¦ä¸²ç”¨äºå­—å¹•æœåŠ¡ (Gemini API å¯†é’¥ç­‰)
ENCRYPTED_SUBTITLE_CONFIG = "gAAAAABovr1QY48Fe_jcPlBAYeYyO8Dx2woxKVlQcukRmw-Uh0mVw63YT0OBVdmXMaF5Ksj-zTZSdjAvJtpht7sjx7WtjMSXeYzB_dzqrb43X7zqGN_T-blYp285D33QUnr0tfCVR4j9BuTP7KquQNm7yrFhV0zOXOnherA2OoP9htPWJ3Z45igpAj_LxYh4cOXRtPMCGdRza5vnBjlgWyuQVTg6p-1kYfiPi9lGzwgGoqoSsfJkK69TFWUM9IRrrIIz56urbQOsvYP7wtM1pqJ0ReKKRSWq6A4mXVJKMJj0Su1tuQTPSX8otiM3Y2EYwFy6J7JXdVs6zbTSRFzzQ05JXSpEl0eZ62G91ZnBtmpOArrUvqh0JsWfWVUk3D2-ijxfBnVcJ3xu1y6slW2jPtS7J2PBI_lgi939-aTecrKYl-yPj7XbxeAvSZecl_tJj8l0p1XMDmFWNol_A4UeRxI8exMCQd9QCXyTT07kIo2CF5r32FF0yPIlCdKmw-0petX2wnYxnkrG" # ç¤ºä¾‹ï¼Œè¯·æ›¿æ¢

# -- C. æœ¬åœ°æœåŠ¡ä¸ç«¯å£é…ç½® --
MIXFILE_LOCAL_PORT = 4719
FLASK_API_LOCAL_PORT = 5000
MIXFILE_REMOTE_PORT = 20000  # æ˜ å°„åˆ°å…¬ç½‘çš„ MixFile ç«¯å£
FLASK_API_REMOTE_PORT = 20001  # æ˜ å°„åˆ°å…¬ç½‘çš„ Flask API ç«¯å£

# -- D. Killer API & è¿›ç¨‹ç®¡ç†é…ç½® --
KILLER_API_SHUTDOWN_TOKEN = "123456" # !!! å¼ºçƒˆå»ºè®®ä¿®æ”¹ !!!
PROCESS_KEYWORDS_TO_KILL = ["java", "frpc", "python -c"] # æ–°å¢pythonå­è¿›ç¨‹å…³é”®è¯
EXCLUDE_KEYWORDS_FROM_KILL = ["jupyter", "kernel", "ipykernel", "conda", "grep"]

# -- E. å­—å¹•æå–æµç¨‹é…ç½® --
# è¿™äº›å€¼æœªæ¥ä¹Ÿå¯ä»¥åŠ å…¥åˆ°åŠ å¯†é…ç½®ä¸­
SUBTITLE_CHUNK_DURATION_MS = 10 * 60 * 1000  # 10åˆ†é’Ÿ
SUBTITLE_BATCH_SIZE = 40
SUBTITLE_CONCURRENT_REQUESTS = 8
SUBTITLE_REQUESTS_PER_MINUTE = 8

# =============================================================================
# --- ç¬¬ 2 æ­¥: è§£å¯†ä¸é…ç½®åŠ è½½æ¨¡å— ---
# =============================================================================

class DecryptionError(Exception):
    # è‡ªå®šä¹‰å¼‚å¸¸ï¼Œç”¨äºè¡¨ç¤ºè§£å¯†è¿‡ç¨‹ä¸­çš„ç‰¹å®šå¤±è´¥ã€‚
    pass

def _get_decryption_cipher():
    #
    # è¾…åŠ©å‡½æ•°ï¼šä» Kaggle Secrets è·å–å¯†ç å¹¶ç”Ÿæˆ Fernet è§£å¯†å™¨ã€‚
    # è¿™æ˜¯ä¸€ä¸ªå…±äº«é€»è¾‘ï¼Œè¢«å…¶ä»–è§£å¯†å‡½æ•°è°ƒç”¨ã€‚
    #
    if not UserSecretsClient or not Fernet:
        raise DecryptionError("å…³é”®åº“ (kaggle_secrets, cryptography) æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥ã€‚")
    
    print("ğŸ” æ­£åœ¨ä» Kaggle Secrets è·å–è§£å¯†å¯†é’¥ 'FRP_DECRYPTION_KEY'...")
    try:
        secrets = UserSecretsClient()
        decryption_key_password = secrets.get_secret("FRP_DECRYPTION_KEY")
        if not decryption_key_password:
             raise ValueError("Kaggle Secret 'FRP_DECRYPTION_KEY' çš„å€¼ä¸ºç©ºã€‚")
    except Exception as e:
        print(f"âŒ æ— æ³•ä» Kaggle Secrets è·å–å¯†é’¥ï¼è¯·ç¡®ä¿ä½ å·²æ­£ç¡®è®¾ç½®äº†åä¸º 'FRP_DECRYPTION_KEY' çš„ Secretã€‚")
        raise DecryptionError(f"è·å– Kaggle Secret å¤±è´¥: {e}") from e

    # ä½¿ç”¨ SHA256 ä»ç”¨æˆ·å¯†ç æ´¾ç”Ÿä¸€ä¸ªç¡®å®šæ€§çš„ã€å®‰å…¨çš„32å­—èŠ‚å¯†é’¥
    key = base64.urlsafe_b64encode(hashlib.sha256(decryption_key_password.encode()).digest())
    return Fernet(key)

def get_decrypted_config(encrypted_string: str, config_name: str) -> dict:
    #
    # é€šç”¨çš„è§£å¯†å‡½æ•°ï¼Œç”¨äºè§£å¯†ä»»ä½•ç»™å®šçš„åŠ å¯†å­—ç¬¦ä¸²ã€‚
    #
    # Args:
    #     encrypted_string (str): ä» encrypt_util.py è·å–çš„ Base64 ç¼–ç çš„åŠ å¯†å­—ç¬¦ä¸²ã€‚
    #     config_name (str): é…ç½®çš„åç§°ï¼Œç”¨äºæ—¥å¿—è¾“å‡º (ä¾‹å¦‚ "FRP", "Subtitle")ã€‚
    #
    # Returns:
    #     dict: è§£å¯†å¹¶è§£æåçš„é…ç½®å­—å…¸ã€‚
    # 
    # Raises:
    #     DecryptionError: å¦‚æœè§£å¯†è¿‡ç¨‹çš„ä»»ä½•æ­¥éª¤å¤±è´¥ã€‚
    #
    print(f"ğŸ”‘ æ­£åœ¨å‡†å¤‡è§£å¯† {config_name} é…ç½®...")
    try:
        if "PASTE_YOUR_ENCRYPTED" in encrypted_string:
             raise ValueError(f"æ£€æµ‹åˆ°å ä½ç¬¦åŠ å¯†å­—ç¬¦ä¸²ï¼Œè¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®é…ç½®ã€‚")
        
        cipher = _get_decryption_cipher()
        decrypted_bytes = cipher.decrypt(encrypted_string.encode('utf-8'))
        config = json.loads(decrypted_bytes.decode('utf-8'))
        print(f"âœ… {config_name} é…ç½®è§£å¯†æˆåŠŸï¼")
        return config
    except ValueError as e:
        print(f"âŒ è§£å¯† {config_name} é…ç½®å¤±è´¥: {e}")
        raise DecryptionError(f"é…ç½®å­—ç¬¦ä¸²æ— æ•ˆ: {e}")
    except Exception as e:
        print(f"âŒ è§£å¯† {config_name} é…ç½®å¤±è´¥ï¼")
        print("   è¿™é€šå¸¸æ„å‘³ç€ä½ çš„ Kaggle Secret (å¯†ç ) ä¸åŠ å¯†æ—¶ä½¿ç”¨çš„å¯†ç ä¸åŒ¹é…ï¼Œ")
        print("   æˆ–è€…åŠ å¯†å­—ç¬¦ä¸²å·²æŸåã€‚")
        raise DecryptionError(f"è§£å¯†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}") from e
        
# =============================================================================
# --- ç¬¬ 3 æ­¥: ç¯å¢ƒæ£€æŸ¥ã€è¾…åŠ©å‡½æ•°ä¸ Killer API ---
# =============================================================================

def check_environment():
    #
    # æ£€æŸ¥è¿è¡Œæ‰€éœ€çš„æ‰€æœ‰å…³é”®ä¾èµ–å’Œåº“æ˜¯å¦éƒ½å·²æ­£ç¡®å®‰è£…ã€‚
    # å¦‚æœç¼ºå°‘å…³é”®ç»„ä»¶ï¼Œåˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œä½¿ç¨‹åºæå‰å¤±è´¥ã€‚
    #
    print("\n" + "="*60)
    print("ğŸ”¬ æ­£åœ¨æ‰§è¡Œç¯å¢ƒä¾èµ–æ£€æŸ¥...")
    print("="*60)
    
    # æ£€æŸ¥åŸºç¡€åº“
    if not UserSecretsClient or not Fernet:
        raise RuntimeError("å…³é”®å®‰å…¨åº“ 'kaggle_secrets' æˆ– 'cryptography' æœªæ‰¾åˆ°ã€‚æ— æ³•ç»§ç»­ã€‚")
    print("âœ… å®‰å…¨åº“ (kaggle_secrets, cryptography) - æ­£å¸¸")
    
    # æ£€æŸ¥å­—å¹•æå–æ ¸å¿ƒåº“
    critical_subtitle_libs = {
        "torch": torch,
        "torchaudio": torchaudio,
        "pydub": AudioSegment,
        "pydantic": pydantic
    }
    for name, lib in critical_subtitle_libs.items():
        if not lib:
            raise RuntimeError(f"å­—å¹•æå–æ ¸å¿ƒåº“ '{name}' æœªæ‰¾åˆ°æˆ–å¯¼å…¥å¤±è´¥ã€‚è¯·æ£€æŸ¥ Kaggle ç¯å¢ƒã€‚")
        print(f"âœ… å­—å¹•åº“ ({name}) - æ­£å¸¸")

    # æ£€æŸ¥å‘½ä»¤è¡Œå·¥å…·
    if not shutil.which("ffmpeg"):
        raise RuntimeError("'ffmpeg' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¿™æ˜¯æå–éŸ³é¢‘æ‰€å¿…éœ€çš„ã€‚")
    print("âœ… å‘½ä»¤è¡Œå·¥å…· (ffmpeg) - æ­£å¸¸")
    
    if not shutil.which("java"):
        raise RuntimeError("'java' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¿™æ˜¯è¿è¡Œ MixFileCLI æ‰€å¿…éœ€çš„ã€‚")
    print("âœ… å‘½ä»¤è¡Œå·¥å…· (java) - æ­£å¸¸")

    print("\nâœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼æ‰€æœ‰å…³é”®ä¾èµ–å‡å·²å°±ç»ªã€‚\n")


def log_system_event(level: str, message: str, in_worker=False):
    # ä¸€ä¸ªç®€å•çš„å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—è®°å½•å™¨ã€‚
    # æ–°å¢ in_worker å‚æ•°ä»¥åŒºåˆ†æ—¥å¿—æ¥æº
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
    worker_tag = "[WORKER]" if in_worker else "[SYSTEM]"
    prefix = f"[{timestamp} {worker_tag} {level.upper()}]"
    print(f"{prefix} {message}", flush=True)


def ms_to_srt_time(ms: int) -> str:
    # å°†æ¯«ç§’è½¬æ¢ä¸º SRT å­—å¹•æ–‡ä»¶çš„æ—¶é—´æ ¼å¼ (HH:MM:SS,ms)ã€‚
    try:
        ms = int(ms)
    except (ValueError, TypeError):
        ms = 0
    td = timedelta(milliseconds=ms)
    hours, remainder = divmod(td.seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    milliseconds = td.microseconds // 1000
    return f"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}"


def run_command(command: str, log_file: str = None) -> subprocess.Popen:
    #
    # åœ¨åå°ä»¥éé˜»å¡æ–¹å¼æ‰§è¡Œä¸€ä¸ª shell å‘½ä»¤ã€‚
    #
    # Args:
    #     command (str): è¦æ‰§è¡Œçš„å‘½ä»¤ã€‚
    #     log_file (str, optional): å°† stdout å’Œ stderr é‡å®šå‘åˆ°çš„æ—¥å¿—æ–‡ä»¶åã€‚
    #
    # Returns:
    #     subprocess.Popen: æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹å¯¹è±¡ã€‚
    #
    log_system_event("info", f"æ‰§è¡Œå‘½ä»¤: {command}")
    stdout_dest = None
    stderr_dest = None
    if log_file:
        log_handle = open(log_file, 'w', encoding='utf-8')
        stdout_dest = log_handle
        stderr_dest = log_handle
    
    # ä½¿ç”¨ Popen ä»¥éé˜»å¡æ–¹å¼å¯åŠ¨è¿›ç¨‹
    process = subprocess.Popen(
        command,
        shell=True,
        stdout=stdout_dest,
        stderr=stderr_dest,
        encoding='utf-8',
        errors='ignore'
    )
    return process


def wait_for_port(port: int, host: str = '127.0.0.1', timeout: float = 60.0) -> bool:
    #
    # ç­‰å¾…æŒ‡å®šçš„æœ¬åœ°ç«¯å£å˜ä¸ºå¯ç”¨çŠ¶æ€ã€‚
    #
    # Args:
    #     port (int): è¦æ£€æŸ¥çš„ç«¯å£å·ã€‚
    #     host (str, optional): ç›‘å¬çš„ä¸»æœºåœ°å€ã€‚é»˜è®¤ä¸º '127.0.0.1'ã€‚
    #     timeout (float, optional): æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ã€‚é»˜è®¤ä¸º 60.0ã€‚
    #
    # Returns:
    #     bool: å¦‚æœç«¯å£åœ¨è¶…æ—¶å‰å˜ä¸ºå¯ç”¨ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    #
    log_system_event("info", f"æ­£åœ¨ç­‰å¾…ç«¯å£ {host}:{port} å¯åŠ¨...")
    start_time = time.perf_counter()
    while True:
        try:
            with socket.create_connection((host, port), timeout=1):
                log_system_event("info", f"âœ… ç«¯å£ {port} å·²æˆåŠŸå¯åŠ¨ï¼")
                return True
        except (socket.timeout, ConnectionRefusedError):
            time.sleep(1)
            if time.perf_counter() - start_time >= timeout:
                log_system_event("error", f"âŒ ç­‰å¾…ç«¯å£ {port} è¶…æ—¶ ({timeout}ç§’)ã€‚")
                return False

# --- Killer API é€»è¾‘ (ç”¨äºè¿œç¨‹å…³é—­) ---

def _find_and_kill_targeted_processes(signal_to_send=signal.SIGTERM):
    # æŸ¥æ‰¾å¹¶ç»ˆæ­¢æ­¤è„šæœ¬å¯åŠ¨çš„æ‰€æœ‰å…³é”®å­è¿›ç¨‹ (java, frpc, python -c ...)ã€‚
    killed_pids_info = []
    log_system_event("info", f"æŸ¥æ‰¾å¹¶å°è¯•ç»ˆæ­¢ä¸ '{PROCESS_KEYWORDS_TO_KILL}' ç›¸å…³çš„è¿›ç¨‹...")
    
    current_kernel_pid = os.getpid()
    pids_to_target = set()

    try:
        # ä½¿ç”¨ ps å‘½ä»¤è·å–æ‰€æœ‰è¿›ç¨‹ä¿¡æ¯
        ps_output = subprocess.check_output(['ps', '-eo', 'pid,ppid,args'], text=True)
        
        for line in ps_output.splitlines()[1:]:
            parts = line.strip().split(None, 2)
            if len(parts) < 3: continue
            
            try:
                pid = int(parts[0])
                command_line = parts[2]
                command_lower = command_line.lower()
            except (ValueError, IndexError):
                continue
            
            # æ’é™¤å½“å‰å†…æ ¸è¿›ç¨‹å’Œç³»ç»Ÿå…³é”®è¿›ç¨‹
            if pid == current_kernel_pid: continue
            is_excluded = any(ex_kw.lower() in command_lower for ex_kw in EXCLUDE_KEYWORDS_FROM_KILL)
            if is_excluded: continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡è¿›ç¨‹
            is_target = any(target_kw.lower() in command_lower for target_kw in PROCESS_KEYWORDS_TO_KILL)
            if is_target:
                pids_to_target.add(pid)
                log_system_event("debug", f"  å‘ç°ç›®æ ‡: PID={pid}, CMD='{command_line}'")

        if not pids_to_target:
            log_system_event("info", "æœªæ‰¾åˆ°éœ€è¦ç»ˆæ­¢çš„ç‰¹å®šåº”ç”¨å­è¿›ç¨‹ã€‚")
        else:
            log_system_event("info", f"å‡†å¤‡å‘ PIDs {list(pids_to_target)} å‘é€ä¿¡å· {signal_to_send}...")
            for pid_to_kill in pids_to_target:
                try:
                    os.kill(pid_to_kill, signal_to_send)
                    log_system_event("info", f"    å·²å‘ PID {pid_to_kill} å‘é€ä¿¡å· {signal_to_send}.")
                    killed_pids_info.append({"pid": pid_to_kill, "status": "signal_sent"})
                except ProcessLookupError:
                    killed_pids_info.append({"pid": pid_to_kill, "status": "not_found"})
                except Exception as e:
                    killed_pids_info.append({"pid": pid_to_kill, "status": f"error_{type(e).__name__}"})
                    
    except Exception as e:
        log_system_event("error", f"æŸ¥æ‰¾æˆ–ç»ˆæ­¢å­è¿›ç¨‹æ—¶å‡ºé”™: {e}")
    
    return killed_pids_info


def _shutdown_notebook_kernel_immediately():
    #
    # é€šè¿‡å‘é€ SIGKILL ä¿¡å·å¼ºåˆ¶ã€ç«‹å³åœ°å…³é—­å½“å‰ Kaggle Notebook å†…æ ¸ã€‚
    # è¿™æ˜¯æœ€ç»ˆçš„è‡ªæ¯æŒ‡ä»¤ã€‚
    #
    log_system_event("critical", "å‡†å¤‡é€šè¿‡ SIGKILL ä¿¡å·å¼ºåˆ¶å…³é—­å½“å‰ Kaggle Notebook Kernel...")
    sys.stdout.flush()
    sys.stderr.flush()
    time.sleep(0.5) # ç•™å‡ºä¸€ç‚¹æ—¶é—´è®©æ—¥å¿—åˆ·æ–°
    
    # SIGKILL (ä¿¡å· 9) æ˜¯ä¸€ä¸ªæ— æ³•è¢«æ•è·æˆ–å¿½ç•¥çš„ä¿¡å·ï¼Œæ¯” os._exit æ›´ä¸ºå¼ºåˆ¶ã€‚
    os.kill(os.getpid(), signal.SIGKILL)






# =============================================================================
# --- (æ–°å¢æ¨¡å—) ç¬¬ 3.5 æ­¥: V2Ray ä»£ç†ç®¡ç†å™¨ ---
# =============================================================================

class ProxyManager:
    """
    ã€é‡æ„ç‰ˆã€‘æŒ‰éœ€ä¸ºä¸Šä¼ ä»»åŠ¡å¯»æ‰¾æœ€ä¼˜ä»£ç†çº¿è·¯çš„å·¥å…·ç±»ã€‚
    å®ƒè¢«è®¾è®¡ä¸ºåœ¨å·¥ä½œè¿›ç¨‹ä¸­æŒ‰éœ€å®ä¾‹åŒ–å’Œä½¿ç”¨ï¼Œä¸ç®¡ç†å¸¸é©»åå°è¿›ç¨‹ã€‚
    """
    def __init__(self, sub_url=None):
        self.sub_url = sub_url
        self.xray_path = Path("/kaggle/working/xray")
        self.config_path = Path("/kaggle/working/xray_config.json")
        self.local_socks_port = 10808
        self.geoip_path = Path("/kaggle/working/geoip.dat")
        self.geosite_path = Path("/kaggle/working/geosite.dat")

    def _ensure_xray_assets(self):
        """ç¡®ä¿ Xray æ ¸å¿ƒåŠæ•°æ®åº“æ–‡ä»¶å·²ä¸‹è½½ã€‚"""
        if self.xray_path.exists() and self.geoip_path.exists() and self.geosite_path.exists():
            return

        log_system_event("info", "æ£€æµ‹åˆ°Xrayç»„ä»¶ç¼ºå¤±ï¼Œæ­£åœ¨ä¸‹è½½...", in_worker=True)
        xray_url = "https://github.com/XTLS/Xray-core/releases/download/v1.8.10/Xray-linux-64.zip"
        geoip_url = "https://github.com/Loyalsoldier/v2ray-rules-dat/releases/latest/download/geoip.dat"
        geosite_url = "https://github.com/Loyalsoldier/v2ray-rules-dat/releases/latest/download/geosite.dat"
        zip_path = Path("/kaggle/working/xray.zip")
        
        try:
            if not self.xray_path.exists():
                log_system_event("info", "  -> ä¸‹è½½ Xray core...", in_worker=True)
                run_command(f"wget -q -O {zip_path} {xray_url}").wait()
                run_command(f"unzip -o {zip_path} xray -d /kaggle/working/").wait()
                self.xray_path.chmod(0o755)
                zip_path.unlink()
            if not self.geoip_path.exists():
                log_system_event("info", "  -> ä¸‹è½½ geoip.dat...", in_worker=True)
                run_command(f"wget -q -O {self.geoip_path} {geoip_url}").wait()
            if not self.geosite_path.exists():
                log_system_event("info", "  -> ä¸‹è½½ geosite.dat...", in_worker=True)
                run_command(f"wget -q -O {self.geosite_path} {geosite_url}").wait()
            log_system_event("info", "âœ… Xrayç»„ä»¶ä¸‹è½½å®Œæˆã€‚", in_worker=True)
        except Exception as e:
            raise RuntimeError(f"ä¸‹è½½ Xray ç»„ä»¶å¤±è´¥: {e}")

    def _fetch_and_parse_subscription(self):
        """è·å–å¹¶è§£æè®¢é˜…é“¾æ¥ï¼Œè¿”å›èŠ‚ç‚¹é“¾æ¥åˆ—è¡¨ã€‚"""
        if not self.sub_url:
            return []
        log_system_event("info", f"æ­£åœ¨ä» {self.sub_url[:30]}... è·å–è®¢é˜…...", in_worker=True)
        try:
            response = requests.get(self.sub_url, timeout=20)
            response.raise_for_status()
            decoded_content = base64.b64decode(response.content).decode('utf-8')
            return decoded_content.strip().split('\n')
        except Exception as e:
            log_system_event("error", f"è·å–æˆ–è§£æè®¢é˜…å¤±è´¥: {e}", in_worker=True)
            return []

    def _generate_node_config(self, node_url):
        """æ ¹æ®èŠ‚ç‚¹URLç”ŸæˆXrayçš„JSONé…ç½®ï¼ˆåŒ…å«è·¯ç”±ï¼‰ã€‚"""
        try:
            parsed_url = urlparse(node_url)
            node_name_raw = parsed_url.fragment
            node_name = unquote(node_name_raw) if node_name_raw else "Unnamed Node"
            
            # --- æ­¥éª¤ 1: æ„å»º Outbounds ---
            #
            # Proxy Outbound (tag: "proxy")
            protocol = parsed_url.scheme
            proxy_outbound = {"protocol": protocol, "settings": {}, "tag": "proxy"}
            
            if protocol == "vmess":
                try:
                    decoded_vmess_str = base64.b64decode(parsed_url.netloc).decode('utf-8')
                    decoded_vmess = json.loads(decoded_vmess_str)
                except Exception:
                     return None, f"Invalid VMess format for node {node_name}"

                proxy_outbound["settings"]["vnext"] = [{
                    "address": decoded_vmess["add"],
                    "port": int(decoded_vmess["port"]),
                    "users": [{"id": decoded_vmess["id"], "alterId": int(decoded_vmess["aid"]), "security": decoded_vmess.get("scy", "auto")}]
                }]
                stream_settings = {"network": decoded_vmess.get("net", "tcp")}
                if stream_settings["network"] == "ws":
                    ws_settings = {"path": decoded_vmess.get("path", "/")}
                    host = decoded_vmess.get("host")
                    if host:
                        ws_settings["headers"] = {"Host": host}
                    stream_settings["wsSettings"] = ws_settings
                if decoded_vmess.get("tls", "") == "tls":
                     stream_settings["security"] = "tls"
                     stream_settings["tlsSettings"] = {"serverName": decoded_vmess.get("sni", decoded_vmess.get("host", decoded_vmess["add"]))}
                proxy_outbound["streamSettings"] = stream_settings

            elif protocol == "vless":
                qs = parse_qs(parsed_url.query)
                user_obj = { "id": parsed_url.username, "encryption": "none", "flow": qs.get("flow", [None])[0], "alterId": 0, "security": "auto" }
                if user_obj["flow"] is None: del user_obj["flow"]
                proxy_outbound["settings"]["vnext"] = [{"address": parsed_url.hostname, "port": int(parsed_url.port), "users": [user_obj]}]
                stream_settings = {"network": qs.get("type", ["tcp"])[0]}
                if stream_settings["network"] == "ws":
                    ws_path = unquote(qs.get("path", ["/"])[0])
                    ws_host = unquote(qs.get("host", [""])[0])
                    ws_settings = {"path": ws_path}
                    if ws_host: ws_settings["headers"] = {"Host": ws_host}
                    stream_settings["wsSettings"] = ws_settings
                if qs.get("security", ["none"])[0] == "tls":
                    stream_settings["security"] = "tls"
                    tls_sni = unquote(qs.get("sni", [""])[0]) or unquote(qs.get("host", [""])[0]) or parsed_url.hostname
                    fp = qs.get("fp", ["random"])[0]
                    tls_settings = {"serverName": tls_sni, "fingerprint": fp, "allowInsecure": False, "show": False}
                    alpn = qs.get("alpn")
                    if alpn: tls_settings["alpn"] = [val for val in alpn[0].split(',') if val]
                    stream_settings["tlsSettings"] = tls_settings
                proxy_outbound["streamSettings"] = stream_settings
            else:
                return None, f"Unsupported protocol: {protocol}"

            # Direct Outbound (tag: "direct")
            direct_outbound = {"protocol": "freedom", "settings": {}, "tag": "direct"}
            
            # Block Outbound (tag: "block") - for ads, etc.
            block_outbound = {"protocol": "blackhole", "settings": {}, "tag": "block"}

            # --- æ­¥éª¤ 2: æ„å»ºæœ€ç»ˆé…ç½® ---
            config = {
                "log": {"loglevel": "warning"},
                "dns": { "servers": ["8.8.8.8", "1.1.1.1", "localhost"] },
                "inbounds": [{
                    "port": self.local_socks_port,
                    "protocol": "socks",
                    "listen": "127.0.0.1",
                    "settings": {"auth": "noauth", "udp": True},
                    "sniffing": { "enabled": True, "destOverride": ["http", "tls"] }
                }],
                "outbounds": [
                    proxy_outbound,
                    direct_outbound,
                    block_outbound
                ],
                "routing": {
                    "domainStrategy": "AsIs",
                    "rules": [
                        { "type": "field", "ip": ["geoip:private"], "outboundTag": "direct" },
                        { "type": "field", "domain": ["geosite:category-ads-all"], "outboundTag": "block" },
                    ]
                }
            }
            return config, node_name
        except Exception as e:
            import traceback
            traceback.print_exc()
            return None, f"Error parsing node '{node_name}': {e}"

    def _test_upload_speed(self, test_upload_url, proxies=None):
        """
        ã€çŠ¶æ€éš”ç¦»ä¿®å¤ç‰ˆã€‘æµ‹è¯•ä¸Šä¼ é€Ÿåº¦çš„æ ¸å¿ƒå‡½æ•°ã€‚
        å®ƒæ¥æ”¶ä¸€ä¸ªå®Œæ•´çš„ã€åŒ…å«å”¯ä¸€æ–‡ä»¶åçš„ URL è¿›è¡Œæµ‹è¯•ã€‚
        """
        try:
            # ä½¿ç”¨è¾ƒå°çš„æµ‹è¯•æ•°æ®ä»¥åŠ å¿«æµ‹é€Ÿè¿‡ç¨‹
            test_data_size = 5 * 1024 * 1024  # 1MB
            test_data = os.urandom(test_data_size)
            
            start_time = time.time()
            # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ã€å”¯ä¸€çš„ URL
            response = requests.put(test_upload_url, data=test_data, proxies=proxies, timeout=30)
            end_time = time.time()
            
            response.raise_for_status() # ç¡®ä¿ä¸Šä¼ æˆåŠŸ (è¿”å› 2xx çŠ¶æ€ç )

            # æ³¨æ„ï¼šæˆ‘ä»¬ä¸å†å°è¯•åˆ é™¤æµ‹é€Ÿæ–‡ä»¶ï¼Œå› ä¸ºå®ƒä»¬çš„æ–‡ä»¶åæ˜¯å”¯ä¸€çš„ï¼Œ
            # ç•™å­˜åœ¨æœåŠ¡å™¨ä¸Šä¸ä¼šå¯¹åç»­æ“ä½œäº§ç”Ÿå†²çªã€‚
            # MixFileCLI æœåŠ¡é‡å¯æˆ–æ‰‹åŠ¨æ¸…ç†å³å¯ã€‚

            duration = end_time - start_time
            if duration > 0:
                return (test_data_size / duration) / (1024 * 1024)  # MB/s
            else:
                # å¦‚æœæ—¶é—´è¿‡çŸ­ï¼Œç»™ä¸€ä¸ªæé«˜çš„é€Ÿåº¦å€¼ï¼Œè€Œä¸æ˜¯0ï¼Œä»¥è¡¨ç¤ºè¿æ¥éå¸¸å¿«
                return 999 
        except Exception as e:
            log_system_event("debug", f"æµ‹é€Ÿå¤±è´¥ (proxy: {bool(proxies)}): {type(e).__name__}", in_worker=True)
            return 0

    def get_best_proxy_for_upload(self, api_client_base_url):
        """
        ã€çŠ¶æ€éš”ç¦»æœ€ç»ˆç‰ˆã€‘æ‰§è¡Œå®Œæ•´çš„æŒ‰éœ€æµ‹é€Ÿæµç¨‹ã€‚
        ä¸ºæ¯ä¸€æ¬¡ç‹¬ç«‹çš„æµ‹é€Ÿï¼ˆåŒ…æ‹¬ç›´è¿ï¼‰éƒ½ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ä¸Šä¼ URLï¼Œç¡®ä¿æ— çŠ¶æ€å†²çªã€‚
        """
        log_system_event("info", "====== å¼€å§‹æŒ‰éœ€æµ‹é€Ÿ ======", in_worker=True)
        self._ensure_xray_assets()

        # --- å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€çš„æµ‹è¯•URL ---
        def generate_unique_test_url():
            random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))
            test_filename = f"speed_test_{random_suffix}.tmp"
            return urljoin(api_client_base_url, f"/api/upload/{quote(test_filename)}")
        print("æ­£åœ¨æµ‹è¯•ä¸‹è½½é€Ÿåº¦...")
        
        download_speed = wifi.download() / 1_000_000  # è½¬æ¢ä¸ºMbps
        print(f"ä¸‹è½½é€Ÿåº¦: {download_speed:.2f} Mbps")

        print("æ­£åœ¨æµ‹è¯•ä¸Šä¼ é€Ÿåº¦...")
        upload_speed = wifi.upload() / 1_000_000  # è½¬æ¢ä¸ºMbps
        print(f"ä¸Šä¼ é€Ÿåº¦: {upload_speed:.2f} Mbps")     
        # 1. æµ‹è¯•ç›´è¿é€Ÿåº¦
        direct_test_url = generate_unique_test_url()
        direct_speed = self._test_upload_speed(direct_test_url)
  
        log_system_event("info", f"  -> ç›´è¿é€Ÿåº¦: {direct_speed:.2f} MB/s", in_worker=True)

        best_node_config = None
        best_node_speed = direct_speed
        best_node_name = "Direct Connection"

        # 2. å¾ªç¯æµ‹è¯•æ‰€æœ‰ä»£ç†èŠ‚ç‚¹
        node_urls = self._fetch_and_parse_subscription()
        if not node_urls:
            log_system_event("warning", "æœªè·å–åˆ°ä»£ç†èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨ç›´è¿ã€‚", in_worker=True)
            log_system_event("info", "====== æµ‹é€Ÿç»“æŸ ======", in_worker=True)
            return None, None

        # é™åˆ¶æµ‹é€ŸèŠ‚ç‚¹æ•°é‡
        num_to_test = 3 if len(node_urls) > 3 else len(node_urls)
        nodes_to_test = random.sample(node_urls, num_to_test)
        log_system_event("info", f"éšæœºé€‰æ‹© {len(nodes_to_test)} ä¸ªèŠ‚ç‚¹è¿›è¡Œæµ‹é€Ÿ...", in_worker=True)

        for node_url in nodes_to_test:
            node_config, node_name = self._generate_node_config(node_url.strip())
            if not node_config: 
                log_system_event("warning", f"è§£æèŠ‚ç‚¹å¤±è´¥ï¼Œè·³è¿‡: {node_name}", in_worker=True)
                continue

            log_system_event("info", f"  -> æ­£åœ¨æµ‹è¯•èŠ‚ç‚¹: {node_name}...", in_worker=True)
            with open(self.config_path, 'w') as f:
                json.dump(node_config, f)
            
            process = run_command(f"{self.xray_path} -c {self.config_path}")
            if not wait_for_port(self.local_socks_port, host='127.0.0.1', timeout=10):
                log_system_event("warning", f"     èŠ‚ç‚¹ {node_name} å¯åŠ¨å¤±è´¥ã€‚", in_worker=True)
                process.terminate()
                process.wait()
                continue
            
            proxies = {'http': f'socks5h://127.0.0.1:{self.local_socks_port}', 'https': f'socks5h://127.0.0.1:{self.local_socks_port}'}
            
            # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¸ºæ¯ä¸ªä»£ç†èŠ‚ç‚¹ä¹Ÿç”Ÿæˆå”¯ä¸€çš„æµ‹è¯•URL
            node_test_url = generate_unique_test_url()
            node_speed = self._test_upload_speed(node_test_url, proxies=proxies)
            log_system_event("info", f"     èŠ‚ç‚¹ {node_name} é€Ÿåº¦: {node_speed:.2f} MB/s", in_worker=True)

            if node_speed > best_node_speed:
                best_node_speed = node_speed
                best_node_config = node_config
                best_node_name = node_name

            process.terminate()
            process.wait()
            time.sleep(1)

        log_system_event("info", "="*28, in_worker=True)
        log_system_event("info", f"  æœ€ä¼˜çº¿è·¯: {best_node_name}", in_worker=True)
        log_system_event("info", f"  æœ€é«˜é€Ÿåº¦: {best_node_speed:.2f} MB/s", in_worker=True)
        log_system_event("info", "="*28, in_worker=True)
        log_system_event("info", "====== æµ‹é€Ÿç»“æŸ ======", in_worker=True)

        if best_node_config:
            return {'http': f'socks5h://127.0.0.1:{self.local_socks_port}', 'https': f'socks5h://127.0.0.1:{self.local_socks_port}'}, best_node_config
        else:
            return None, None
# =============================================================================
# --- ç¬¬ 4 æ­¥: å­—å¹•æå–æ ¸å¿ƒæ¨¡å— (åœ¨å­è¿›ç¨‹ä¸­è°ƒç”¨) ---
# =============================================================================

# --- A. Pydantic æ•°æ®éªŒè¯æ¨¡å‹ ---
# ç”¨äºä¸¥æ ¼éªŒè¯ Gemini API è¿”å›çš„ JSON ç»“æ„ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚

class SubtitleLine(pydantic.BaseModel):
    start_ms: int
    end_ms: int | None = None
    text: str

class BatchTranscriptionResult(pydantic.BaseModel):
    subtitles: list[SubtitleLine]

def load_ai_models():
    """
    åœ¨å·¥ä½œè¿›ç¨‹å¯åŠ¨æ—¶é¢„åŠ è½½æ‰€æœ‰éœ€è¦çš„ AI æ¨¡å‹ã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«å·²åŠ è½½æ¨¡å‹çš„å­—å…¸ã€‚
    """
    log_system_event("info", "å·¥ä½œè¿›ç¨‹æ­£åœ¨é¢„åŠ è½½ AI æ¨¡å‹...", in_worker=True)
    loaded_models = {
        "denoiser": None
    }
    
    # 1. å°è¯•åŠ è½½ AI é™å™ªæ¨¡å‹
    try:
        from denoiser import pretrained
        log_system_event("info", "æ­£åœ¨åŠ è½½ AI é™å™ªæ¨¡å‹ (denoiser)...", in_worker=True)
        denoiser_model = pretrained.dns64().cuda()
        loaded_models["denoiser"] = denoiser_model
        log_system_event("info", "âœ… AI é™å™ªæ¨¡å‹åŠ è½½æˆåŠŸã€‚", in_worker=True)
    except Exception as e:
        log_system_event("warning", f"é¢„åŠ è½½ AI é™å™ªæ¨¡å‹å¤±è´¥ï¼Œé™å™ªåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚é”™è¯¯: {e}", in_worker=True)

    # 2. VAD æ¨¡å‹è¯´æ˜
    # faster-whisper çš„ VAD åŠŸèƒ½ (get_speech_timestamps) æ˜¯ä¸€ä¸ªè½»é‡çº§å‡½æ•°ï¼Œ
    # ä¸éœ€è¦åƒé™å™ªæ¨¡å‹é‚£æ ·è¿›è¡Œé‡é‡çº§çš„é¢„åŠ è½½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡ŒåªåŠ è½½é™å™ªæ¨¡å‹ã€‚
    
    return loaded_models

# --- B. åŠ¨æ€æç¤ºè¯ä¸æ–‡ä»¶å¤„ç† ---

def get_dynamic_prompts(api_url: str) -> tuple[str, str]:
    #
    # ä»æŒ‡å®šçš„ API è·å–åŠ¨æ€æç¤ºè¯ã€‚å¦‚æœå¤±è´¥ï¼Œåˆ™è¿”å›ç¡¬ç¼–ç çš„å¤‡ç”¨æç¤ºè¯ã€‚
    #
    log_system_event("info", "æ­£åœ¨å°è¯•ä» API è·å–åŠ¨æ€æç¤ºè¯...", in_worker=True)
    try:
        response = requests.get(api_url, timeout=10)
        response.raise_for_status()
        prompts = response.json()
        if "system_instruction" in prompts and "prompt_for_task" in prompts:
            log_system_event("info", "âœ… æˆåŠŸä» API è·å–åŠ¨æ€æç¤ºè¯ã€‚", in_worker=True)
            return prompts['system_instruction'], prompts['prompt_for_task']
        else:
            raise ValueError("API å“åº”ä¸­ç¼ºå°‘å¿…è¦çš„é”®ã€‚")
    except Exception as e:
        log_system_event("warning", f"è·å–åŠ¨æ€æç¤ºè¯å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨å¤‡ç”¨æç¤ºè¯ã€‚", in_worker=True)
        # --- å¤‡ç”¨æç¤ºè¯ (Fallback Prompts) ---
        fallback_system = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­—å¹•ç¿»è¯‘æ¨¡å‹ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æä¾›çš„ä»»ä½•è¯­è¨€çš„éŸ³é¢‘å†…å®¹ç¿»è¯‘æˆ**ç®€ä½“ä¸­æ–‡**ã€‚"
            "åœ¨ä½ çš„æ‰€æœ‰è¾“å‡ºä¸­ï¼Œ`text` å­—æ®µçš„å€¼**å¿…é¡»æ˜¯ç®€ä½“ä¸­æ–‡**ã€‚"
            "**ç»å¯¹ç¦æ­¢**åœ¨ `text` å­—æ®µä¸­è¿”å›ä»»ä½•åŸå§‹è¯­è¨€ï¼ˆå¦‚æ—¥è¯­ï¼‰çš„æ–‡æœ¬ã€‚"
        )
        fallback_task = (
            "æˆ‘å°†ä¸ºä½ æä¾›ä¸€ç³»åˆ—éŸ³é¢‘ç‰‡æ®µå’Œå®ƒä»¬åœ¨è§†é¢‘ä¸­çš„ç»å¯¹æ—¶é—´ `[AUDIO_INFO] start_ms --> end_ms`ã€‚\n"
            "è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n"
            "1.  **å¬å–éŸ³é¢‘**å¹¶ç†è§£å…¶å†…å®¹ã€‚\n"
            "2.  **åˆ›å»ºå†…éƒ¨æ—¶é—´è½´**: å°†**ç¿»è¯‘æˆä¸­æ–‡å**çš„æ–‡æœ¬ï¼Œæ ¹æ®è¯­éŸ³åœé¡¿åˆ†å‰²æˆæ›´çŸ­çš„å­—å¹•è¡Œï¼Œå¹¶ä¸ºæ¯ä¸€è¡Œä¼°ç®—ä¸€ä¸ªç²¾ç¡®çš„ `start_ms` å’Œ `end_ms`ã€‚\n"
            "3.  **æ ¼å¼åŒ–è¾“å‡º**: ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªæ ¼å¼å®Œå…¨æ­£ç¡®çš„ JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½• markdown æ ‡è®°ã€‚ç»“æ„å¦‚ä¸‹ï¼Œå…¶ä¸­ `text` å­—æ®µçš„å€¼å¿…é¡»æ˜¯ä½ ç¿»è¯‘åçš„**ç®€ä½“ä¸­æ–‡**ï¼š\n"
            '{\n'
            '  "subtitles": [\n'
            '    { "start_ms": 12345, "end_ms": 13456, "text": "è¿™æ˜¯ç¿»è¯‘åçš„ç¬¬ä¸€å¥å­—å¹•" },\n'
            '    { "start_ms": 13600, "text": "è¿™æ˜¯ç¬¬äºŒå¥" }\n'
            '  ]\n'
            '}\n'
        )
        return fallback_system, fallback_task


def read_and_encode_file_base64(filepath: str) -> str | None:
    # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è¿”å› Base64 ç¼–ç çš„å­—ç¬¦ä¸²ã€‚
    try:
        with open(filepath, "rb") as f:
            binary_data = f.read()
        return base64.b64encode(binary_data).decode('utf-8')
    except Exception as e:
        log_system_event("error", f"æ— æ³•è¯»å–æˆ–ç¼–ç æ–‡ä»¶: {filepath}. é”™è¯¯: {e}", in_worker=True)
        return None

# --- C. éŸ³é¢‘é¢„å¤„ç†ç®¡é“ ---

def extract_audio_with_ffmpeg(
    video_path: Path,
    temp_dir: Path,
    update_status_callback: callable
) -> Path:
    """
    ã€æ–°å¢ã€‘åªè´Ÿè´£ä»è§†é¢‘ä¸­æå–éŸ³é¢‘çš„å‡½æ•°ã€‚
    è¿™ä¸ªè¿‡ç¨‹ä¼šé”å®šè§†é¢‘æ–‡ä»¶ï¼Œå®Œæˆåå³é‡Šæ”¾ã€‚
    è¿”å›æå–å‡ºçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ã€‚
    """
    update_status_callback(stage="subtitle_extract_audio", details="æ­£åœ¨ä»è§†é¢‘ä¸­æå–éŸ³é¢‘ (ffmpeg)...")
    raw_audio_path = temp_dir / "raw_audio.wav"
    try:
        command = [
            "ffmpeg", "-i", str(video_path),
            "-ac", "1", "-ar", "16000",
            "-vn", "-y", "-loglevel", "error", str(raw_audio_path)
        ]
        process = subprocess.run(command, check=True, capture_output=True, text=True)
        return raw_audio_path
    except subprocess.CalledProcessError as e:
        log_system_event("error", f"FFmpeg æå–éŸ³é¢‘å¤±è´¥ã€‚Stderr: {e.stderr}", in_worker=True)
        raise RuntimeError(f"FFmpeg æå–éŸ³é¢‘å¤±è´¥: {e.stderr}")

def preprocess_audio_for_subtitles(
    raw_audio_path: Path, # <--- è¾“å…¥å‚æ•°å˜æ›´
    temp_dir: Path,
    update_status_callback: callable,
    ai_models: dict
) -> list[dict]:
    """
    ã€ä¿®æ”¹åã€‘æ¥æ”¶ä¸€ä¸ªå·²æå–çš„éŸ³é¢‘æ–‡ä»¶ï¼Œæ‰§è¡Œåç»­çš„é™å™ªã€VADåˆ‡åˆ†ã€‚
    è¿™ä¸ªè¿‡ç¨‹ä¸å†æ¥è§¦åŸå§‹è§†é¢‘æ–‡ä»¶ã€‚
    """
    # 1. æ£€æŸ¥é™å™ªæ¨¡å‹
    denoiser_model = ai_models.get("denoiser")
    if denoiser_model:
        update_status_callback(stage="subtitle_denoise", details="AI é™å™ªæ¨¡å‹å·²åŠ è½½ï¼Œå‡†å¤‡å¤„ç†...")
        log_system_event("info", "å°†ä½¿ç”¨é¢„åŠ è½½çš„ AI é™å™ªæ¨¡å‹ã€‚", in_worker=True)
    else:
        log_system_event("warning", "é™å™ªæ¨¡å‹ä¸å¯ç”¨ï¼Œå°†è·³è¿‡é™å™ªæ­¥éª¤ã€‚", in_worker=True)

    # 2. åˆ†å—å¤„ç†éŸ³é¢‘ï¼šé™å™ª -> VAD
    update_status_callback(stage="subtitle_vad", details="æ­£åœ¨è¿›è¡ŒéŸ³é¢‘åˆ†å—ä¸è¯­éŸ³æ£€æµ‹...")
    try:
        original_audio = AudioSegment.from_wav(raw_audio_path)
    except Exception as e:
        raise RuntimeError(f"æ— æ³•åŠ è½½æå–å‡ºçš„éŸ³é¢‘æ–‡ä»¶ {raw_audio_path}: {e}")
        
    total_duration_ms = len(original_audio)
    chunk_files = []
    chunks_dir = temp_dir / "audio_chunks"
    chunks_dir.mkdir(exist_ok=True)
    
    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§éŸ³é¢‘å—
    for item in chunks_dir.iterdir():
        item.unlink()

    num_chunks = -(-total_duration_ms // SUBTITLE_CHUNK_DURATION_MS)

    for i in range(num_chunks):
        start_time_ms = i * SUBTITLE_CHUNK_DURATION_MS
        end_time_ms = min((i + 1) * SUBTITLE_CHUNK_DURATION_MS, total_duration_ms)
        
        log_system_event("info", f"æ­£åœ¨å¤„ç†éŸ³é¢‘æ€»å— {i+1}/{num_chunks}...", in_worker=True)
        
        audio_chunk = original_audio[start_time_ms:end_time_ms]
        temp_chunk_path = temp_dir / f"temp_chunk_{i}.wav"
        audio_chunk.export(temp_chunk_path, format="wav")
        
        processing_path = temp_chunk_path
        
        # 2.1 AI é™å™ª (å¦‚æœæ¨¡å‹åŠ è½½æˆåŠŸ)
        if denoiser_model:
            try:
                wav, sr = torchaudio.load(temp_chunk_path)
                wav = wav.cuda()
                with torch.no_grad():
                    denoised_wav = denoiser_model(wav[None])[0]
                denoised_chunk_path = temp_dir / f"denoised_chunk_{i}.wav"
                torchaudio.save(denoised_chunk_path, denoised_wav.cpu(), 16000)
                processing_path = denoised_chunk_path
            except Exception as e:
                log_system_event("warning", f"å½“å‰å—é™å™ªå¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹éŸ³é¢‘ã€‚é”™è¯¯: {e}", in_worker=True)
        
        # 2.2 VAD è¯­éŸ³æ£€æµ‹
        try:
            from faster_whisper.audio import decode_audio
            from faster_whisper.vad import VadOptions, get_speech_timestamps
            
            vad_parameters = { "threshold": 0.38, "min_speech_duration_ms": 150, "max_speech_duration_s": 15.0, "min_silence_duration_ms": 1500, "speech_pad_ms": 500 }
            sampling_rate = 16000
            audio_data = decode_audio(str(processing_path), sampling_rate=sampling_rate)
            speech_timestamps = get_speech_timestamps(audio_data, vad_options=VadOptions(**vad_parameters))
            
            # 2.3 æ ¹æ® VAD ç»“æœä»åŸå§‹éŸ³é¢‘ä¸­ç²¾ç¡®åˆ‡ç‰‡
            for speech in speech_timestamps:
                relative_start_ms = int(speech["start"] / sampling_rate * 1000)
                relative_end_ms = int(speech["end"] / sampling_rate * 1000)
                absolute_start_ms = start_time_ms + relative_start_ms
                absolute_end_ms = start_time_ms + relative_end_ms
                
                final_chunk = original_audio[absolute_start_ms:absolute_end_ms]
                final_chunk_path = chunks_dir / f"chunk_{absolute_start_ms}.wav"
                final_chunk.export(str(final_chunk_path), format="wav")
                chunk_files.append({ "path": str(final_chunk_path), "start_ms": absolute_start_ms, "end_ms": absolute_end_ms })
        except Exception as e:
            log_system_event("error", f"å½“å‰å— VAD å¤„ç†å¤±è´¥: {e}", in_worker=True)
    
    log_system_event("info", f"éŸ³é¢‘åˆ†å—å¤„ç†å®Œæˆï¼Œæ€»å…±åˆ‡åˆ†ä¸º {len(chunk_files)} ä¸ªæœ‰æ•ˆè¯­éŸ³ç‰‡æ®µã€‚", in_worker=True)
    return chunk_files

# --- D. AI äº¤äº’ä¸å¹¶å‘è°ƒåº¦ ---

def _process_subtitle_batch_with_ai(
    chunk_group: list[dict],
    group_index: int,
    api_key: str,
    gemini_endpoint_prefix: str,
    prompt_api_url: str
) -> list[dict]:
    """
    å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„éŸ³é¢‘å—ï¼Œè°ƒç”¨ Gemini API è·å–å­—å¹•ã€‚
    (ä¼˜åŒ–ç‰ˆ v2: æ˜ç¡®æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œä¸æ˜¯è¿”å›ç©ºåˆ—è¡¨ï¼Œä»¥ä¾¿ä¸Šå±‚æ•è·)
    """
    log_system_event("info", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å·²åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­å¯åŠ¨...", in_worker=True)
    
    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘: å°†æ•´ä¸ªå‡½æ•°é€»è¾‘åŒ…è£¹åœ¨ä¸€ä¸ªå¤§çš„ try...except å—ä¸­ã€‚
    # è¿™æ ·åšçš„ç›®çš„æ˜¯ï¼Œæ— è®ºå‘ç”Ÿä½•ç§ç±»å‹çš„é”™è¯¯ï¼ˆç½‘ç»œã€APIã€æ•°æ®è§£æç­‰ï¼‰ï¼Œ
    # éƒ½èƒ½ç¡®ä¿å®ƒè¢«é‡æ–°æŠ›å‡ºï¼Œè€Œä¸æ˜¯è¢«å‡½æ•°å†…éƒ¨æ¶ˆåŒ–æ‰ã€‚
    try:
        # 1. è·å–åŠ¨æ€æç¤ºè¯
        system_instruction, prompt_for_task = get_dynamic_prompts(prompt_api_url)

        # 2. æ„å»º REST API è¯·æ±‚ä½“ (payload)
        model_name = "gemini-2.5-flash"
        generate_url = f"{gemini_endpoint_prefix.rstrip('/')}/v1beta/models/{model_name}:generateContent"
        headers = {"x-goog-api-key": api_key, "Content-Type": "application/json"}
        
        parts = [{"text": prompt_for_task}]
        for chunk in chunk_group:
            encoded_data = read_and_encode_file_base64(chunk["path"])
            if not encoded_data:
                # å¦‚æœå•ä¸ªæ–‡ä»¶ç¼–ç å¤±è´¥ï¼Œè®°å½•è­¦å‘Šå¹¶è·³è¿‡ï¼Œä¸å½±å“æ•´ä¸ªæ‰¹æ¬¡
                log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ–‡ä»¶ {chunk['path']} ç¼–ç å¤±è´¥ï¼Œå°†è·³è¿‡æ­¤éŸ³é¢‘ç‰‡æ®µã€‚", in_worker=True)
                continue
            parts.append({"text": f"[AUDIO_INFO] {chunk['start_ms']} --> {chunk['end_ms']}"})
            parts.append({"inlineData": {"mime_type": "audio/wav", "data": encoded_data}})
        
        if len(parts) <= 1:
            # å¦‚æœæ•´ä¸ªæ‰¹æ¬¡çš„æ‰€æœ‰æ–‡ä»¶éƒ½æ— æ³•ç¼–ç ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡´å‘½é”™è¯¯ï¼Œåº”æŠ›å‡ºå¼‚å¸¸
            raise ValueError(f"æ‰¹æ¬¡ {group_index+1} ä¸­çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶å‡æ— æ³•ç¼–ç ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚")

        payload = {
            "contents": [{"parts": parts}],
            "systemInstruction": {"parts": [{"text": system_instruction}]},
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
            ],
            "generationConfig": {"responseMimeType": "application/json"}
        }

        # 3. å‘é€è¯·æ±‚å¹¶å®ç°å…¨é¢çš„é‡è¯•é€»è¾‘
        log_system_event("info", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] æ•°æ®å‡†å¤‡å®Œæ¯•ï¼Œæ­£åœ¨è°ƒç”¨ Gemini API...", in_worker=True)
        max_retries = 3
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                # ä½¿ç”¨ç¨é•¿çš„è¶…æ—¶æ—¶é—´ï¼Œå› ä¸ºå¯èƒ½åŒ…å«å¤§é‡éŸ³é¢‘æ•°æ®
                response = requests.post(generate_url, headers=headers, json=payload, timeout=300) 
                
                # å¯é‡è¯•çš„æœåŠ¡å™¨ç«¯é”™è¯¯
                if response.status_code in [429, 500, 503, 504]:
                    log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] é‡åˆ°å¯é‡è¯•çš„ API é”™è¯¯ (HTTP {response.status_code}, å°è¯• {attempt + 1}/{max_retries})ã€‚", in_worker=True)
                    # è§¦å‘ä¸‹ä¸€æ¬¡å¾ªç¯çš„é‡è¯•
                    raise requests.exceptions.HTTPError(f"Server error: {response.status_code}")

                # ä»»ä½•å…¶ä»–é2xxçš„çŠ¶æ€ç ï¼Œéƒ½è§†ä¸ºä¸å¯é‡è¯•çš„å¤±è´¥
                response.raise_for_status()
                
                # --- å¦‚æœè¯·æ±‚æˆåŠŸï¼Œåˆ™å¤„ç†å“åº” ---
                response_data = response.json()
                
                candidates = response_data.get("candidates")
                if not candidates:
                    raise ValueError(f"APIå“åº”ä¸­ç¼ºå°‘ 'candidates' å­—æ®µã€‚å“åº”: {response.text}")

                content = candidates[0].get("content")
                if not content:
                    finish_reason = candidates[0].get("finishReason", "æœªçŸ¥")
                    safety_ratings = candidates[0].get("safetyRatings", [])
                    raise ValueError(f"APIå“åº”å†…å®¹ä¸ºç©º(å¯èƒ½è¢«å®‰å…¨ç­–ç•¥æ‹¦æˆª)ã€‚åŸå› : {finish_reason}, å®‰å…¨è¯„çº§: {safety_ratings}")

                json_text = content.get("parts", [{}])[0].get("text")
                if json_text is None:
                    raise ValueError(f"APIå“åº”çš„ 'parts' ä¸­ç¼ºå°‘ 'text' å­—æ®µã€‚å“åº”: {response.text}")

                # ä½¿ç”¨ Pydantic éªŒè¯ JSON ç»“æ„
                parsed_result = BatchTranscriptionResult.model_validate_json(json_text)
                subtitles_count = len(parsed_result.subtitles)
                log_system_event("info", f"âœ… [å­—å¹•ä»»åŠ¡ {group_index+1}] æˆåŠŸï¼è·å¾— {subtitles_count} æ¡å­—å¹•ã€‚", in_worker=True)
                
                thread_local_srt_list = []
                for i, subtitle in enumerate(parsed_result.subtitles):
                    if subtitle.end_ms is None:
                        if i + 1 < subtitles_count:
                            subtitle.end_ms = parsed_result.subtitles[i+1].start_ms
                        else:
                            subtitle.end_ms = chunk_group[-1]['end_ms']
                    if subtitle.start_ms > subtitle.end_ms:
                        subtitle.end_ms = subtitle.start_ms + 250
                    
                    line = f"{ms_to_srt_time(subtitle.start_ms)} --> {ms_to_srt_time(subtitle.end_ms)}\n{subtitle.text.strip()}\n"
                    thread_local_srt_list.append({"start_ms": subtitle.start_ms, "srt_line": line})
                
                # æˆåŠŸå¤„ç†ï¼Œè¿”å›ç»“æœï¼Œå‡½æ•°ç»“æŸ
                return thread_local_srt_list

            except (requests.exceptions.RequestException, pydantic.ValidationError, ValueError) as e:
                # æ•è·æ‰€æœ‰é¢„æœŸçš„ã€å¯é‡è¯•çš„æˆ–ä¸å¯é‡è¯•çš„é”™è¯¯
                log_system_event("warning", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å°è¯• {attempt + 1}/{max_retries} å¤±è´¥ã€‚é”™è¯¯: {type(e).__name__}: {e}", in_worker=True)
                last_exception = e
                if attempt < max_retries - 1:
                    wait_time = 5 * (2 ** attempt)
                    log_system_event("info", f"{wait_time}ç§’åå°†è‡ªåŠ¨é‡è¯•...", in_worker=True)
                    time.sleep(wait_time)
                else:
                    # æ‰€æœ‰é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œè·³å‡ºå¾ªç¯
                    break
        
        # å¦‚æœå¾ªç¯ç»“æŸï¼ˆæ„å‘³ç€æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼‰ï¼Œåˆ™æŠ›å‡ºæœ€åçš„å¼‚å¸¸
        log_system_event("error", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥ã€‚", in_worker=True)
        raise RuntimeError(f"æ‰¹æ¬¡ {group_index+1} å¤±è´¥") from last_exception

    except Exception as e:
        # è¿™æ˜¯ä¸€ä¸ªæœ€ç»ˆçš„æ•è·å™¨ï¼Œç¡®ä¿ä»»ä½•æœªé¢„æ–™åˆ°çš„é”™è¯¯éƒ½ä¼šè¢«è®°å½•å¹¶é‡æ–°æŠ›å‡º
        log_system_event("critical", f"[å­—å¹•ä»»åŠ¡ {group_index+1}] å‘ç”Ÿæœªé¢„æ–™çš„ä¸¥é‡é”™è¯¯: {type(e).__name__}: {e}", in_worker=True)
        # ã€å…³é”®ã€‘é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ä¾¿ ProcessPoolExecutor èƒ½å¤Ÿæ•è·åˆ°è¿™ä¸ªä»»åŠ¡çš„å¤±è´¥çŠ¶æ€
        raise e

def run_subtitle_extraction_pipeline(subtitle_config: dict, chunk_files: list[dict], update_status_callback: callable) -> str:
    """
    ä¸»è°ƒåº¦å™¨ï¼Œè´Ÿè´£å¹¶å‘å¤„ç†æ‰€æœ‰éŸ³é¢‘æ‰¹æ¬¡å¹¶ç”Ÿæˆæœ€ç»ˆçš„ SRT å†…å®¹ã€‚
    (ä¼˜åŒ–ç‰ˆ v2: ä½¿ç”¨ ProcessPoolExecutor å®ç°è¿›ç¨‹éš”ç¦»ï¼Œé¿å…çº¿ç¨‹æ­»é”)
    """
    
    api_keys = subtitle_config.get('GEMINI_API_KEYS', [])
    prompt_api_url = subtitle_config.get('PROMPT_API_URL', '')
    gemini_endpoint_prefix = subtitle_config.get('GEMINI_API_ENDPOINT_PREFIX', '')
    
    if not all([api_keys, prompt_api_url, gemini_endpoint_prefix]):
        raise ValueError("å­—å¹•é…ç½®ä¸­ç¼ºå°‘ 'GEMINI_API_KEYS', 'PROMPT_API_URL', æˆ– 'GEMINI_API_ENDPOINT_PREFIX'ã€‚")
    
    total_chunks = len(chunk_files)
    if total_chunks == 0:
        log_system_event("warning", "æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆçš„è¯­éŸ³ç‰‡æ®µï¼Œæ— æ³•ç”Ÿæˆå­—å¹•ã€‚", in_worker=True)
        return ""

    chunk_groups = [chunk_files[i:i + SUBTITLE_BATCH_SIZE] for i in range(0, total_chunks, SUBTITLE_BATCH_SIZE)]
    num_groups = len(chunk_groups)
    log_system_event("info", f"å·²å°†è¯­éŸ³ç‰‡æ®µåˆ†ä¸º {num_groups} ä¸ªæ‰¹æ¬¡ï¼Œå‡†å¤‡é€šè¿‡å¤šè¿›ç¨‹å¹¶å‘å¤„ç†ã€‚", in_worker=True)
    update_status_callback(stage="subtitle_transcribing", details=f"å‡†å¤‡å¤„ç† {num_groups} ä¸ªå­—å¹•æ‰¹æ¬¡...")
    
    all_srt_blocks = []
    
    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘: ä½¿ç”¨ ProcessPoolExecutor æ›¿ä»£ ThreadPoolExecutor
    # max_workers å»ºè®®ä¸è¦è®¾ç½®å¾—è¿‡é«˜ï¼Œå› ä¸ºå®ƒä¼šæ¶ˆè€—æ›´å¤šå†…å­˜ã€‚4-8ä¸ªè¿›ç¨‹é€šå¸¸æ˜¯æ¯”è¾ƒå¥½çš„èµ·ç‚¹ã€‚
    # SUBTITLE_CONCURRENT_REQUESTS è¿™ä¸ªå…¨å±€å˜é‡çš„å€¼å¯ä»¥æ ¹æ®æƒ…å†µè°ƒæ•´ã€‚
    with ProcessPoolExecutor(max_workers=SUBTITLE_CONCURRENT_REQUESTS) as executor:
        # ä½¿ç”¨å­—å…¸å°† future æ˜ å°„å›å…¶ä»»åŠ¡ç´¢å¼•ï¼Œä¾¿äºæ—¥å¿—è®°å½•
        future_to_index = {}
        delay_between_submissions = 60.0 / SUBTITLE_REQUESTS_PER_MINUTE
        
        log_system_event("info", "å¼€å§‹å‘è¿›ç¨‹æ± æäº¤æ‰€æœ‰å­—å¹•ä»»åŠ¡...", in_worker=True)
        for i, group in enumerate(chunk_groups):
            api_key_for_process = api_keys[i % len(api_keys)]
            future = executor.submit(
                _process_subtitle_batch_with_ai,
                group,
                i, # ä»»åŠ¡ç´¢å¼•
                api_key_for_process,
                gemini_endpoint_prefix,
                prompt_api_url
            )
            future_to_index[future] = i + 1  # ä»»åŠ¡ç´¢å¼•ä»1å¼€å§‹ï¼Œæ›´ç¬¦åˆæ—¥å¿—ä¹ æƒ¯
            
            if i < num_groups - 1:
                time.sleep(delay_between_submissions)
        
        log_system_event("info", f"æ‰€æœ‰ {num_groups} ä¸ªä»»åŠ¡å‡å·²æäº¤ã€‚ç°åœ¨å¼€å§‹ç­‰å¾…å¹¶å¤„ç†è¿”å›ç»“æœ...", in_worker=True)
        
        completed_count = 0
        for future in as_completed(future_to_index):
            task_index = future_to_index[future]
            completed_count += 1
            
            try:
                # ã€æ–°å¢ã€‘ä¸ºè·å–ç»“æœè®¾ç½®ä¸€ä¸ªåˆç†çš„è¶…æ—¶æ—¶é—´ï¼ˆä¾‹å¦‚15åˆ†é’Ÿï¼‰
                # è¿™ä¸ªæ—¶é—´åº”è¯¥å¤§äºå•ä¸ªæ‰¹æ¬¡å¤„ç†çš„æœ€å¤§å¯èƒ½æ—¶é—´ï¼ˆåŒ…æ‹¬é‡è¯•ï¼‰
                # timeout = (å•ä¸ªè¯·æ±‚è¶…æ—¶ + é‡è¯•ç­‰å¾…) * é‡è¯•æ¬¡æ•°ï¼Œå†åŠ ä¸€äº›ä½™é‡
                # timeout = (360s + 5s*1 + 5s*2) * 3 = (375s) * 3 ~= 20åˆ†é’Ÿ
                result_timeout = 10 * 60 # 20åˆ†é’Ÿ
                
                # è·å–å·²å®Œæˆä»»åŠ¡çš„ç»“æœã€‚å¦‚æœå­è¿›ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼Œ.result()ä¼šé‡æ–°æŠ›å‡ºå®ƒ
                result = future.result(timeout=result_timeout)
                
                if result:
                    all_srt_blocks.extend(result)
                    log_system_event("info", f"âœ… å·²æˆåŠŸå¤„ç†å®Œå­—å¹•ä»»åŠ¡ {task_index} çš„ç»“æœã€‚", in_worker=True)
                else:
                    # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œé™¤é_process_subtitle_batch_with_aiåœ¨æ²¡æœ‰ç»“æœæ—¶è¿”å›äº†Noneæˆ–[]
                    log_system_event("warning", f"å­—å¹•ä»»åŠ¡ {task_index} è¿”å›äº†ç©ºç»“æœï¼Œå¯èƒ½å¤„ç†å¤±è´¥ä½†æœªæŠ›å‡ºå¼‚å¸¸ã€‚", in_worker=True)

            except TimeoutError:
                # ã€æ–°å¢ã€‘æ•è·è¶…æ—¶é”™è¯¯
                log_system_event("error", f"âŒ è·å–å­—å¹•ä»»åŠ¡ {task_index} çš„ç»“æœè¶…æ—¶ï¼è¯¥å­è¿›ç¨‹å¯èƒ½å·²åƒµæ­»ã€‚", in_worker=True)
                # å³ä½¿ä¸€ä¸ªä»»åŠ¡è¶…æ—¶ï¼Œæˆ‘ä»¬ä¾ç„¶è¦ç»§ç»­å¤„ç†å…¶ä»–å·²å®Œæˆçš„ä»»åŠ¡
                
            except Exception as e:
                # ã€å…³é”®ã€‘ç°åœ¨å¯ä»¥æ­£ç¡®æ•è·å­è¿›ç¨‹ä¸­çš„æ‰€æœ‰å¼‚å¸¸
                log_system_event("error", f"âŒ è·å–å­—å¹•ä»»åŠ¡ {task_index} çš„ç»“æœæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
            
            finally:
                # æ— è®ºæˆåŠŸå¤±è´¥ï¼Œéƒ½æ›´æ–°è¿›åº¦
                update_status_callback(stage="subtitle_transcribing", details=f"å·²å¤„ç† {completed_count}/{num_groups} ä¸ªå­—å¹•æ‰¹æ¬¡...")

    log_system_event("info", "æ‰€æœ‰å¹¶å‘ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ­£åœ¨æ•´åˆå­—å¹•...", in_worker=True)
    
    if not all_srt_blocks:
        log_system_event("warning", "æ‰€æœ‰å­—å¹•æ‰¹æ¬¡å¤„ç†å‡å¤±è´¥æˆ–æœªè¿”å›ä»»ä½•å†…å®¹ï¼Œæœ€ç»ˆç”Ÿæˆçš„å­—å¹•ä¸ºç©ºã€‚", in_worker=True)
        # æ ¹æ®ä¸šåŠ¡éœ€æ±‚ï¼Œå¯ä»¥é€‰æ‹©è¿”å›ç©ºå­—ç¬¦ä¸²æˆ–æŠ›å‡ºå¼‚å¸¸
        # è¿™é‡Œé€‰æ‹©è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œè®©ä¸»æµç¨‹åˆ¤æ–­
        return ""

    all_srt_blocks.sort(key=lambda x: x["start_ms"])
    
    final_srt_lines = [f"{i + 1}\n{block['srt_line']}" for i, block in enumerate(all_srt_blocks)]
    final_srt_content = "\n".join(final_srt_lines)
    
    log_system_event("info", f"å­—å¹•æ•´åˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_srt_blocks)} æ¡å­—å¹•ã€‚", in_worker=True)
    
    return final_srt_content

# =============================================================================
# --- ç¬¬ 5 æ­¥: MixFileCLI å®¢æˆ·ç«¯ ---
# =============================================================================

class MixFileCLIClient:
    # ä¸€ä¸ªç®€å•çš„ç”¨äºä¸ MixFileCLI åç«¯ API äº¤äº’çš„å®¢æˆ·ç«¯ã€‚
    def __init__(self, base_url: str, proxies: dict = None):
        if not base_url.startswith("http"):
            raise ValueError("Base URL å¿…é¡»ä»¥ http æˆ– https å¼€å¤´")
        self.base_url = base_url
        self.session = requests.Session()
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘è®¾ç½®ä»£ç†
        if proxies:
            self.session.proxies = proxies

    def _make_request(self, method: str, url: str, **kwargs):
        # ç»Ÿä¸€çš„è¯·æ±‚å‘é€æ–¹æ³•ï¼ŒåŒ…å«é”™è¯¯å¤„ç†ã€‚
        try:
            # ç¡®ä¿è¯·æ±‚ä¸ä¼šè¢«ç¼“å­˜
            headers = kwargs.get('headers', {})
            headers.update({'Cache-Control': 'no-cache', 'Pragma': 'no-cache'})
            kwargs['headers'] = headers
            
            # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´ä»¥é€‚åº”æ…¢é€Ÿç½‘ç»œ
            response = self.session.request(method, url, timeout=600, **kwargs)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            error_text = str(e)
            status_code = 500 # é»˜è®¤ä¸ºé€šç”¨æœåŠ¡å™¨é”™è¯¯
            if e.response is not None:
                error_text = e.response.text
                status_code = e.response.status_code
            return (status_code, error_text)

    def upload_file(self, local_file_path: str, progress_callback: callable = None):
        #
        # ã€ä¿®æ”¹åã€‘ä¸Šä¼ å•ä¸ªæ–‡ä»¶å¹¶è·å–åˆ†äº«ç ï¼Œå¢åŠ äº†è¿›åº¦å›è°ƒåŠŸèƒ½ã€‚
        #
        # Args:
        #     local_file_path (str): æœ¬åœ°æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
        #     progress_callback (callable, optional): è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (bytes_uploaded, total_bytes)ã€‚
        #
        # Returns:
        #     requests.Response or tuple: æˆåŠŸæ—¶è¿”å› Response å¯¹è±¡ï¼Œå¤±è´¥æ—¶è¿”å› (status_code, error_text)ã€‚
        #
        filename = os.path.basename(local_file_path)
        upload_url = urljoin(self.base_url, f"/api/upload/{quote(filename)}")
        
        try:
            file_size = os.path.getsize(local_file_path)
        except OSError as e:
             # å¦‚æœæ–‡ä»¶åœ¨è¿™é‡Œå°±æ‰¾ä¸åˆ°äº†ï¼Œç›´æ¥è¿”å›é”™è¯¯
            return (404, f"File not found: {e}")

        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘åˆ›å»ºä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå®ƒåœ¨è¯»å–æ–‡ä»¶çš„åŒæ—¶è°ƒç”¨å›è°ƒå‡½æ•°
        def file_reader_generator(file_handle):
            chunk_size = 1 * 1024 * 1024 # 1MB chunk
            bytes_read = 0
            while True:
                chunk = file_handle.read(chunk_size)
                if not chunk:
                    if progress_callback: # ç¡®ä¿æœ€å100%è¢«è°ƒç”¨
                        progress_callback(file_size, file_size)
                    break
                bytes_read += len(chunk)
                if progress_callback:
                    progress_callback(bytes_read, file_size)
                yield chunk

        try:
            with open(local_file_path, 'rb') as f:
                # å°†ç”Ÿæˆå™¨ä½œä¸º data ä¼ é€’
                return self._make_request("PUT", upload_url, data=file_reader_generator(f))
        except FileNotFoundError as e:
            return (404, str(e))

# =============================================================================
# --- ç¬¬ 6 æ­¥: ç»Ÿä¸€çš„ Flask API æœåŠ¡ (ä¸»è¿›ç¨‹) ---
# =============================================================================

# --- A. åº”ç”¨åˆå§‹åŒ–ä¸ä»»åŠ¡ç®¡ç† ---
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False
# å…¨å±€ä»£ç†é…ç½®ï¼Œå°†åœ¨mainå‡½æ•°ä¸­è¢«è®¾ç½®
GLOBAL_PROXY_SETTINGS = None
# ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡çš„çŠ¶æ€ï¼Œå¹¶ç”¨é”æ¥ä¿è¯çº¿ç¨‹å®‰å…¨
tasks = {}
tasks_lock = threading.Lock()

# å…¨å±€å˜é‡ï¼Œå°†åœ¨ main å‡½æ•°ä¸­è¢«åˆå§‹åŒ–
api_client = None 
subtitle_config_global = {}
FRP_SERVER_ADDR = None # å£°æ˜å…¨å±€å˜é‡
TASK_QUEUE = None # å¤šè¿›ç¨‹ä»»åŠ¡é˜Ÿåˆ—
RESULT_QUEUE = None # å¤šè¿›ç¨‹ç»“æœé˜Ÿåˆ—

# --- B. API è·¯ç”±å®šä¹‰ ---

# =============================================================================
# --- (æ–¹æ³•1/3) unified_upload_endpoint [ä¿®æ”¹å] ---
# =============================================================================
@app.route("/api/upload", methods=["POST"])
def unified_upload_endpoint():
    #
    # ç»Ÿä¸€çš„åª’ä½“å¤„ç†å…¥å£ APIã€‚
    # ã€ä¿®æ”¹åã€‘å¢åŠ äº†å°†è¯·æ±‚å‚æ•°å­˜å…¥ä»»åŠ¡å­—å…¸çš„é€»è¾‘ï¼Œä»¥æ”¯æŒåŠ¨æ€è¿›åº¦è®¡ç®—ã€‚
    #
    data = request.get_json()
    if not data or "url" not in data:
        return jsonify({"error": "è¯·æ±‚ä½“ä¸­å¿…é¡»åŒ…å« 'url' å­—æ®µ"}), 400

    task_id = str(uuid.uuid4())
    
    # ä»è¯·æ±‚ä¸­æå–å‚æ•°
    request_params = {
        "url": data["url"],
        "extract_subtitle": data.get("extract_subtitle", False),
        "upload_video": data.get("upload_video", True),
        "upload_subtitle": data.get("upload_subtitle", False),
    }

    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€ç»“æ„
    with tasks_lock:
        tasks[task_id] = {
            "taskId": task_id,
            "status": "QUEUED",
            "progress": 0,
            "error": None,
            "results": {
                "video": {
                    "status": "PENDING" if request_params["upload_video"] else "SKIPPED",
                    "details": "ç­‰å¾…å¤„ç†" if request_params["upload_video"] else "ç”¨æˆ·æœªè¯·æ±‚æ­¤æ“ä½œ",
                    "output": None,
                    "error": None
                },
                "subtitle": {
                    "status": "PENDING" if request_params["extract_subtitle"] else "SKIPPED",
                    "details": "ç­‰å¾…å¤„ç†" if request_params["extract_subtitle"] else "ç”¨æˆ·æœªè¯·æ±‚æ­¤æ“ä½œ",
                    "output": None,
                    "error": None
                }
            },
            # --- START OF MODIFICATION ---
            # çº¦å®šä¸€ä¸ªå†…éƒ¨é”®æ¥å­˜å‚¨åŸå§‹å‚æ•°ï¼Œä¾›è¿›åº¦æ¨¡å‹ä½¿ç”¨
            "_internal_params": request_params,
            # --- END OF MODIFICATION ---
            "createdAt": time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime()),
            "updatedAt": time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())
        }

    # å°†ä»»åŠ¡æ•°æ®æ”¾å…¥é˜Ÿåˆ—ï¼Œç”±å­è¿›ç¨‹å¤„ç†
    task_data = {
        'task_id': task_id,
        'params': request_params,
        'subtitle_config': subtitle_config_global,
        'api_client_base_url': api_client.base_url,
        'frp_server_addr': FRP_SERVER_ADDR
    }
    TASK_QUEUE.put(task_data)
    
    log_system_event("info", f"å·²åˆ›å»ºæ–°ä»»åŠ¡ {task_id} å¹¶æ¨å…¥å¤„ç†é˜Ÿåˆ—ã€‚")

    # è¿”å›ä½“ä¿æŒä¸å˜
    return jsonify({
        "task_id": task_id,
        "status_url": f"/api/tasks/{task_id}"
    }), 202

# --- (å°†ä¸‹é¢çš„å‡½æ•°å®Œæ•´æ›¿æ¢æ‰åŸæ¥çš„ç‰ˆæœ¬) ---
@app.route("/api/tasks/<task_id>", methods=["GET"])
def get_task_status_endpoint(task_id):
    #
    # ã€å¥å£®ç‰ˆã€‘è·å–ä»»åŠ¡çŠ¶æ€ï¼Œå¹¶å¤„ç†åƒµå°¸/è¶…æ—¶ä»»åŠ¡ï¼Œç¡®ä¿æ€»èƒ½å‘å‡ºâ€œä»»åŠ¡å®Œæˆâ€ä¿¡å·ã€‚
    #
    with tasks_lock:
        task = tasks.get(task_id)

    if not task:
        # å¦‚æœä»»åŠ¡ä¸å­˜åœ¨ï¼Œè¿™æœ¬èº«å°±æ˜¯ä¸€ä¸ªæœ€ç»ˆçŠ¶æ€ã€‚
        # æˆ‘ä»¬è¿”å› 404ï¼Œä½†å¯ä»¥åŠ ä¸Šå®Œæˆä¿¡å·å¤´ï¼Œ
        # ä»¥é˜²åè°ƒå™¨ä¾§è®¤ä¸ºä¼šè¯å­˜åœ¨ä½†Workerä¾§å·²æ¸…ç†ã€‚
        response = jsonify({"error": "æœªæ‰¾åˆ°æŒ‡å®šçš„ task_id", "status": "NOT_FOUND"})
        response.status_code = 404
        response.headers['X-Task-Completed'] = 'true'
        log_system_event("warning", f"æŸ¥è¯¢ä¸å­˜åœ¨çš„ä»»åŠ¡ {task_id}ï¼Œå‘é€æ¸…ç†ä¿¡å·ã€‚")
        return response

    # --- START OF THE MODIFICATION ---
    
    # å®šä¹‰ä»»åŠ¡çš„â€œæœ€ç»ˆçŠ¶æ€â€å’Œâ€œæ´»åŠ¨çŠ¶æ€â€
    terminal_states = {"SUCCESS", "FAILED", "PARTIAL_SUCCESS"}
    active_states = {"QUEUED", "RUNNING"}
    
    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²ç»å¤„äºå·²çŸ¥çš„æœ€ç»ˆçŠ¶æ€
    current_status = task.get("status")
    if current_status in terminal_states:
        # ä»»åŠ¡å·²æ­£å¸¸ç»“æŸï¼ŒæŒ‰åŸè®¡åˆ’æ·»åŠ å®Œæˆä¿¡å·
        response = jsonify(task)
        response.headers['X-Task-Completed'] = 'true'
        return response

    # å¦‚æœä»»åŠ¡ä¸å¤„äºæœ€ç»ˆçŠ¶æ€ï¼Œåˆ™æ£€æŸ¥æ˜¯å¦â€œåƒµæ­»â€
    if current_status in active_states:
        # å®šä¹‰ä¸€ä¸ªè¶…æ—¶é˜ˆå€¼ï¼Œä¾‹å¦‚30åˆ†é’Ÿ
        # updatedAt åº”è¯¥åœ¨æ¯æ¬¡çŠ¶æ€æ›´æ–°æ—¶è¢«ä¿®æ”¹
        MAX_STUCK_DURATION = timedelta(minutes=300)
        try:
            # è§£æä»»åŠ¡çš„ updatedAt æ—¶é—´æˆ³
            updated_at_str = task.get("updatedAt", task.get("createdAt"))
            # ç§»é™¤ 'Z' å¹¶å¤„ç†æ¯«ç§’éƒ¨åˆ†
            updated_at_str = updated_at_str.replace('Z', '').split('.')[0]
            last_update_time = datetime.strptime(updated_at_str, '%Y-%m-%dT%H:%M:%S')

            if datetime.utcnow() - last_update_time > MAX_STUCK_DURATION:
                log_system_event("error", f"ä»»åŠ¡ {task_id} çŠ¶æ€ä¸º {current_status} å·²è¶…è¿‡ {MAX_STUCK_DURATION} æœªæ›´æ–°ï¼Œåˆ¤å®šä¸ºåƒµæ­»ã€‚")
                
                # ä¸»åŠ¨å°†ä»»åŠ¡æ ‡è®°ä¸ºå¤±è´¥
                task["status"] = "FAILED"
                task["error"] = {
                    "code": "TASK_TIMEOUT",
                    "message": f"Task was stuck in '{current_status}' state for too long."
                }
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼Œä»¥ä¾¿ä¸‹æ¬¡æŸ¥è¯¢èƒ½çœ‹åˆ°å¤±è´¥ä¿¡æ¯
                with tasks_lock:
                    tasks[task_id] = task

                # æ—¢ç„¶æˆ‘ä»¬å·²ç»ä»²è£äº†å®ƒçš„æœ€ç»ˆçŠ¶æ€ï¼Œç°åœ¨å°±å¯ä»¥å‘é€â€œå®Œæˆâ€ä¿¡å·äº†
                response = jsonify(task)
                response.status_code = 500 # è¡¨ç¤ºä»»åŠ¡æœ¬èº«å¤±è´¥äº†
                response.headers['X-Task-Completed'] = 'true'
                return response
        except (ValueError, KeyError) as e:
            # å¦‚æœæ—¶é—´æˆ³æ ¼å¼ä¸æ­£ç¡®æˆ–ä¸å­˜åœ¨ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­
            log_system_event("warning", f"æ— æ³•è§£æä»»åŠ¡ {task_id} çš„æ—¶é—´æˆ³æ¥æ£€æŸ¥è¶…æ—¶: {e}")

    # å¦‚æœä»»åŠ¡ä¸å¤„äºæœ€ç»ˆçŠ¶æ€ï¼Œä¹Ÿä¸è¢«è®¤ä¸ºæ˜¯åƒµæ­»ï¼Œ
    # åˆ™æ­£å¸¸è¿”å›å½“å‰çŠ¶æ€ï¼Œä¸å¸¦â€œå®Œæˆâ€ä¿¡å·
    return jsonify(task)
    
    # --- END OF THE MODIFICATION ---

@app.route('/killer_status_frp', methods=['GET'])
def health_status_endpoint():
    # ç”¨äºå¤–éƒ¨å¥åº·æ£€æŸ¥çš„ç®€å•ç«¯ç‚¹ã€‚
    return jsonify({
        "status": "killer_api_is_running_via_frp",
        "message": "ç»Ÿä¸€åª’ä½“å¤„ç† API æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶é€šè¿‡ FRP æš´éœ²ã€‚",
        "timestamp": time.time()
    }), 200


@app.route('/force_shutdown_notebook', methods=['GET'])
def force_shutdown_endpoint():
    #
    # è¿œç¨‹å…³é—­ APIï¼Œæ¥æ”¶ä¸€ä¸ª token è¿›è¡ŒéªŒè¯ã€‚
    #
    log_system_event("info", "API /force_shutdown_notebook è¢«è°ƒç”¨...")
    token_from_request = request.args.get('token')

    if token_from_request != KILLER_API_SHUTDOWN_TOKEN:
        log_system_event("error", "API Auth å¤±è´¥: Token æ— æ•ˆã€‚")
        return jsonify({"status": "error", "message": "Unauthorized"}), 401
    
    log_system_event("info", "API Auth æˆåŠŸã€‚æ­£åœ¨å®‰æ’åå°å…³é—­ä»»åŠ¡...")

    def delayed_full_shutdown():
        # Step 1: å…ˆå¹³æ»‘åœ°æ€æ­»å­è¿›ç¨‹
        log_system_event("info", "åå°å…³é—­ä»»åŠ¡ï¼šå¼€å§‹ç»ˆæ­¢å­è¿›ç¨‹...")
        _find_and_kill_targeted_processes(signal.SIGTERM)
        time.sleep(2)
        
        # Step 2: å¼ºåˆ¶ç»ˆæ­¢ä»ç„¶å­˜åœ¨çš„å­è¿›ç¨‹
        log_system_event("info", "åå°å…³é—­ä»»åŠ¡ï¼šå¼ºåˆ¶ç»ˆæ­¢ä»»ä½•æ®‹ç•™å­è¿›ç¨‹...")
        _find_and_kill_targeted_processes(signal.SIGKILL)
        time.sleep(1)

        # Step 3: ç»ˆæ­¢ä¸»å†…æ ¸
        _shutdown_notebook_kernel_immediately()
    
    # ç«‹å³å¯åŠ¨åå°çº¿ç¨‹ï¼Œç„¶åç«‹åˆ»è¿”å›å“åº”ï¼Œç¡®ä¿è°ƒç”¨æ–¹æ”¶åˆ°ç¡®è®¤
    threading.Thread(target=delayed_full_shutdown, daemon=True).start()
    
    return jsonify({
        "status": "shutdown_initiated",
        "message": "å…³é—­ä¿¡å·å·²æ¥æ”¶ã€‚åå°æ­£åœ¨æ‰§è¡Œæ¸…ç†å’Œå†…æ ¸å…³é—­æ“ä½œã€‚",
    }), 200

# =============================================================================
# --- ç¬¬ 7 æ­¥: é«˜å®¹é”™çš„ç»Ÿä¸€ä»»åŠ¡å¤„ç†å™¨ (åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œ) ---
# =============================================================================
# åˆ äº†å“¦å°±å®‰å…¨äº†
def process_unified_task(task_data: dict, result_queue: multiprocessing.Queue, upload_queue: multiprocessing.Queue, subtitle_config: dict, ai_models: dict):
    """
    ã€è¿›åº¦è®¡ç®—æœ€ç»ˆä¿®æ­£ç‰ˆã€‘æ­¤å‡½æ•°ä¸å†è®¡ç®—æ€»è¿›åº¦ã€‚
    å®ƒåªå‘ä¸»è¿›ç¨‹æŠ¥å‘Šæœ€åŸå§‹çš„é˜¶æ®µæ€§è¿›åº¦äº‹ä»¶ï¼Œç”±ä¸»è¿›ç¨‹è¿›è¡Œæƒå¨è®¡ç®—ã€‚
    """
    task_id = task_data['task_id']
    params = task_data['params']
    temp_dir = Path(f"/kaggle/working/task_{task_id}")
    temp_dir.mkdir(exist_ok=True)
    
    # --- æ–°çš„ã€ç®€åŒ–çš„çŠ¶æ€æ›´æ–°å‡½æ•° ---
    def _update_component_status(component, status=None, details=None, output=None, error_obj=None):
        payload = {'type': 'component_update', 'task_id': task_id, 'component': component, 'data': {}}
        if status: payload['data']['status'] = status
        if details: payload['data']['details'] = details
        if output: payload['data']['output'] = output
        if error_obj: payload['data']['error'] = error_obj
        if payload['data']:
            result_queue.put(payload)

    def _report_stage_progress(stage_key, stage_percent):
        """åªæŠ¥å‘Šé˜¶æ®µå’Œå…¶å­è¿›åº¦ï¼Œä¸è®¡ç®—æ€»è¿›åº¦ã€‚"""
        payload = {
            'type': 'progress_event', 
            'task_id': task_id, 
            'stage': stage_key, 
            'percent': stage_percent # (0.0 to 1.0)
        }
        result_queue.put(payload)
    
    try:
        # --- æ­¥éª¤ 1 (ä¸‹è½½æ–‡ä»¶) ---
        file_url = params['url']
        filename = unquote(file_url.split("/")[-1].split("?")[0] or f"file_{task_id}")
        local_file_path = temp_dir / filename

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½ï¼ˆå¦‚æœè§†é¢‘å’Œå­—å¹•éƒ½ä¸ä¸Šä¼ ä¸”ä¸æå–å­—å¹•ï¼Œåˆ™è·³è¿‡ï¼‰
        needs_download = params.get("upload_video", True) or params.get("extract_subtitle", False)

        if needs_download:
            _update_component_status(component="video", status="RUNNING", details="å¼€å§‹ä¸‹è½½æ–‡ä»¶...")
            _update_component_status(component="subtitle", status="RUNNING", details="ç­‰å¾…è§†é¢‘ä¸‹è½½...")
            _report_stage_progress('download', 0.0)
            
            with requests.get(file_url, stream=True, timeout=60) as r:
                r.raise_for_status()
                total_size = int(r.headers.get('content-length', 0))
                bytes_downloaded = 0
                with open(local_file_path, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        f.write(chunk)
                        bytes_downloaded += len(chunk)
                        if total_size > 0:
                            dl_percent = bytes_downloaded / total_size
                            _update_component_status(component="video", details=f"ä¸‹è½½ä¸­ ({int(dl_percent*100)}%)...")
                            _report_stage_progress('download', dl_percent)
            _update_component_status(component="video", details="ä¸‹è½½å®Œæˆ")
            _report_stage_progress('download', 1.0) # æŠ¥å‘Šä¸‹è½½é˜¶æ®µ100%å®Œæˆ
        else:
             local_file_path = None

        # --- æ­¥éª¤ 2 (éŸ³é¢‘æå–) ---
        raw_audio_path_for_subtitle = None
        if params["extract_subtitle"]:
            mime_type, _ = mimetypes.guess_type(local_file_path) if local_file_path and local_file_path.exists() else (None, None)
            if not (mime_type and mime_type.startswith("video")):
                _update_component_status(component="subtitle", status="SKIPPED", details="æºæ–‡ä»¶ä¸æ˜¯è§†é¢‘æ ¼å¼")
                _report_stage_progress('subtitle_pipeline', 1.0) # å¦‚æœè·³è¿‡ï¼Œä¹ŸæŠ¥å‘Šæ­¤é˜¶æ®µ100%å®Œæˆ
            else:
                try:
                    def sub_progress_callback(stage, details): _update_component_status(component="subtitle", details=details)
                    raw_audio_path_for_subtitle = extract_audio_with_ffmpeg(local_file_path, temp_dir, sub_progress_callback)
                except Exception as e:
                    _update_component_status(component="subtitle", status="FAILED", details=f"éŸ³é¢‘æå–å¤±è´¥: {e}", error_obj={"code": "AUDIO_EXTRACTION_FAILED", "message": str(e)})
                    _report_stage_progress('subtitle_pipeline', 1.0) # å¤±è´¥ä¹ŸæŠ¥å‘Šæ­¤é˜¶æ®µ100%å®Œæˆ
        else:
            _update_component_status(component="subtitle", status="SKIPPED", details="ç”¨æˆ·æœªè¯·æ±‚æå–")

        # --- æ­¥éª¤ 3 (å¹¶è¡Œæ´¾å‘ä»»åŠ¡) ---
        if params["upload_video"]:
            _update_component_status(component="video", status="RUNNING", details="å·²æ´¾å‘ä¸Šä¼ ä»»åŠ¡")
            _report_stage_progress('video_upload', 0.01) # æŠ¥å‘Šä¸Šä¼ é˜¶æ®µå¼€å§‹
            upload_queue.put({
                'task_id': task_id, 'component': 'video', 'local_file_path': str(local_file_path),
                'filename_for_link': filename
            })
        else:
            _update_component_status(component="video", status="SKIPPED", details="ç”¨æˆ·æœªè¯·æ±‚ä¸Šä¼ ")
            _report_stage_progress('video_upload', 1.0) # å¦‚æœè·³è¿‡ï¼Œä¹ŸæŠ¥å‘Šæ­¤é˜¶æ®µ100%å®Œæˆ
        
        if raw_audio_path_for_subtitle:
            try:
                # -- å­—å¹•ç®¡çº¿çš„å­è¿›åº¦æŠ¥å‘Š --
                def subtitle_pipeline_progress_reporter(sub_stage_name, sub_stage_progress_percent):
                    # VADå 25%ï¼ŒAIå¤„ç†å 65%ï¼Œä¸Šä¼ å 10%
                    if sub_stage_name == "preprocess":
                        stage_percent = 0.10 + (sub_stage_progress_percent * 0.25)
                    elif sub_stage_name == "ai_transcribe":
                        stage_percent = 0.35 + (sub_stage_progress_percent * 0.65)
                    else: # å…¶ä»–æƒ…å†µä¸æŠ¥å‘Šå­è¿›åº¦
                        stage_percent = 0.10 # åœç•™åœ¨éŸ³é¢‘æå–å®Œæˆçš„é˜¶æ®µ
                    
                    _report_stage_progress('subtitle_pipeline', stage_percent)
                
                def sub_progress_callback(stage, details): 
                    _update_component_status(component="subtitle", details=details)
                    if stage == 'subtitle_vad':
                        subtitle_pipeline_progress_reporter("preprocess", 1.0)
                    if stage == 'subtitle_transcribing':
                        try:
                            parts = details.split(" ")
                            if len(parts) > 2 and "/" in parts[1]:
                                done, total = map(int, parts[1].split('/'))
                                subtitle_pipeline_progress_reporter("ai_transcribe", done / total)
                        except: pass
                
                _report_stage_progress('subtitle_pipeline', 0.10) # æŠ¥å‘ŠéŸ³é¢‘æå–å®Œæˆï¼Œå­—å¹•ç®¡çº¿å®Œæˆ10%
                audio_chunks = preprocess_audio_for_subtitles(raw_audio_path_for_subtitle, temp_dir, sub_progress_callback, ai_models)
                
                srt_content = run_subtitle_extraction_pipeline(subtitle_config, audio_chunks, sub_progress_callback)
                if not srt_content: raise RuntimeError("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å­—å¹•å†…å®¹ã€‚")
                
                _update_component_status(component="subtitle", output={"contentBase64": base64.b64encode(srt_content.encode('utf-8')).decode('utf-8')})
                
                if params["upload_subtitle"]:
                    _update_component_status(component="subtitle", status="RUNNING", details="å·²æ´¾å‘å­—å¹•ä¸Šä¼ ä»»åŠ¡")
                    _report_stage_progress('subtitle_upload', 0.01) # è¿™æ˜¯ä¸€ä¸ªæ–°çš„é˜¶æ®µäº‹ä»¶
                    srt_filename = local_file_path.stem + ".srt"
                    srt_path = temp_dir / srt_filename
                    with open(srt_path, "w", encoding="utf-8") as f: f.write(srt_content)
                    upload_queue.put({
                       'task_id': task_id, 'component': 'subtitle', 'local_file_path': str(srt_path),
                       'filename_for_link': srt_filename
                    })
                else:
                    _update_component_status(component="subtitle", status="SUCCESS", details="æå–æˆåŠŸ")
                    _report_stage_progress('subtitle_pipeline', 1.0)

            except Exception as e:
                _update_component_status(component="subtitle", status="FAILED", details=str(e), error_obj={"code": "SUBTITLE_PIPELINE_FAILED", "message": str(e)})
                _report_stage_progress('subtitle_pipeline', 1.0)
    except Exception as e:
        log_system_event("error", f"å¤„ç†ä»»åŠ¡ {task_id} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
        # å‘é€ä¸€ä¸ªé¡¶çº§çš„å¤±è´¥äº‹ä»¶
        result_queue.put({'type': 'task_failed', 'task_id': task_id, 'error': {"code": "FATAL_ERROR", "message": str(e)}})
    finally:
        log_system_event("info", f"åª’ä½“å¤„ç†è¿›ç¨‹ {task_id} çš„ä¸»è¦å·¥ä½œå·²å®Œæˆã€‚", in_worker=True)

# =============================================================================
# --- ç¬¬ 8 æ­¥: å¤šè¿›ç¨‹ Worker ä¸ä¸»ç¨‹åº ---
# =============================================================================


# =============================================================================
# --- (æ–°å¢æ–¹æ³• 1/3) uploader_process_loop [å…¨æ–°] ---
# =============================================================================

# è¿™ä¸ªæ–°å‡½æ•°ä¸“é—¨è´Ÿè´£æ–‡ä»¶ä¸Šä¼ ï¼Œè¿è¡Œåœ¨ç‹¬ç«‹çš„è¿›ç¨‹ä¸­
def uploader_process_loop(upload_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue):
    """
    ã€è¿›åº¦è®¡ç®—æœ€ç»ˆä¿®æ­£ç‰ˆã€‘ä¸Šä¼ å·¥ä½œè¿›ç¨‹ï¼Œä¸å†å…³å¿ƒæ€»è¿›åº¦ã€‚
    åªå‘ä¸»è¿›ç¨‹æŠ¥å‘Šå…¶è´Ÿè´£ç»„ä»¶çš„åŸå§‹ä¸Šä¼ è¿›åº¦äº‹ä»¶ã€‚
    """
    log_system_event("info", "ä¸Šä¼ ä¸“ç”¨å·¥ä½œè¿›ç¨‹å·²å¯åŠ¨ã€‚", in_worker=True)
    task_proxy_cache = {}
    proxy_manager = ProxyManager(subtitle_config_global.get("V2RAY_SUB_URL"))
    xray_process = None

    # start_xray_for_task å†…éƒ¨é€»è¾‘ä¸å˜
    def start_xray_for_task(config):
        nonlocal xray_process
        if xray_process:
            xray_process.terminate()
            xray_process.wait()
        
        config_path = Path("/kaggle/working/xray_final_config.json")
        with open(config_path, 'w') as f: json.dump(config, f)
        
        xray_process = run_command(f"{proxy_manager.xray_path} -c {config_path}", "xray_upload.log")
        if not wait_for_port(proxy_manager.local_socks_port, host='127.0.0.1', timeout=10):
            log_system_event("error", "å¯åŠ¨æœ€ä¼˜ä»£ç†èŠ‚ç‚¹ç”¨äºä¸Šä¼ å¤±è´¥ï¼", in_worker=True)
            return False
        return True

    # --- æ–°çš„ã€ç®€åŒ–çš„çŠ¶æ€æ›´æ–°å’Œè¿›åº¦æŠ¥å‘Šå‡½æ•° ---
    def _update_component_status(task_id, component, status=None, details=None, output=None, error_obj=None):
        payload = {'type': 'component_update', 'task_id': task_id, 'component': component, 'data': {}}
        if status: payload['data']['status'] = status
        if details: payload['data']['details'] = details
        if output: payload['data']['output'] = output
        if error_obj: payload['data']['error'] = error_obj
        if payload['data']:
            result_queue.put(payload)
            
    def _report_upload_progress(task_id, component, upload_percent):
        """æŠ¥å‘Šä¸Šä¼ è¿›åº¦çš„åŸå§‹äº‹ä»¶ã€‚"""
        stage_key = 'video_upload' if component == 'video' else 'subtitle_upload'
        payload = {
            'type': 'progress_event',
            'task_id': task_id,
            'stage': stage_key,
            'percent': upload_percent # (0.0 to 1.0)
        }
        result_queue.put(payload)
        
    try:
        while True:
            try:
                upload_task_data = upload_queue.get()
                if upload_task_data is None: break

                task_id = upload_task_data['task_id']
                component = upload_task_data['component']
                local_file_path_str = upload_task_data['local_file_path']
                
                # V2: ä»ä¸»è¿›ç¨‹ä¼ é€’ API å®¢æˆ·ç«¯åŸºç¡€URLï¼Œä»¥æ”¯æŒæµ‹é€Ÿ
                # åœ¨ V3 ä¸­è¿™ä¸ªä¿¡æ¯ç°åœ¨ä» upload_task_data ä¸­æ¥
                api_client_base_url = upload_task_data.get('api_client_base_url', f"http://127.0.0.1:{MIXFILE_LOCAL_PORT}")

                # --- æŒ‰éœ€æµ‹é€Ÿé€»è¾‘ (ä¿æŒä¸å˜) ---
                if task_id not in task_proxy_cache:
                    proxies_for_task, config_for_task = proxy_manager.get_best_proxy_for_upload(api_client_base_url)
                    task_proxy_cache[task_id] = {'proxies': proxies_for_task, 'config': config_for_task}
                    if config_for_task:
                        if not start_xray_for_task(config_for_task):
                            task_proxy_cache[task_id]['proxies'] = None
                    elif xray_process:
                        xray_process.terminate()
                        xray_process.wait()
                        xray_process = None

                current_proxies = task_proxy_cache[task_id]['proxies']

                try:
                    filename_for_link = upload_task_data['filename_for_link']
                    
                    api_client = MixFileCLIClient(base_url=api_client_base_url, proxies=current_proxies)
                    log_system_event("info", f"[ä¸Šä¼ è¿›ç¨‹] [{component}] å¼€å§‹å¤„ç†ä¸Šä¼ ä»»åŠ¡ (ä»£ç†: {'å¯ç”¨' if current_proxies else 'ç¦ç”¨'})...", in_worker=True)
                    _update_component_status(task_id, component, status="RUNNING", details="æ­£åœ¨ä¸Šä¼  (0%)...")
                    
                    # --- è¿›åº¦å›è°ƒé€»è¾‘ä¿®æ”¹ ---
                    def progress_callback(bytes_uploaded, total_bytes):
                        if total_bytes > 0:
                            percent_float = bytes_uploaded / total_bytes
                            # åªæŠ¥å‘ŠåŸå§‹äº‹ä»¶
                            _report_upload_progress(task_id, component, percent_float)
                            # details æ›´æ–°å¯ä»¥ä¿ç•™ï¼Œå› ä¸ºå®ƒå¯¹UIå‹å¥½
                            _update_component_status(task_id, component, details=f"æ­£åœ¨ä¸Šä¼  ({int(percent_float * 100)}%)")

                    response = api_client.upload_file(local_file_path_str, progress_callback=progress_callback)
                    
                    if isinstance(response, requests.Response) and response.ok:
                        share_code = response.text.strip()
                        output_data = {"shareCode": share_code}
                        _update_component_status(task_id, component, "SUCCESS", "ä¸Šä¼ æˆåŠŸ", output=output_data)
                        log_system_event("info", f"âœ… [ä¸Šä¼ è¿›ç¨‹] [{component}] ä¸Šä¼ æˆåŠŸã€‚", in_worker=True)
                    else:
                        error_msg = f"HTTP Error"
                        if isinstance(response, requests.Response):
                            error_msg = f"HTTP {response.status_code}: {response.text}"
                        elif isinstance(response, tuple):
                            error_msg = f"è¯·æ±‚å¤±è´¥ (çŠ¶æ€ç  {response[0]}): {response[1]}"
                        raise RuntimeError(error_msg)

                except Exception as e:
                    log_system_event("error", f"âŒ [ä¸Šä¼ è¿›ç¨‹] [{component}] ä¸Šä¼ ä»»åŠ¡å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
                    _update_component_status(task_id, component, "FAILED", f"ä¸Šä¼ å¤±è´¥: {e}", error_obj={"code": "UPLOAD_FAILED", "message": str(e)})

                finally:
                    # æ–‡ä»¶æ¸…ç†å’Œä»£ç†ç¼“å­˜æ¸…ç†é€»è¾‘ (ä¿æŒä¸å˜)
                    try: Path(local_file_path_str).unlink(missing_ok=True)
                    except OSError: pass
                    
                    is_last_upload = False
                    with tasks_lock: # éœ€è¦è®¿é—®ä¸»è¿›ç¨‹çš„å…¨å±€ tasks æ¥åˆ¤æ–­
                        if task_id in tasks:
                             # å¦‚æœå­—å¹•ç»„ä»¶è¢«è·³è¿‡ï¼Œåˆ™è§†é¢‘ä¸Šä¼ å°±æ˜¯æœ€åä¸€ä¸ª
                            if component == 'video' and tasks[task_id]['results']['subtitle']['status'] in ('SKIPPED', 'FAILED'):
                                is_last_upload = True
                            # å¦‚æœå­—å¹•ç»„ä»¶æ²¡è¢«è·³è¿‡ï¼Œåˆ™å­—å¹•ä¸Šä¼ æ˜¯æœ€åä¸€ä¸ª
                            elif component == 'subtitle':
                                is_last_upload = True

                    if is_last_upload:
                        log_system_event("info", f"ä»»åŠ¡ {task_id} ä¸Šä¼ ç¯èŠ‚å®Œæˆï¼Œæ¸…ç†ä»£ç†ç¼“å­˜ã€‚", in_worker=True)
                        if task_id in task_proxy_cache: del task_proxy_cache[task_id]
                        if xray_process:
                           xray_process.terminate()
                           xray_process.wait()
                           xray_process = None

            except QueueEmpty: continue
            except Exception as e:
                log_system_event("critical", f"ä¸Šä¼ ä¸“ç”¨å·¥ä½œè¿›ç¨‹å¾ªç¯å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}", in_worker=True)
    finally:
        if xray_process:
            xray_process.terminate()
            xray_process.wait()
        log_system_event("info", "ä¸Šä¼ ä¸“ç”¨å·¥ä½œè¿›ç¨‹å·²å…³é—­ã€‚", in_worker=True)

def worker_process_loop(task_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue, upload_queue: multiprocessing.Queue):
    """
    åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å¾ªç¯ï¼Œè´Ÿè´£åª’ä½“å¤„ç†å¹¶å°†ä¸Šä¼ ä»»åŠ¡å¤–åŒ…ã€‚
    """
    log_system_event("info", "åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å·²å¯åŠ¨ã€‚", in_worker=True)
    ai_models = load_ai_models()
    # subtitle_config_global æ˜¯åœ¨mainå‡½æ•°ä¸­è§£å¯†çš„å…¨å±€å˜é‡ï¼Œå­è¿›ç¨‹å¯ä»¥ç›´æ¥è®¿é—®
    
    while True:
        try:
            task_data = task_queue.get()
            if task_data is None:
                break
            log_system_event("info", f"åª’ä½“å¤„ç†è¿›ç¨‹æ¥æ”¶åˆ°æ–°ä»»åŠ¡: {task_data['task_id']}", in_worker=True)
            process_unified_task(task_data, result_queue, upload_queue, subtitle_config_global, ai_models)
        except KeyboardInterrupt:
            break
        except Exception as e:
            log_system_event("critical", f"åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å¾ªç¯å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", in_worker=True)
    log_system_event("info", "åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹å·²å…³é—­ã€‚", in_worker=True)

def result_processor_thread_loop(result_queue: multiprocessing.Queue):
    """
    ã€è¿›åº¦è®¡ç®—æœ€ç»ˆä¿®æ­£ç‰ˆã€‘ç»“æœå¤„ç†å™¨ã€‚
    è¿™æ˜¯å”¯ä¸€çš„ã€æƒå¨çš„è¿›åº¦è®¡ç®—ä¸­å¿ƒã€‚å®ƒç›‘å¬æ¥è‡ªå­è¿›ç¨‹çš„åŸå§‹äº‹ä»¶ï¼Œ
    å¹¶ä½¿ç”¨å†…ç½®çš„ TaskProgressModel å°†å…¶è½¬åŒ–ä¸ºå‡†ç¡®çš„æ€»è¿›åº¦ã€‚
    """
    log_system_event("info", "ç»“æœå¤„ç†çº¿ç¨‹å·²å¯åŠ¨ã€‚")
    
    task_progress_models = {}

    class TaskProgressModel:
        def __init__(self, params):
            self.params = params
            self.completed_weight = 0
            self._completed_stages = set()
            self.last_known_stage_progress = {}

            # -- æƒé‡å®šä¹‰ --
            # æƒé‡ç°åœ¨æ›´ç»†ç²’åº¦ï¼Œå­—å¹•ä¸Šä¼ ç‹¬ç«‹å‡ºæ¥
            PROGRESS_WEIGHTS = {
                'download': 20,
                'video_upload': 40,
                'subtitle_pipeline': 30, # éŸ³é¢‘æå–ã€VADã€AI
                'subtitle_upload': 10,
            }

            self.active_weights = {}
            if self.params.get("upload_video", True):
                self.active_weights['video_upload'] = PROGRESS_WEIGHTS['video_upload']
            
            if self.params.get("extract_subtitle", False):
                self.active_weights['subtitle_pipeline'] = PROGRESS_WEIGHTS['subtitle_pipeline']
                if self.params.get("upload_subtitle", False):
                    self.active_weights['subtitle_upload'] = PROGRESS_WEIGHTS['subtitle_upload']

            # åªæœ‰åœ¨æœ‰éœ€è¦å¤„ç†çš„ç»„ä»¶æ—¶ï¼Œæ‰åŠ ä¸Šä¸‹è½½çš„æƒé‡
            if self.active_weights:
                self.active_weights['download'] = PROGRESS_WEIGHTS['download']
            
            self.total_active_weight = sum(self.active_weights.values())
            if self.total_active_weight == 0: self.total_active_weight = 100
        
        def _update_stage_progress(self, stage_key: str, percent: float):
            # ç¡®ä¿è¿›åº¦ä¸ä¼šå›é€€
            if percent >= self.last_known_stage_progress.get(stage_key, 0.0):
                self.last_known_stage_progress[stage_key] = percent
                if percent >= 1.0:
                    self._mark_stage_as_completed(stage_key)

        def _mark_stage_as_completed(self, stage_key: str):
            if stage_key in self.active_weights and stage_key not in self._completed_stages:
                self.completed_weight += self.active_weights[stage_key]
                self._completed_stages.add(stage_key)
        
        def get_total_progress(self) -> int:
            current_progress_contribution = 0
            for stage, weight in self.active_weights.items():
                if stage not in self._completed_stages:
                    stage_percent = self.last_known_stage_progress.get(stage, 0.0)
                    current_progress_contribution += weight * stage_percent
            
            total_progress = self.completed_weight + current_progress_contribution
            normalized_progress = int((total_progress / self.total_active_weight) * 100)
            return min(normalized_progress, 100) # ç¡®ä¿ä¸è¶…è¿‡100

        def update_from_event(self, event_type: str, component: str, data: dict):
            # å½“ç»„ä»¶æ›´æ–°æœ€ç»ˆçŠ¶æ€æ—¶ï¼Œä¹Ÿæ›´æ–°è¿›åº¦æ¨¡å‹çš„å®ŒæˆçŠ¶æ€
            status = data.get("status")
            if status not in ("PENDING", "RUNNING", None): # (SUCCESS, FAILED, SKIPPED)
                if component == "video":
                    self._update_stage_progress("video_upload", 1.0)
                elif component == "subtitle":
                    self._update_stage_progress("subtitle_pipeline", 1.0)
                    if self.params.get("upload_subtitle", False) and status != 'SKIPPED':
                        # å¦‚æœå­—å¹•ä¸Šä¼ å¤±è´¥æˆ–æˆåŠŸï¼Œéƒ½ç®—è¿™ä¸ªé˜¶æ®µç»“æŸäº†
                        self._update_stage_progress("subtitle_upload", 1.0)

    while True:
        try:
            result_data = result_queue.get(timeout=3600)
            task_id = result_data['task_id']
            data_type = result_data.get('type')

            with tasks_lock:
                if task_id not in tasks: continue
                task = tasks[task_id]

                if task_id not in task_progress_models:
                    task_params = task.get('_internal_params')
                    if task_params:
                        task_progress_models[task_id] = TaskProgressModel(task_params)
                
                progress_model = task_progress_models.get(task_id)

                task['updatedAt'] = time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())
                
                # --- äº‹ä»¶å¤„ç†ä¸è¿›åº¦è®¡ç®— ---
                if data_type == 'progress_event' and progress_model:
                    stage = result_data['stage']
                    percent = result_data['percent']
                    progress_model._update_stage_progress(stage, percent)
                
                elif data_type == 'component_update':
                    component = result_data['component']
                    data = result_data['data']
                    if component in task['results']:
                        # åˆå¹¶ output
                        if 'output' in data and isinstance(task['results'][component].get('output'), dict):
                            task['results'][component]['output'].update(data['output'])
                            del data['output']
                        # æ›´æ–° status, details ç­‰
                        task['results'][component].update(data)
                        if progress_model:
                            progress_model.update_from_event(data_type, component, data)

                elif data_type == 'task_failed':
                    task['status'] = 'FAILED'
                    task['error'] = result_data.get('error')
                    task['progress'] = 100
                
                # åªæœ‰åœ¨æ”¶åˆ°éç»ˆç»“æ€§äº‹ä»¶æ—¶æ‰æ›´æ–°è¿›åº¦
                if task['status'] not in ["SUCCESS", "FAILED", "PARTIAL_SUCCESS"]:
                    task['status'] = "RUNNING"
                    if progress_model:
                        task['progress'] = progress_model.get_total_progress()

                # --- ä»»åŠ¡æœ€ç»ˆçŠ¶æ€è¯„ä¼° (é€»è¾‘åŸºæœ¬ä¸å˜) ---
                video_status = task['results']['video']['status']
                subtitle_status = task['results']['subtitle']['status']
                is_finished = "PENDING" not in (video_status, subtitle_status) and "RUNNING" not in (video_status, subtitle_status)

                if is_finished and task['status'] not in ["SUCCESS", "FAILED", "PARTIAL_SUCCESS"]:
                    # (è¿™éƒ¨åˆ†é€»è¾‘ä¸ä¹‹å‰ä¸€æ ·ï¼Œåªæ˜¯ç¡®ä¿åœ¨æœ€ç»ˆçŠ¶æ€è¯„ä¼°å‰è®¾ç½®è¿›åº¦ä¸º100%)
                    log_system_event("info", f"ä»»åŠ¡ {task_id} æ‰€æœ‰ç»„ä»¶å·²å®Œæˆï¼Œæ­£åœ¨è¿›è¡Œæœ€ç»ˆçŠ¶æ€è¯„ä¼°ã€‚")
                    task['progress'] = 100
                    
                    for comp in task['results']:
                        if task['results'][comp]['status'] == 'RUNNING':
                            task['results'][comp]['status'] = 'FAILED' #...

                    active_statuses = [s for s in (task['results']['video']['status'], task['results']['subtitle']['status']) if s != "SKIPPED"]
                    
                    if not active_statuses: task['status'] = "SUCCESS"
                    elif all(s == "SUCCESS" for s in active_statuses): task['status'] = "SUCCESS"
                    elif "FAILED" in active_statuses:
                        task['status'] = "PARTIAL_SUCCESS" if "SUCCESS" in active_statuses else "FAILED"
                    else: task['status'] = "SUCCESS"

                    log_system_event("info", f"ä»»åŠ¡ {task_id} æœ€ç»ˆçŠ¶æ€è¢«è®¾ç½®ä¸º: {task['status']}")
                    
                    if task_id in task_progress_models: del task_progress_models[task_id]

                    temp_dir_to_clean = Path(f"/kaggle/working/task_{task_id}")
                    if temp_dir_to_clean.exists():
                        log_system_event("info", f"ä»»åŠ¡ {task_id} å·²ç»“æŸï¼Œå‡†å¤‡æ¸…ç†ä¸´æ—¶ç›®å½•...")
                        try:
                            shutil.rmtree(temp_dir_to_clean)
                            log_system_event("info", f"âœ… æˆåŠŸæ¸…ç†ä»»åŠ¡ {task_id} çš„ä¸´æ—¶ç›®å½•ã€‚")
                        except Exception as e:
                            log_system_event("error", f"âŒ æ¸…ç†ä»»åŠ¡ {task_id} çš„ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

        except QueueEmpty: continue
        except Exception as e: log_system_event("error", f"ç»“æœå¤„ç†çº¿ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

def main():
    #
    # ä¸»æ‰§è¡Œå‡½æ•°ï¼Œè´Ÿè´£åˆå§‹åŒ–å’Œå¯åŠ¨æ‰€æœ‰æœåŠ¡ã€‚
    #
    global api_client, subtitle_config_global, FRP_SERVER_ADDR
    global TASK_QUEUE, RESULT_QUEUE, UPLOAD_QUEUE # <--- æ–°å¢ UPLOAD_QUEUE

    try:
        # --- 1. å¯åŠ¨å‰å‡†å¤‡ ---
        log_system_event("info", "æœåŠ¡æ­£åœ¨å¯åŠ¨...")
        
        install_torch_cmd = (
            "pip uninstall -y torch torchvision torchaudio && "
            "pip install torch==2.3.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
        )
        log_system_event("info", "æ­£åœ¨å®‰è£…å…¼å®¹çš„ PyTorch ç‰ˆæœ¬...")
        install_proc = subprocess.run(install_torch_cmd, shell=True, capture_output=True, text=True)
        if install_proc.returncode != 0:
            log_system_event("error", f"PyTorch å®‰è£…å¤±è´¥ï¼\nStdout: {install_proc.stdout}\nStderr: {install_proc.stderr}")
            raise RuntimeError("æœªèƒ½å®‰è£…å…¼å®¹çš„ PyTorch ç‰ˆæœ¬ã€‚")
        log_system_event("info", "âœ… å…¼å®¹çš„ PyTorch å®‰è£…å®Œæˆã€‚")
        wifi = speedtest.Speedtest()
        install_other_cmd = "pip install -q pydantic pydub faster-whisper@https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz denoiser google-generativeai requests psutil speedtest-cli"
        log_system_event("info", "æ­£åœ¨å®‰è£…å…¶ä½™ä¾èµ–åº“...")
        subprocess.run(install_other_cmd, shell=True, check=True)
        log_system_event("info", "âœ… å…¶ä½™ä¾èµ–åº“å®‰è£…å®Œæˆã€‚")
        
        check_environment()
        
        # --- è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³• ---
        multiprocessing.set_start_method('fork', force=True)

        # --- 2. è§£å¯†é…ç½® ---
        frp_config = get_decrypted_config(ENCRYPTED_FRP_CONFIG, "FRP")
        subtitle_config_global = get_decrypted_config(ENCRYPTED_SUBTITLE_CONFIG, "Subtitle")
        FRP_SERVER_ADDR = frp_config['FRP_SERVER_ADDR']
        FRP_SERVER_PORT = frp_config['FRP_SERVER_PORT']
        FRP_TOKEN = frp_config['FRP_TOKEN']
        
        # --- 3. åˆå§‹åŒ– MixFile å®¢æˆ·ç«¯ ---
        api_client_base_url = f"http://127.0.0.1:{MIXFILE_LOCAL_PORT}"
        api_client = MixFileCLIClient(base_url=api_client_base_url)

        # --- 4. å¯åŠ¨ MixFileCLI æœåŠ¡ ---
        log_system_event("info", "æ­£åœ¨åˆ›å»º MixFileCLI config.yml...")
        with open("config.yml", "w") as f: f.write(mixfile_config_yaml)
        log_system_event("info", "æ­£åœ¨ä¸‹è½½å¹¶å¯åŠ¨ MixFileCLI...")
        if not os.path.exists("mixfile-cli.jar"):
            run_command("wget -q --show-progress https://raw.githubusercontent.com/jornhand/kagglewithmixfile/refs/heads/main/mixfile-cli-2.0.1.jar -O mixfile-cli.jar").wait()
        run_command("java -jar mixfile-cli.jar", "mixfile.log")
        if not wait_for_port(MIXFILE_LOCAL_PORT):
            raise RuntimeError("MixFileCLI æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ mixfile.logã€‚")

        # --- 5. åˆå§‹åŒ–å¤šè¿›ç¨‹é˜Ÿåˆ—å’Œå·¥ä½œè¿›ç¨‹ ---
        TASK_QUEUE = multiprocessing.Queue()
        RESULT_QUEUE = multiprocessing.Queue()
        UPLOAD_QUEUE = multiprocessing.Queue() # æ–°å¢ä¸Šä¼ é˜Ÿåˆ—

        # å¯åŠ¨åª’ä½“å¤„ç†å·¥ä½œè¿›ç¨‹
        media_worker = multiprocessing.Process(
            target=worker_process_loop,
            args=(TASK_QUEUE, RESULT_QUEUE, UPLOAD_QUEUE), 
            daemon=False
        )
        media_worker.start()
        
        # å¯åŠ¨ä¸“ç”¨çš„ä¸Šä¼ å·¥ä½œè¿›ç¨‹
        uploader_worker = multiprocessing.Process(
            target=uploader_process_loop,
            args=(UPLOAD_QUEUE, RESULT_QUEUE),
            daemon=False
        )
        uploader_worker.start()

        # å¯åŠ¨ç»“æœå¤„ç†çº¿ç¨‹
        result_thread = threading.Thread(
            target=result_processor_thread_loop,
            args=(RESULT_QUEUE,),
            daemon=True
        )
        result_thread.start()

        # --- 6. å¯åŠ¨ Flask API æœåŠ¡ ---
        def run_flask_app():
            app.run(host='0.0.0.0', port=FLASK_API_LOCAL_PORT, debug=False, use_reloader=False)
        log_system_event("info", "æ­£åœ¨åå°å¯åŠ¨ Flask API æœåŠ¡...")
        threading.Thread(target=run_flask_app, daemon=True).start()
        if not wait_for_port(FLASK_API_LOCAL_PORT):
             raise RuntimeError("Flask æœåŠ¡å¯åŠ¨å¤±è´¥ã€‚")

        # --- 7. å¯åŠ¨ frpc å®¢æˆ·ç«¯ ---
        log_system_event("info", "æ­£åœ¨å‡†å¤‡ frpc å®¢æˆ·ç«¯...")
        if not os.path.exists("/kaggle/working/frpc"):
            run_command("wget -q https://github.com/fatedier/frp/releases/download/v0.54.0/frp_0.54.0_linux_amd64.tar.gz && tar -zxvf frp_0.54.0_linux_amd64.tar.gz && mv frp_0.54.0_linux_amd64/frpc /kaggle/working/frpc && chmod +x /kaggle/working/frpc").wait()
        
        frpc_ini = f"""
[common]
server_addr = {FRP_SERVER_ADDR}
server_port = {FRP_SERVER_PORT}
token = {FRP_TOKEN}
log_file = /kaggle/working/frpc.log
[mixfile_webdav_{MIXFILE_REMOTE_PORT}]
type = tcp
local_ip = 127.0.0.1
local_port = {MIXFILE_LOCAL_PORT}
remote_port = {MIXFILE_REMOTE_PORT}
[flask_api_{FLASK_API_REMOTE_PORT}]
type = tcp
local_ip = 127.0.0.1
local_port = {FLASK_API_LOCAL_PORT}
remote_port = {FLASK_API_REMOTE_PORT}
"""
        with open('frpc.ini', 'w') as f: f.write(frpc_ini)
        run_command('/kaggle/working/frpc -c ./frpc.ini')
        log_system_event("info", "frpc å®¢æˆ·ç«¯å·²åœ¨åå°å¯åŠ¨ã€‚")
        time.sleep(3)
        
        # --- 8. æœ€ç»ˆçŠ¶æ€æŠ¥å‘Šä¸ä¿æ´» ---
        public_api_base_url = f"http://{FRP_SERVER_ADDR}:{FLASK_API_REMOTE_PORT}"
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æœåŠ¡å‡å·²æˆåŠŸå¯åŠ¨ï¼æ‚¨çš„ç»Ÿä¸€ API å·²ä¸Šçº¿ã€‚")
        print(f"  -> API å…¥å£ (POST) : {public_api_base_url}/api/upload")
        print(f"  -> ä»»åŠ¡çŠ¶æ€ (GET)  : {public_api_base_url}/api/tasks/<task_id>")
        print(f"  -> å¥åº·æ£€æŸ¥ (GET)  : {public_api_base_url}/killer_status_frp")
        print(f"  -> è¿œç¨‹å…³é—­ (GET)  : {public_api_base_url}/force_shutdown_notebook?token={KILLER_API_SHUTDOWN_TOKEN}")
        print("="*60)
        
        while True:
            time.sleep(300)
            all_workers_alive = media_worker.is_alive() and uploader_worker.is_alive()
            worker_status_msg = 'å…¨éƒ¨å­˜æ´»' if all_workers_alive else 'å­˜åœ¨å·²é€€å‡ºçš„å·¥ä½œè¿›ç¨‹'
            log_system_event("info", f"æœåŠ¡æŒç»­è¿è¡Œä¸­... ({time.ctime()}) å·¥ä½œè¿›ç¨‹çŠ¶æ€: {worker_status_msg}")
            
    except (DecryptionError, RuntimeError, ValueError) as e:
        log_system_event("critical", f"ç¨‹åºå¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
        log_system_event("critical", "æœåŠ¡æ— æ³•å¯åŠ¨ï¼Œç¨‹åºå°†ç»ˆæ­¢ã€‚")
    except KeyboardInterrupt:
        log_system_event("info", "æœåŠ¡å·²æ‰‹åŠ¨åœæ­¢ã€‚")
        if 'TASK_QUEUE' in globals() and TASK_QUEUE is not None:
            TASK_QUEUE.put(None)
        if 'UPLOAD_QUEUE' in globals() and UPLOAD_QUEUE is not None:
            UPLOAD_QUEUE.put(None)
    except Exception as e:
        log_system_event("critical", f"å‘ç”ŸæœªçŸ¥çš„è‡´å‘½é”™è¯¯: {e}")

if __name__ == '__main__':
    main()